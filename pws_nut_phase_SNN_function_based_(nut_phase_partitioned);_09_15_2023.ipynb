{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNJJsEYhvbB1er8oONviZc3",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/alegtr2003/alegtr2003/blob/main/pws_nut_phase_SNN_function_based_(nut_phase_partitioned)%3B_09_15_2023.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "**Program Overview**\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "mc64Z58F0tDe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Author: Carlos R. Sulsona (CS)\n",
        "# Date Created: 08/09/2023\n",
        "# Latest Revision Date: 09/20/2023, CS\n",
        "# Description: This module contains Python code for constructing a Sequential Neural Network(SNN)\n",
        "# using Scikit-Learn, Tensorflow, and Keras APIs.  The model uses the multi-class classification algorithm\n",
        "# categorical_crossentropy to predict Prader-Willi Syndrome (PWS) nutritional phase on data collected\n",
        "# from a nutritional phase questionnaire.\n"
      ],
      "metadata": {
        "id": "14y7eUMs1KEu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "**Required Modules, Libraries and Packages**\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "vfD-lU1S1Nb1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#!pip install -q keras\n",
        "#!pip install tf2onnx\n",
        "#!pip install git+https://github.com/onnx/tensorflow-onnx\n",
        "!pip install --upgrade tensorflow"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "2YWjMRZVE3jQ",
        "outputId": "e4cd66bd-93b5-4936-eb27-992a7b7624c1",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.10/dist-packages (2.15.0)\n",
            "Collecting tensorflow\n",
            "  Downloading tensorflow-2.16.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (589.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m589.8/589.8 MB\u001b[0m \u001b[31m1.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=23.5.26 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (24.3.25)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.5.4)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.2.0)\n",
            "Collecting h5py>=3.10.0 (from tensorflow)\n",
            "  Downloading h5py-3.11.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.3/5.3 MB\u001b[0m \u001b[31m72.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (18.1.1)\n",
            "Collecting ml-dtypes~=0.3.1 (from tensorflow)\n",
            "  Downloading ml_dtypes-0.3.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.2/2.2 MB\u001b[0m \u001b[31m68.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.3.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow) (24.0)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.20.3)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.31.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow) (67.7.2)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.16.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.4.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (4.11.0)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.14.1)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.64.0)\n",
            "Collecting tensorboard<2.17,>=2.16 (from tensorflow)\n",
            "  Downloading tensorboard-2.16.2-py3-none-any.whl (5.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.5/5.5 MB\u001b[0m \u001b[31m75.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting keras>=3.0.0 (from tensorflow)\n",
            "  Downloading keras-3.3.3-py3-none-any.whl (1.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m69.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.37.0)\n",
            "Requirement already satisfied: numpy<2.0.0,>=1.23.5 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.25.2)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow) (0.43.0)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.10/dist-packages (from keras>=3.0.0->tensorflow) (13.7.1)\n",
            "Collecting namex (from keras>=3.0.0->tensorflow)\n",
            "  Downloading namex-0.0.8-py3-none-any.whl (5.8 kB)\n",
            "Collecting optree (from keras>=3.0.0->tensorflow)\n",
            "  Downloading optree-0.11.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (311 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m311.2/311.2 kB\u001b[0m \u001b[31m28.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow) (2024.2.2)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.17,>=2.16->tensorflow) (3.6)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.17,>=2.16->tensorflow) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.17,>=2.16->tensorflow) (3.0.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard<2.17,>=2.16->tensorflow) (2.1.5)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich->keras>=3.0.0->tensorflow) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich->keras>=3.0.0->tensorflow) (2.16.1)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich->keras>=3.0.0->tensorflow) (0.1.2)\n",
            "Installing collected packages: namex, optree, ml-dtypes, h5py, tensorboard, keras, tensorflow\n",
            "  Attempting uninstall: ml-dtypes\n",
            "    Found existing installation: ml-dtypes 0.2.0\n",
            "    Uninstalling ml-dtypes-0.2.0:\n",
            "      Successfully uninstalled ml-dtypes-0.2.0\n",
            "  Attempting uninstall: h5py\n",
            "    Found existing installation: h5py 3.9.0\n",
            "    Uninstalling h5py-3.9.0:\n",
            "      Successfully uninstalled h5py-3.9.0\n",
            "  Attempting uninstall: tensorboard\n",
            "    Found existing installation: tensorboard 2.15.2\n",
            "    Uninstalling tensorboard-2.15.2:\n",
            "      Successfully uninstalled tensorboard-2.15.2\n",
            "  Attempting uninstall: keras\n",
            "    Found existing installation: keras 2.15.0\n",
            "    Uninstalling keras-2.15.0:\n",
            "      Successfully uninstalled keras-2.15.0\n",
            "  Attempting uninstall: tensorflow\n",
            "    Found existing installation: tensorflow 2.15.0\n",
            "    Uninstalling tensorflow-2.15.0:\n",
            "      Successfully uninstalled tensorflow-2.15.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "tf-keras 2.15.1 requires tensorflow<2.16,>=2.15, but you have tensorflow 2.16.1 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed h5py-3.11.0 keras-3.3.3 ml-dtypes-0.3.2 namex-0.0.8 optree-0.11.0 tensorboard-2.16.2 tensorflow-2.16.1\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "h5py",
                  "keras",
                  "ml_dtypes",
                  "tensorboard",
                  "tensorflow"
                ]
              },
              "id": "1f129b20db544d919cce95e864aded36"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "print(\"TensorFlow version:\", tf.__version__)\n",
        "!nvcc --version\n",
        "!apt list --installed | grep cudnn\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "vqcA53sV0ZkD",
        "outputId": "78b1a219-1844-407c-ece0-eb7b110efa7c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TensorFlow version: 2.15.0\n",
            "nvcc: NVIDIA (R) Cuda compiler driver\n",
            "Copyright (c) 2005-2023 NVIDIA Corporation\n",
            "Built on Tue_Aug_15_22:02:13_PDT_2023\n",
            "Cuda compilation tools, release 12.2, V12.2.140\n",
            "Build cuda_12.2.r12.2/compiler.33191640_0\n",
            "\n",
            "WARNING: apt does not have a stable CLI interface. Use with caution in scripts.\n",
            "\n",
            "libcudnn8-dev/unknown,now 8.9.6.50-1+cuda12.2 amd64 [installed,upgradable to: 8.9.7.29-1+cuda12.2]\n",
            "libcudnn8/unknown,now 8.9.6.50-1+cuda12.2 amd64 [installed,upgradable to: 8.9.7.29-1+cuda12.2]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7y4KQlYYWoNZ"
      },
      "outputs": [],
      "source": [
        "# scikit: The scikit-learn library is the most popular library for general machine learning in Python.  Library contains fucntions used in tasks like model evaluation and model hyper-parameter optimization.\n",
        "# Keras: Keras is one of the most popular deep learning libraries in Python for research and development. It is is a high-level, deep learning API developed by Google for implementing neural networks.\n",
        "\n",
        "#--- CLASS MODULES REQUIRED FOR DATA PROCESSING AND DATA VISUALIZATION --------------------\n",
        "# from numpy.random import seed\n",
        "# seed(0)\n",
        "import pandas as pd                                          # imports pandas library for reading datafile and creating and manipulating data frames\n",
        "import sklearn                                               # imports the sklearn scikit library - features various classification, regression and clustering algorithms\n",
        "import numpy as np                                           # library for scientific computing and matrix support for Python\n",
        "import seaborn as sn                                         # library for graphical visualization of data\n",
        "import pickle\n",
        "from flask import Flask\n",
        "import importlib.metadata\n",
        "\n",
        "from sklearn.model_selection import train_test_split         # imports the sklearn scikit library required to split dataset into train and test sets\n",
        "from sklearn.metrics import accuracy_score                   # used to validate trained model\n",
        "import matplotlib.pyplot as plt                              # used for plotting data\n",
        "from sklearn.utils import shuffle                            # contains tools for shuffling data in a dataframe\n",
        "from IPython.display import clear_output                     # contains functions for updating data during program execution\n",
        "#from keras.callbacks import Callback                         # contains functions that can perform actions during training and allow views of the current state of the model\n",
        "#from keras.api._v2.keras.backend import clear_session        # contains functions for releasing resources used in creating a model\n",
        "from sklearn.preprocessing import MinMaxScaler               # scales values to a range of 0 to 1\n",
        "from sklearn.preprocessing import StandardScaler             # scales values to a range of 0 to 1\n",
        "from sklearn.preprocessing import LabelEncoder               # contains functions for encoding categorical data to values of zeroes(0) and ones(1)\n",
        "from keras.utils import normalize                            # contains functions to normalize a numpy array\n",
        "\n",
        "#--- CLASS MODULES REQUIRED FOR BUILDING AND TRAINING OF NEURAL NETWORK --------------------\n",
        "import tensorflow as tf                                      # contains methods for creating a neural network\n",
        "from tensorflow import keras                                 # high-level API running on top of Tensorflow used for the implementation of a neural network\n",
        "from keras.models import Sequential                          # contains functions for building a sequential (multi-layered) neural network\n",
        "from keras.optimizers import Adam                            # model optimization loss reduction function\n",
        "from keras.metrics import categorical_crossentropy           # model optimization loss reduction function\n",
        "from keras.layers import Dense                               # contains functions for building the layers of a dense neural network\n",
        "# from keras.utils.vis_utils import plot_model               # converts a Keras model to dot format and save to a file\n",
        "from keras.models import save_model                          # contains functions for saving a trained model to a file\n",
        "from keras.models import load_model                          # contains functions for loading a saved model from a file\n",
        "from keras import optimizers                                 # contains functions for implementing various optimization algorithms\n",
        "#from keras.saving.saving_api import save_weights           # contains functions for saving a model's edge weights\n",
        "from sklearn.model_selection import KFold, StratifiedKFold   # contains function required to perform cross-validation\n",
        "from keras.models import Sequential\n",
        "from keras.models import load_model\n",
        "\n",
        "#--- MODEL EVALUATION MODULES -------------------\n",
        "from sklearn.model_selection import cross_val_score          # contains function required to perform cross-validation scores\n",
        "from sklearn.model_selection import cross_validate           # contains metrics to evaluate by cross-validation\n",
        "from sklearn.model_selection import cross_val_predict        # contains metrics to evaluate by cross-validation\n",
        "from sklearn.model_selection import GridSearchCV             # contains functions for evaluating a model using cross-validation grid search\n",
        "from sklearn.metrics import confusion_matrix                 # contains fucntions for conducting a confusion matrix for model performance\n",
        "from sklearn.linear_model import SGDClassifier, LogisticRegression\n",
        "#from sklearn.wrappers import KerasClassifier\n",
        "from sklearn import metrics\n",
        "\n",
        "\n",
        "#--- UTILITY CLASS MODULES  --------------------\n",
        "from datetime import date                                    # contains functions for loading current date\n",
        "from time import time                                        # contains functions for loading current time\n",
        "from datetime import datetime                                # contains functions for loading current date and local time\n",
        "import sys                                                   # contains functions for obtaining information of the Python Runtime Environment, such as version, etc.\n",
        "from keras.callbacks import History                          # contains fucntions for recording events into a History object\n",
        "from sklearn.pipeline import Pipeline                        # contains functions for creating a pipeline of transforms with a final estimator\n",
        "from sklearn.preprocessing import Normalizer                 # contains functions for normalizing the features dataset (X_values)\n",
        "from sklearn.preprocessing import Binarizer                  # contains functions for binarization of values in the features dataset (X_values)\n",
        "from keras import backend as K                               # contains functions for reseting all global states generated by Keras API during model implementation\n",
        "import statistics as st                                      # module for performing statistics (mean, stdDev, etc.)\n",
        "from weakref import ref                                      # contains functions to allow creation of weak references to objects\n",
        "import random                                                # contains random number generators\n",
        "#import tf2onnx\n",
        "#import onnx\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "**Global Variables**\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "rsb4kbZh8Uhd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#--- GLOBAL AND TEMPORARY VARIABLES  --------------------\n",
        "models = []                                                  # list for storing the loop generated models at the various percentages\n",
        "modelTrainingDatasets=[]                                     # list for storing datasets to be used for training a model\n",
        "mergedDatasets=[]                                            # list for storing merged datasets\n",
        "history=Sequential()                                         # variable stores shell to create a sequential neural network object\n",
        "tempModel=Sequential()                                       # variable stores shell to create a sequential neural network object\n",
        "tempModel_1=Sequential()                                     # variable stores shell to create a sequential neural network object\n",
        "tempModel_2=Sequential()                                     # variable stores shell to create a sequential neural network object"
      ],
      "metadata": {
        "id": "zFsVhr7-8Pta"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "**Version of Programming Language(s) and API's Used**\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "A4Xj41InxbpZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Display Platform Versions Used\n",
        "print(\"Python version: \" + sys.version)\n",
        "print(\"TensorFlow, Keras version: \" + tf.__version__)\n",
        "print(\"Scikit-Learn version: \" + sklearn.__version__)\n",
        "print(\"Pickle version: \" + pickle.format_version)\n",
        "print(\"Flask version: \" + importlib.metadata.version(\"flask\"))\n",
        "print(\"Pandas version: \" + importlib.metadata.version(\"pandas\"))\n",
        "print(\"Numpy version: \" + importlib.metadata.version(\"numpy\"))\n",
        "#print(\"ONNX version: \" + importlib.metadata.version(\"onnx\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cIKoN7KPxivZ",
        "outputId": "c435c9d0-2730-4ed6-ac90-3fe74f579c9a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Python version: 3.10.12 (main, Nov 20 2023, 15:14:05) [GCC 11.4.0]\n",
            "TensorFlow, Keras version: 2.16.1\n",
            "Scikit-Learn version: 1.2.2\n",
            "Pickle version: 4.0\n",
            "Flask version: 2.2.5\n",
            "Pandas version: 2.0.3\n",
            "Numpy version: 1.25.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Use from_function for tf functions\n",
        "model = tf.keras.Sequential()\n",
        "model.add(tf.keras.layers.Dense(4, activation=\"relu\"))\n",
        "\n",
        "input_signature = [tf.TensorSpec([43,6], tf.float32, name='x')]   # the tf.TensorSpec => num of input values and num of output values, nothing to do with the number of records!\n",
        "onnx_model, _ = tf2onnx.convert.from_keras(model, input_signature, opset=13)\n",
        "onnx.save(onnx_model, \"sample_data/model.onnx\")"
      ],
      "metadata": {
        "id": "wLuC7LKfUJJe",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 372
        },
        "collapsed": true,
        "outputId": "bdb03a5b-e939-4710-95cd-28f295807264"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "'Sequential' object has no attribute 'output_names'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-15-6f7b71d7c228>\u001b[0m in \u001b[0;36m<cell line: 6>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0minput_signature\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensorSpec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m43\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m6\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'x'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m   \u001b[0;31m# the tf.TensorSpec => num of input values and num of output values, nothing to do with the number of records!\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0monnx_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf2onnx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_keras\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_signature\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m13\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0monnx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0monnx_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"sample_data/model.onnx\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tf2onnx/convert.py\u001b[0m in \u001b[0;36mfrom_keras\u001b[0;34m(model, input_signature, opset, custom_ops, custom_op_handlers, custom_rewriter, inputs_as_nchw, outputs_as_nchw, extra_opset, shape_override, target, large_model, output_path, optimizers)\u001b[0m\n\u001b[1;32m    440\u001b[0m                                outputs_as_nchw, extra_opset, shape_override, target, large_model, output_path)\n\u001b[1;32m    441\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 442\u001b[0;31m     \u001b[0mold_out_names\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_rename_duplicate_keras_model_names\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    443\u001b[0m     \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msaving\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msaving_utils\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0m_saving_utils\u001b[0m \u001b[0;31m# pylint: disable=import-outside-toplevel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    444\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tf2onnx/convert.py\u001b[0m in \u001b[0;36m_rename_duplicate_keras_model_names\u001b[0;34m(model)\u001b[0m\n\u001b[1;32m    329\u001b[0m     \"\"\"\n\u001b[1;32m    330\u001b[0m     \u001b[0mold_out_names\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 331\u001b[0;31m     \u001b[0;32mif\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput_names\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput_names\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput_names\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    332\u001b[0m         \u001b[0;31m# In very rare cases, keras has a bug where it will give multiple outputs the same name\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    333\u001b[0m         \u001b[0;31m# We must edit the model or the TF trace will fail\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'Sequential' object has no attribute 'output_names'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "-4fFyWO-URaL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "**Classes**\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "JPwbcxSmGp89"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#--- CUSTOM CLASSES --------------------\n",
        "class ShowModelLearning(keras.callbacks.Callback):\n",
        "    \"\"\"Class to graphically visualize training probabilistic loss and accuracy of model during training\"\"\"\n",
        "    def on_train_begin(self, logs={}):\n",
        "        self.metrics = {}\n",
        "        for metric in logs:\n",
        "            self.metrics[metric] = []\n",
        "\n",
        "    def on_epoch_end(self, epoch, logs={}):\n",
        "        # Storing metrics\n",
        "        for metric in logs:\n",
        "            if metric in self.metrics:\n",
        "                self.metrics[metric].append(logs.get(metric))\n",
        "            else:\n",
        "                self.metrics[metric] = [logs.get(metric)]\n",
        "\n",
        "        # Plot loss and accuracy values\n",
        "        metrics = [x for x in logs if \"val\" not in x]\n",
        "\n",
        "        f, axis = plt.subplots(1, len(metrics), figsize=(15,5))\n",
        "        clear_output(wait=True)\n",
        "\n",
        "        for i, metric in enumerate(metrics):\n",
        "            axis[i].plot(range(1, epoch + 2),\n",
        "                        self.metrics[metric],\n",
        "                        label=metric)\n",
        "            if logs[\"val_\" + metric]:\n",
        "                axis[i].plot(range(1, epoch + 2),\n",
        "                            self.metrics[\"val_\" + metric],\n",
        "                            label=\"val_\" + metric)\n",
        "\n",
        "            axis[i].legend()\n",
        "            axis[i].grid()\n",
        "\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "\n",
        "# Create an object of the ShowModelLearning Class\n",
        "callbacks_list = ShowModelLearning()\n"
      ],
      "metadata": {
        "id": "CjO4tp9jjVst"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "**Utility Functions**\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "nrNsFJtmGy2H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def readCSVFile(datafile):\n",
        "    '''Function reads a csv file passed by the function caller and copies its contents into a pandas dataframe object'''\n",
        "    df = pd.read_csv(datafile)    # main dataframe\n",
        "    df2 = df    # working copy of the main data frame\n",
        "\n",
        "    return(df2)\n"
      ],
      "metadata": {
        "id": "fUJKriVyW6N8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def listMultiAppend(*args):\n",
        "    '''Accepts 'n' elements and appends them to a list then returns the list'''\n",
        "\n",
        "    elementsToAppend=args\n",
        "    appendedElements=[]\n",
        "\n",
        "    for element in elementsToAppend:\n",
        "        appendedElements.append(element)\n",
        "\n",
        "    return(appendedElements)\n"
      ],
      "metadata": {
        "id": "0CnCyBxbSMmc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def dictMultiAppend(*args):\n",
        "    '''Accepts 'n' elements and appends them to a dictionary then returns the dictionary'''\n",
        "\n",
        "    elementsToAppend=args\n",
        "    appendedElements={}\n",
        "\n",
        "    for element in elementsToAppend:\n",
        "        appendedElements.append(element)\n",
        "\n",
        "    return(appendedElements)"
      ],
      "metadata": {
        "id": "DjDiTtrIfe_R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def averageMultiple2DArrays(inputArray):\n",
        "    '''Function inputs an array of 2D lists (3D object) and returns the averages of the lists within each 2D list block'''\n",
        "\n",
        "    listsToAvg=inputArray\n",
        "    block=0\n",
        "    arrStruct=0\n",
        "    elem=0\n",
        "    avg=0\n",
        "    tempAvg=[]\n",
        "    results=[]\n",
        "\n",
        "    while block<len(listsToAvg): #n=2\n",
        "        while elem<len(listsToAvg[0][0]): #n=5\n",
        "            while arrStruct<len(listsToAvg[0]): #n=2\n",
        "                avg=avg+listsToAvg[block][arrStruct][elem]\n",
        "                arrStruct+=1\n",
        "            tempAvg.append(avg/len(listsToAvg[0]))\n",
        "            avg=0\n",
        "            arrStruct=0\n",
        "            elem+=1\n",
        "        results.append(tempAvg)\n",
        "        tempAvg=[]\n",
        "        elem=0\n",
        "        block+=1\n",
        "    block=0\n",
        "\n",
        "    i=0\n",
        "\n",
        "    return(results)\n"
      ],
      "metadata": {
        "id": "B6YNqvwIlaDA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "**Dataset Preprocessing Functions**\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "Y_0sv4Ux8DE0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def datasetMinMaxScaler(X_values):\n",
        "    '''Scales data in a dataset so that all values fall within the range of values,\n",
        "    typically zero(0) and one(1). Function takes in a 2D dataframe object and\n",
        "    returns a 2D dataframe object'''\n",
        "\n",
        "    # Variables\n",
        "    dfColNames=X_values.columns\n",
        "\n",
        "    # Initialize a MinMaxScaler object\n",
        "    scaler=MinMaxScaler(feature_range=(0,1))  # \"feature_range\" defines the range to scale data to\n",
        "\n",
        "    # Fit and transform the data to range of zero(0) and one(1) values\n",
        "    minMaxScaledFeatures=scaler.fit_transform(X_values)\n",
        "    dfScale=minMaxScaledFeatures\n",
        "\n",
        "\n",
        "    # Convert array to a Pandas dataframe object and reassign names to columns\n",
        "    minMaxScaledFeatures=pd.DataFrame(minMaxScaledFeatures)\n",
        "    minMaxScaledFeatures.columns=dfColNames\n",
        "    scaledData=minMaxScaledFeatures\n",
        "\n",
        "    return(minMaxScaledFeatures)    # retruns a 2D dataframe object\n",
        "\n"
      ],
      "metadata": {
        "id": "luFGiMF78NYG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dfScale=pd.read_csv(\"mlpa_results (no_ID).2; 09-20-2023.csv\")\n",
        "dfScale=dfScale.drop(columns=[\"id\"])\n",
        "\n",
        "results=datasetMinMaxScaler(dfScale)\n",
        "# dfScale=results\n",
        "dfRev=results\n",
        "results\n",
        "\n",
        "results=results[\"Insulin\"]\n",
        "results=results\n",
        "results\n",
        "\n",
        "\n",
        "# -- Dataset visualization --\n",
        "# Apply default theme\n",
        "sn.set_theme()\n",
        "\n",
        "# Create plot\n",
        "dfScale=dfScale['Insulin']\n",
        "dfScale=pd.DataFrame(dfScale)\n",
        "dfScale\n",
        "nums=list(range(0,len(dfScale)))\n",
        "dfScale.index=nums\n",
        "index=dfScale.index\n",
        "sn.barplot(data=dfScale, y=\"Insulin\", x=index)\n",
        "sn.relplot(data=dfScale, y=\"Insulin\", x=index, kind=\"line\")\n",
        "\n",
        "dfRev\n"
      ],
      "metadata": {
        "id": "jIeVNmeoFa4x",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 390
        },
        "outputId": "247f2fe4-9dff-4a8a-c6e2-b8984b627d69",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: 'mlpa_results (no_ID).2; 09-20-2023.csv'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-29-9caaf9d85168>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdfScale\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"mlpa_results (no_ID).2; 09-20-2023.csv\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mdfScale\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdfScale\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"id\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mresults\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdatasetMinMaxScaler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdfScale\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# dfScale=results\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/util/_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    209\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    210\u001b[0m                     \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnew_arg_name\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnew_arg_value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 211\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    212\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    213\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/util/_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    329\u001b[0m                     \u001b[0mstacklevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfind_stack_level\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m                 )\n\u001b[0;32m--> 331\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    332\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    333\u001b[0m         \u001b[0;31m# error: \"Callable[[VarArg(Any), KwArg(Any)], Any]\" has no\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[1;32m    948\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    949\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 950\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    951\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    952\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    603\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    604\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 605\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    606\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    607\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1440\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1441\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1442\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1443\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1444\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1733\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1734\u001b[0m                     \u001b[0mmode\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m\"b\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1735\u001b[0;31m             self.handles = get_handle(\n\u001b[0m\u001b[1;32m   1736\u001b[0m                 \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1737\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    854\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    855\u001b[0m             \u001b[0;31m# Encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 856\u001b[0;31m             handle = open(\n\u001b[0m\u001b[1;32m    857\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    858\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'mlpa_results (no_ID).2; 09-20-2023.csv'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def datasetStandardScaler(X_values):\n",
        "    '''Standardizes the input values (x-values) by removing the mean and scaling\n",
        "       values to unit variance (z = (x - u) / s). This centers the data around\n",
        "       zero, with a stdDev of 1.  x=sample, u is the mean, s is standard deviation.\n",
        "       Function takes in a 1D dataframe object and returns a 1D dataframe object.\n",
        "       \"Centering and scaling happen independently on each feature by computing\n",
        "       the relevant statistics on the samples in the training set. Mean and\n",
        "       standard deviation are then stored to be used on later data using transform.\n",
        "       Standardization of a dataset is a common requirement for many machine\n",
        "       learning estimators: they might behave badly if the individual features\n",
        "       do not more or less look like standard normally distributed data\n",
        "       (e.g. Gaussian with 0 mean and unit variance).\" - scikitlearn'''\n",
        "\n",
        "    # Variables\n",
        "    dfColNames=X_values.columns    # stores dataframe column names\n",
        "\n",
        "    # Initialize a MinMaxScaler object\n",
        "    scaler=StandardScaler()\n",
        "\n",
        "    # Fit and transform the data to range of zero(0) and one(1) values\n",
        "    standardized_X_values=scaler.fit_transform(X_values)\n",
        "\n",
        "    # Convert array to a Pandas dataframe object and reassign names to columns\n",
        "    standardized_X_values=pd.DataFrame(standardized_X_values)\n",
        "    standardized_X_values.columns=dfColNames\n",
        "\n",
        "    return(standardized_X_values)    # retruns a 2D dataframe object\n"
      ],
      "metadata": {
        "id": "PMnBjMn4DYJq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dfScale=pd.read_csv(\"multiclass_test_dataset; 09-18-2023.csv\")\n",
        "dfScale=dfScale.drop(columns=[\"id\"])\n",
        "\n",
        "results=datasetStandardScaler(dfScale)\n",
        "dfScale=results\n",
        "\n",
        "results=results[\"Leptin\"]\n",
        "results=results\n",
        "results\n",
        "\n",
        "# -- Dataset visualization --\n",
        "# Apply default theme\n",
        "sn.set_theme()\n",
        "\n",
        "# Create plot\n",
        "dfScale=dfScale['Leptin']\n",
        "dfScale=pd.DataFrame(dfScale)\n",
        "dfScale\n",
        "nums=list(range(0,len(dfScale)))\n",
        "dfScale.index=nums\n",
        "index=dfScale.index\n",
        "sn.barplot(data=dfScale, y=\"Leptin\", x=index)\n",
        "sn.displot(data=dfScale)\n",
        "sn.displot(data=dfScale)\n"
      ],
      "metadata": {
        "id": "HdZ15GwlJBpN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def datasetNormalization(X_values):\n",
        "    '''Function takes in a dataframe of x-values (features) and returns a\n",
        "    dataframe of normalized values. Used in Vector Space Model for\n",
        "    text classification or clustering'''\n",
        "\n",
        "    # Variables\n",
        "    dfColNames=X_values.columns    # stores dataframe column names\n",
        "\n",
        "    # Normalize X_values\n",
        "    scaler = Normalizer()\n",
        "\n",
        "    # Fit and transform the data to range of zero(0) and one(1) values\n",
        "    scaler=scaler.fit(X_values)\n",
        "    normalized_features = scaler.transform(X_values)\n",
        "\n",
        "    # Convert array to a Pandas dataframe object and reassign names to columns\n",
        "    normalized_features = pd.DataFrame(normalized_features)\n",
        "    normalized_features.columns=dfColNames\n",
        "\n",
        "    return(normalized_features)    # returns a 2D dataframe object\n"
      ],
      "metadata": {
        "id": "sqjWe9itu6hl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dfScale=pd.read_csv(\"mlpa_results (no_ID).2; 09-20-2023.csv\")\n",
        "dfScale=dfScale.drop(columns=[\"id\", \"Phase_1a\", \"Phase_1b\", \"Phase_2a\", \"Phase_2b\", \"Phase_3\", \"Phase_4\"])\n",
        "dfCat=dfScale\n",
        "\n",
        "results=datasetMinMaxScaler(dfScale)\n",
        "\n",
        "results=datasetNormalization(results)\n",
        "\n",
        "dfScale=results\n",
        "dfCatNorm=dfScale\n",
        "\n",
        "# results=results[\"Leptin\"]\n",
        "# results=results\n",
        "\n",
        "\n",
        "\n",
        "# -- Dataset visualization --\n",
        "# Apply default theme\n",
        "sn.set_theme()\n",
        "\n",
        "# # Create plot\n",
        "# dfScale=dfScale['Leptin']\n",
        "# dfScale=pd.DataFrame(dfScale)\n",
        "# nums=list(range(0,96))\n",
        "# dfScale.index=nums\n",
        "# index=dfScale.index\n",
        "# cols=[]\n",
        "# cols=listMultiAppend(dfCat.columns)\n",
        "# cols=cols[0]\n",
        "# sn.set(rc={'figure.figsize':(11.7,8.27)})\n",
        "# sn.barplot(data=dfScale, y=\"Leptin\", x=index)\n",
        "# sn.relplot(data=dfScale, y=\"Leptin\", x=index)\n",
        "# sn.displot(data=dfScale, y=\"Leptin\", x=index, stat=\"density\")\n",
        "# sn.displot(data=dfCatNorm, y=\"Leptin\", x=index, stat=\"probability\", kind=\"kde\")\n",
        "\n",
        "# Dataset by single feature\n",
        "feature=pd.DataFrame(dfCat[\"Leptin\"])\n",
        "normFeature=pd.DataFrame(dfCatNorm[\"Leptin\"])\n",
        "\n",
        "feature2=pd.DataFrame(dfCat[\"Ghrelin\"])\n",
        "normFeature2=pd.DataFrame(dfCatNorm[\"Ghrelin\"])\n",
        "\n",
        "feature3=pd.DataFrame(dfCat[\"Insulin\"])\n",
        "normFeature3=pd.DataFrame(dfCatNorm[\"Insulin\"])\n",
        "\n",
        "# Plots\n",
        "# sn.catplot(data=dfCat, kind=\"box\")\n",
        "# sn.catplot(data=dfCatNorm, kind=\"box\")\n",
        "sn.displot(data=feature2, kde=\"True\")\n",
        "sn.displot(data=normFeature2, kde=\"True\")\n",
        "\n",
        "# sn.displot(data=feature2, kind=\"kde\")\n",
        "# sn.displot(data=normFeature2, kind=\"kde\")\n",
        "# sn.displot(data=normFeature2, kind=\"ecdf\")\n",
        "\n",
        "# sn.displot(data=results, x=\"Leptin\", y=\"Ghrelin\", kind=\"kde\")\n",
        "\n",
        "# # for normFeature in dfCatNorm:\n",
        "# # sn.displot(data=dfCat[normFeature], kde=\"True\")\n",
        "# # sn.displot(data=dfCatNorm[normFeature], kde=\"True\")\n",
        "\n",
        "sn.displot(data=feature3, kde=\"True\")\n",
        "sn.displot(data=normFeature3, kde=\"True\")\n",
        "\n",
        "dfCatNorm"
      ],
      "metadata": {
        "id": "uksLid21M0R2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def datasetBinarization(X_values):\n",
        "    '''Function takes in a dataframe of x-values (features) and returns a 2D\n",
        "    dataframe of binary values.'''\n",
        "\n",
        "    # Variables\n",
        "    dfColNames=X_values.columns    # stores dataframe column names\n",
        "\n",
        "    # Normalize X_values\n",
        "    scaler = Binarizer(threshold=500)    # threshold is crucial, setting prob that a value is either 0 or 1\n",
        "\n",
        "    # Fit and transform the data to range of zero(0) and one(1) values\n",
        "    scaler=scaler.fit(X_values)\n",
        "    binarized_features = scaler.transform(X_values)\n",
        "\n",
        "    # Convert array to a Pandas dataframe object and reassign names to columns\n",
        "    binarized_features = pd.DataFrame(binarized_features)\n",
        "    binarized_features.columns=dfColNames\n",
        "\n",
        "    return(binarized_features)    # returns a 2D dataframe object\n"
      ],
      "metadata": {
        "id": "1vpqKXV1jYy4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dfScale=pd.read_csv(\"multiclass_test_dataset; 09-18-2023.csv\")\n",
        "dfScale=dfScale.drop(columns=[\"id\"])\n",
        "\n",
        "datasetBinarization(dfScale)"
      ],
      "metadata": {
        "id": "upwZrwvllNnI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def categoricalDataEncoder(datasetToEncode):\n",
        "    '''Endcodes textual data a dataframe column so that all values are numerical.\n",
        "    Values range from 0 to (number of catgories) -1.\n",
        "    Takes in a 1D dataframe and returns a 1D dataframe object'''\n",
        "\n",
        "    # Variables\n",
        "    dfColName=datasetToEncode.name    # stores dataframe column names\n",
        "\n",
        "    # Initialize a LabelEncoder object\n",
        "    labelEncoder=LabelEncoder()\n",
        "\n",
        "    # Fit and transform the data to values of zeroes(0) and ones(1)\n",
        "    encodedData=labelEncoder.fit_transform(datasetToEncode)\n",
        "\n",
        "    # Convert array to a Pandas dataframe object and reassign name to column\n",
        "    encodedData=pd.DataFrame(encodedData)\n",
        "    encodedData.columns=[dfColName]\n",
        "\n",
        "    return(encodedData)    # returns a 1D dataframe object\n"
      ],
      "metadata": {
        "id": "oeELzwCZAJTV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dfOri=pd.read_csv(\"multiclass_test_dataset_with_categorical_data; 09-18-2023.csv\")\n",
        "dfEncoded=dfOri[\"nut_phase\"]\n",
        "dfNew=dfOri.drop(columns=[\"nut_phase\"])\n",
        "\n",
        "dfEncoded=categoricalDataEncoder(dfScale)\n",
        "dfEncoded=pd.concat([dfNew, dfEncoded], axis=1)\n",
        "\n",
        "dfEncoded"
      ],
      "metadata": {
        "id": "ycE3IEefnmND"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def categoricalDataEncoderDummies(datasetToEncode):\n",
        "    ''' '''"
      ],
      "metadata": {
        "id": "hdQ1gkE5yvA_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "**Split Dataframe(s) By Nutritional Phase Functions**\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "SqF62HtvHoP8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def splitDataFrameByNutPhase(dataframeToSplit):\n",
        "    '''Splits a dataset into subsets separating the data by nutritional phase.\n",
        "        Returns a 1D array of six(6) lists, each containing input values and\n",
        "        output values for one of the six(6) nut_phases.'''\n",
        "\n",
        "    #Variables\n",
        "    df=pd.DataFrame()\n",
        "    dataframesByNutPhase=[]\n",
        "    nutPhaseSubsets=[]\n",
        "\n",
        "    # Copy the dataset\n",
        "    df=dataframeToSplit\n",
        "\n",
        "    # Columns to omit from dataframe\n",
        "    nonEssential_columns = [\"rec_num\", \"sample_id\", \"nut_phase\", \"Phase_1a\",\n",
        "                         \"Phase_1b\",\"Phase_2a\", \"Phase_2b\",\"Phase_3\", \"Phase_4\"]\n",
        "    output_columns = [\"Phase_1a\",\"Phase_1b\",\"Phase_2a\", \"Phase_2b\",\n",
        "                      \"Phase_3\", \"Phase_4\"]\n",
        "\n",
        "    # nut_phase 1a dataset (n=11)\n",
        "    df_1a=df[df[\"Phase_1a\"] > 0.0]  # creates a df with only Phase 1a values\n",
        "\n",
        "    # nut_phase 1b dataset (n=12)\n",
        "    df_1b=df[df[\"Phase_1b\"] > 0.0]  # creates a df with only Phase 1b values\n",
        "\n",
        "    # nut_phase 2a dataset (n=10)\n",
        "    df_2a=df[df[\"Phase_2a\"] > 0.0]  # creates a df with only Phase 2a values\n",
        "\n",
        "    # nut_phase 2b dataset (n=42)\n",
        "    df_2b=df[df[\"Phase_2b\"] > 0.0]  # creates a df with only Phase 2b values\n",
        "\n",
        "    # nut_phase 3 dataset (n=19)\n",
        "    df_3=df[df[\"Phase_3\"] > 0.0]  # creates a df with only Phase 3 values\n",
        "\n",
        "    # nut_phase 4 dataset (n=7)\n",
        "    df_4=df[df[\"Phase_4\"] > 0.0]  # creates a df with only Phase 4 values\n",
        "\n",
        "    # package all subsets\n",
        "    dataframesByNutPhase=listMultiAppend(df_1a, df_1b, df_2a, df_2b, df_3, df_4)    # 1D array\n",
        "\n",
        "    return(dataframesByNutPhase)\n"
      ],
      "metadata": {
        "id": "X9fZZBrmV5hy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def splitDataFrameIntoInputOutputByNutPhase(dataframeToSplit):\n",
        "    '''Splits a dataset into subsets separating the data by nutritional phase.\n",
        "       Returns a 2D array of three(3) lists, input values, output values,\n",
        "       mergedInputOutput values'''\n",
        "\n",
        "    #Variables\n",
        "    df=pd.DataFrame()\n",
        "    inputValues=[]\n",
        "    outputValues=[]\n",
        "    nutPhaseSubsets=[]\n",
        "\n",
        "    # Copy the dataset\n",
        "    df=dataframeToSplit\n",
        "\n",
        "    # Columns to omit from dataframe (43 features are retained representing inputs for each nut_phase)\n",
        "    nonEssential_columns = [\"rec_num\", \"sample_id\", \"nut_phase\", \"Phase_1a\",\n",
        "                         \"Phase_1b\",\"Phase_2a\", \"Phase_2b\",\"Phase_3\", \"Phase_4\"]\n",
        "    output_columns = [\"Phase_1a\", \"Phase_1b\", \"Phase_2a\", \"Phase_2b\",\n",
        "                      \"Phase_3\", \"Phase_4\"]\n",
        "\n",
        "    # nut_phase 1a dataset (n=11)\n",
        "    df_1a=df[df[\"Phase_1a\"] > 0.0]  # creates a df with only Phase 1a values\n",
        "    X_1a_input=df_1a.drop(columns=nonEssential_columns)\n",
        "    y_1a_output=df_1a[output_columns]\n",
        "\n",
        "    # nut_phase 1b dataset (n=12)\n",
        "    df_1b=df[df[\"Phase_1b\"] > 0.0]  # creates a df with only Phase 1b values\n",
        "    X_1b_input=df_1b.drop(columns=nonEssential_columns)\n",
        "    y_1b_output=df_1b[output_columns]\n",
        "\n",
        "    # nut_phase 2a dataset (n=10)\n",
        "    df_2a=df[df[\"Phase_2a\"] > 0.0]  # creates a df with only Phase 2a values\n",
        "    X_2a_input=df_2a.drop(columns=nonEssential_columns)\n",
        "    y_2a_output=df_2a[output_columns]\n",
        "\n",
        "    # nut_phase 2b dataset (n=42)\n",
        "    df_2b=df[df[\"Phase_2b\"] > 0.0]  # creates a df with only Phase 2b values\n",
        "    X_2b_input=df_2b.drop(columns=nonEssential_columns)\n",
        "    y_2b_output=df_2b[output_columns]\n",
        "\n",
        "    # nut_phase 3 dataset (n=19)\n",
        "    df_3=df[df[\"Phase_3\"] > 0.0]  # creates a df with only Phase 3 values\n",
        "    X_3_input=df_3.drop(columns=nonEssential_columns)\n",
        "    y_3_output=df_3[output_columns]\n",
        "\n",
        "    # nut_phase 4 dataset (n=7)\n",
        "    df_4=df[df[\"Phase_4\"] > 0.0]  # creates a df with only Phase 4 values\n",
        "    X_4_input=df_4.drop(columns=nonEssential_columns)\n",
        "    y_4_output=df_4[output_columns]\n",
        "\n",
        "    inputValues=listMultiAppend(X_1a_input, X_1b_input, X_2a_input, X_2b_input, X_3_input, X_4_input)\n",
        "    outputValues=listMultiAppend(y_1a_output, y_1b_output, y_2a_output, y_2b_output, y_3_output, y_4_output)\n",
        "\n",
        "    # package all subsets\n",
        "    nutPhaseSubsets=listMultiAppend(inputValues, outputValues)    # 2D Array\n",
        "\n",
        "    return(nutPhaseSubsets)\n"
      ],
      "metadata": {
        "id": "4e3u1Jq8Z9r-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def splitDataFrameByNutPhase2(dataframeToSplit):\n",
        "    '''Splits a dataset into subsets separating the data by nutritional phase. Returns a 2D array of three(3) lists, input values, output values, mergedInputOutput values'''\n",
        "\n",
        "    #Variables\n",
        "    df=pd.DataFrame()\n",
        "    inputValues=[]\n",
        "    outputValues=[]\n",
        "    X_1a_dataset=[]\n",
        "    X_1b_dataset=[]\n",
        "    X_2a_dataset=[]\n",
        "    X_2b_dataset=[]\n",
        "    X_3_dataset=[]\n",
        "    X_4_dataset=[]\n",
        "    allDatasets=[]\n",
        "\n",
        "    # Copy the dataset\n",
        "    df=dataframeToSplit\n",
        "\n",
        "    # Columns to omit from dataframe (43 features are retained representing inputs for each nut_phase)\n",
        "    nonEssential_columns = [\"rec_num\", \"sample_id\", \"nut_phase\", \"Phase_1a\",\n",
        "                         \"Phase_1b\",\"Phase_2a\", \"Phase_2b\",\"Phase_3\", \"Phase_4\"]\n",
        "    output_columns = [\"Phase_1a\",\"Phase_1b\",\"Phase_2a\", \"Phase_2b\",\n",
        "                      \"Phase_3\", \"Phase_4\"]\n",
        "\n",
        "    # nut_phase 1a dataset (n=11)\n",
        "    df_1a=df[df[\"Phase_1a\"] > 0.0]  # creates a df with only Phase 1a values\n",
        "    X_1a_dataset=df_1a.drop(columns=[\"rec_num\", \"sample_id\"])\n",
        "\n",
        "    # nut_phase 1b dataset (n=12)\n",
        "    df_1b=df[df[\"Phase_1b\"] > 0.0]  # creates a df with only Phase 1b values\n",
        "    X_1b_dataset=df_1b.drop(columns=[\"rec_num\", \"sample_id\"])\n",
        "\n",
        "    # nut_phase 2a dataset (n=10)\n",
        "    df_2a=df[df[\"Phase_2a\"] > 0.0]  # creates a df with only Phase 2a values\n",
        "    X_2a_dataset=df_2a.drop(columns=[\"rec_num\", \"sample_id\"])\n",
        "\n",
        "    # nut_phase 2b dataset (n=42)\n",
        "    df_2b=df[df[\"Phase_2b\"] > 0.0]  # creates a df with only Phase 2b values\n",
        "    X_2b_dataset=df_2b.drop(columns=[\"rec_num\", \"sample_id\"])\n",
        "\n",
        "    # nut_phase 3 dataset (n=19)\n",
        "    df_3=df[df[\"Phase_3\"] > 0.0]  # creates a df with only Phase 3 values\n",
        "    X_3_dataset=df_3.drop(columns=[\"rec_num\", \"sample_id\"])\n",
        "\n",
        "    # nut_phase 4 dataset (n=7)\n",
        "    df_4=df[df[\"Phase_4\"] > 0.0]  # creates a df with only Phase 4 values\n",
        "    X_4_dataset=df_4.drop(columns=[\"rec_num\", \"sample_id\"])\n",
        "\n",
        "    allDatasets=listMultiAppend(X_1a_dataset, X_1b_dataset, X_2a_dataset, X_2b_dataset, X_3_dataset, X_4_dataset)\n",
        "\n",
        "    # package all subsets\n",
        "    nutPhaseSubsets=listMultiAppend(allDatasets)\n",
        "\n",
        "    return(nutPhaseSubsets)\n"
      ],
      "metadata": {
        "id": "OeuKh7LDAsXf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def splitDataFrameIntoInputOutputFrames(dataFrameToSplit):\n",
        "    '''Splits dataframe into input and output datasets by nutritional phase'''\n",
        "\n",
        "    #Variables\n",
        "    df=pd.DataFrame()\n",
        "    inputValues=[]\n",
        "    outputValues=[]\n",
        "    allInputOutputValues=[]\n",
        "\n",
        "    # Copy the dataset\n",
        "    df=dataFrameToSplit\n",
        "\n",
        "    # Reference columns\n",
        "    # Columns to omit from dataframe (43 features are retained representing inputs for each nut_phase)\n",
        "    nonEssential_columns = [\"rec_num\", \"sample_id\", \"nut_phase\", \"Phase_1a\",\n",
        "                         \"Phase_1b\",\"Phase_2a\", \"Phase_2b\",\"Phase_3\", \"Phase_4\"]\n",
        "    output_columns = [\"Phase_1a\",\"Phase_1b\",\"Phase_2a\", \"Phase_2b\",\n",
        "                      \"Phase_3\", \"Phase_4\"]\n",
        "\n",
        "    # nut_phase 1a dataset (n=11)\n",
        "    df_1a=df[0]\n",
        "    df_1a=df_1a[df_1a[\"Phase_1a\"] > 0.0]   # creates a df with only Phase_1a values\n",
        "    X_1a_input=df_1a.drop(columns=nonEssential_columns)\n",
        "    y_1a_output=df_1a[output_columns]\n",
        "\n",
        "    # nut_phase 1b dataset (n=12)\n",
        "    df_1b=df[1]\n",
        "    df_1b=df_1b[df_1b[\"Phase_1b\"] > 0.0]  # creates a df with only Phase_1b values\n",
        "    X_1b_input=df_1b.drop(columns=nonEssential_columns)\n",
        "    y_1b_output=df_1b[output_columns]\n",
        "\n",
        "    # nut_phase 2a dataset (n=10)\n",
        "    df_2a=df[2]\n",
        "    df_2a=df_2a[df_2a[\"Phase_2a\"] > 0.0]  # creates a df with only Phase_2a values\n",
        "    X_2a_input=df_2a.drop(columns=nonEssential_columns)\n",
        "    y_2a_output=df_2a[output_columns]\n",
        "\n",
        "    # nut_phase 2b dataset (n=42)\n",
        "    df_2b=df[3]\n",
        "    df_2b=df_2b[df_2b[\"Phase_2b\"] > 0.0]  # creates a df with only Phase_2b values\n",
        "    X_2b_input=df_2b.drop(columns=nonEssential_columns)\n",
        "    y_2b_output=df_2b[output_columns]\n",
        "\n",
        "    # nut_phase 3 dataset (n=19)\n",
        "    df_3=df[4]\n",
        "    df_3=df_3[df_3[\"Phase_3\"] > 0.0]  # creates a df with only Phase_3 values\n",
        "    X_3_input=df_3.drop(columns=nonEssential_columns)\n",
        "    y_3_output=df_3[output_columns]\n",
        "\n",
        "    # nut_phase 4 dataset (n=7)\n",
        "    df_4=df[5]\n",
        "    df_4=df_4[df_4[\"Phase_4\"] > 0.0]  # creates a df with only Phase_4 values\n",
        "    X_4_input=df_4.drop(columns=nonEssential_columns)\n",
        "    y_4_output=df_4[output_columns]\n",
        "\n",
        "    inputValues=listMultiAppend(X_1a_input, X_1b_input, X_2a_input, X_2b_input, X_3_input, X_4_input)\n",
        "    outputValues=listMultiAppend(y_1a_output, y_1b_output, y_2a_output, y_2b_output, y_3_output, y_4_output)\n",
        "\n",
        "    # # package all subsets\n",
        "    allInputOutputValues=listMultiAppend(inputValues, outputValues)\n",
        "\n",
        "    return(allInputOutputValues)\n"
      ],
      "metadata": {
        "id": "YuMQIE7j_Fci"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "**Split Dataframe(s) Into Train & Test Sets Functions**\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "HxqNrJb6cD1a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def splitDataFrameIntoTrainTestSets(dataframeToSplit, shuffleDataFrame):\n",
        "    '''Splits a dataframe into train and test subsets without separating by nutritional phase.\n",
        "    Returns train, test dataframes'''\n",
        "\n",
        "    #Variables\n",
        "    df=pd.DataFrame()\n",
        "    trainSets=[]\n",
        "    testSets=[]\n",
        "    nutPhaseSubsets=[]\n",
        "    toShuffle=shuffleDataFrame\n",
        "\n",
        "    # Copy the dataframe\n",
        "    df=dataframeToSplit\n",
        "\n",
        "    # Columns to omit from dataframe (43 features are retained representing inputs for each nut_phase)\n",
        "    input_columns = [\"rec_num\", \"sample_id\", \"nut_phase\", \"Phase_1a\",\n",
        "                     \"Phase_1b\",\"Phase_2a\", \"Phase_2b\",\"Phase_3\", \"Phase_4\"]\n",
        "    output_columns = [\"Phase_1a\",\"Phase_1b\",\"Phase_2a\", \"Phase_2b\",\n",
        "                      \"Phase_3\", \"Phase_4\"]\n",
        "\n",
        "    X_input=dataframeToSplit.drop(columns=input_columns)\n",
        "    y_output=dataframeToSplit[output_columns]\n",
        "\n",
        "    X_train_fullset, X_test_fullset, y_train_fullset, y_test_fullset = train_test_split(X_input, y_output, test_size=0.30, random_state=8,  shuffle=toShuffle)\n",
        "\n",
        "    trainSets=listMultiAppend(X_train_fullset, y_train_fullset)\n",
        "    testSets=listMultiAppend(X_test_fullset, y_test_fullset)\n",
        "    dataSets=listMultiAppend(trainSets, testSets)\n",
        "\n",
        "    return(dataSets)\n"
      ],
      "metadata": {
        "id": "msWRtWQzWFZw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def splitNutPhaseSeparatedDataFrameIntoTrainTestSets(nutPhaseSeparatedDataFrames, shuffleDataFrames):\n",
        "    '''Splits dataframe into train and test subsets separating dataset by nutritional phase.\n",
        "    Returns a 2D array of train, test dataframes\n",
        "    [trainSets[X-train[...], y_train[...]], testSets[X-test[...], y_test[...]]]'''\n",
        "\n",
        "    #--- SPLIT THE DATA SUBSETS INTO A TRAINING SET AND A TEST SET --------------------\n",
        "    # dataset order --> X_train, X_test, y_train, y_test\n",
        "\n",
        "    # Variables\n",
        "    X_train=[]\n",
        "    y_train=[]\n",
        "    X_test=[]\n",
        "    y_test=[]\n",
        "    trainSets=[]\n",
        "    testSets=[]\n",
        "    toShuffle=shuffleDataFrames\n",
        "\n",
        "    X_input=nutPhaseSeparatedDataFrames[0]    # extract the train sets from the 2D array of data\n",
        "    y_output=nutPhaseSeparatedDataFrames[1]     # extract the test sets from the 2D array of data\n",
        "\n",
        "    # Get data subsets separated by nutritional phase\n",
        "\n",
        "    # Phase_1a: split nut_phase 1a dataset into a training and a testing set (n=11)\n",
        "    X_1a_train, X_1a_test, y_1a_train, y_1a_test = train_test_split(X_input[0], y_output[0], test_size=0.30, random_state=8,  shuffle=toShuffle)\n",
        "\n",
        "    # Phase_1b: split nut_phase 1b dataset into a training and a testing set (n=12)\n",
        "    X_1b_train, X_1b_test, y_1b_train, y_1b_test = train_test_split(X_input[1], y_output[1], test_size=0.30, random_state=8,  shuffle=toShuffle)\n",
        "\n",
        "    # Phase_2a: split nut_phase 2a dataset into a training and a testing set (n=10)\n",
        "    X_2a_train, X_2a_test, y_2a_train, y_2a_test = train_test_split(X_input[2], y_output[2], test_size=0.30, random_state=8,  shuffle=toShuffle)\n",
        "\n",
        "    # Phase_2b: split nut_phase 2b dataset into a training and a testing set (n=42)\n",
        "    X_2b_train, X_2b_test, y_2b_train, y_2b_test = train_test_split(X_input[3], y_output[3], test_size=0.30, random_state=8,  shuffle=toShuffle)\n",
        "\n",
        "    # Phase_3: split nut_phase 3 dataset into a training and a testing set (n=19)\n",
        "    X_3_train, X_3_test, y_3_train, y_3_test = train_test_split(X_input[4], y_output[4], test_size=0.30, random_state=8,  shuffle=toShuffle)\n",
        "\n",
        "    # Phase_4: split nut_phase 4 dataset into a training and a testing set (n=7)\n",
        "    X_4_train, X_4_test, y_4_train, y_4_test = train_test_split(X_input[5], y_output[5], test_size=0.30, random_state=8,  shuffle=toShuffle)\n",
        "\n",
        "    X_train=listMultiAppend(X_1a_train, X_1b_train, X_2a_train, X_2b_train, X_3_train, X_4_train)\n",
        "    y_train=listMultiAppend(y_1a_train, y_1b_train, y_2a_train, y_2b_train, y_3_train, y_4_train)\n",
        "\n",
        "    X_test=listMultiAppend(X_1a_test, X_1b_test, X_2a_test, X_2b_test, X_3_test, X_4_test)\n",
        "    y_test=listMultiAppend(y_1a_test, y_1b_test, y_2a_test, y_2b_test, y_3_test, y_4_test)\n",
        "\n",
        "    trainTestSets=listMultiAppend(X_train, y_train, X_test, y_test)    # 2D array\n",
        "\n",
        "    return(trainTestSets)\n"
      ],
      "metadata": {
        "id": "k0hgk9-1GmKH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def multiFractionSplitNutPhaseSeparatedDataFrameTrainTestSets(nutPhaseSeparatedDataFrames, shuffleDataFrames, minTestFraction, maxTestFraction, fractionIncrement):\n",
        "    '''Performs a multifraction split of nut_phase separated dataframe into train and test subsets separating them by nutritional phase.\n",
        "    Returns a 2D array of dataframes'''\n",
        "\n",
        "    #--- SPLIT THE DATA SUBSETS INTO A TRAINING SET AND A TEST SET --------------------\n",
        "    # dataset order --> X_train, X_test, y_train, y_test\n",
        "\n",
        "    # Variables\n",
        "    X_train=[]\n",
        "    y_train=[]\n",
        "    X_test=[]\n",
        "    y_test=[]\n",
        "    trainSets=[]\n",
        "    testSets=[]\n",
        "    trainTestDatasets=[]\n",
        "    adjTestFractions=[]\n",
        "    toShuffle=shuffleDataFrames\n",
        "    i=minTestFraction\n",
        "\n",
        "    testFractions=list(range(minTestFraction, maxTestFraction, fractionIncrement))\n",
        "    X_input=nutPhaseSeparatedDataFrames[0]      # extract the train sets from the 2D array of data\n",
        "    y_output=nutPhaseSeparatedDataFrames[1]     # extract the test sets from the 2D array of data\n",
        "\n",
        "    for fraction in testFractions:\n",
        "        fraction=float(fraction/100)\n",
        "        adjTestFractions.append(fraction)\n",
        "\n",
        "\n",
        "    # While loop\n",
        "    for testFraction in adjTestFractions:\n",
        "\n",
        "        # Phase_1a: split nut_phase 1a dataset into a training and a testing set (n=11)\n",
        "        X_1a_train, X_1a_test, y_1a_train, y_1a_test = train_test_split(X_input[0], y_output[0], test_size=testFraction, random_state=8,  shuffle=toShuffle)\n",
        "\n",
        "        # Phase_1b: split nut_phase 1b dataset into a training and a testing set (n=12)\n",
        "        X_1b_train, X_1b_test, y_1b_train, y_1b_test = train_test_split(X_input[1], y_output[1], test_size=testFraction, random_state=8,  shuffle=toShuffle)\n",
        "\n",
        "        # Phase_2a: split nut_phase 2a dataset into a training and a testing set (n=10)\n",
        "        X_2a_train, X_2a_test, y_2a_train, y_2a_test = train_test_split(X_input[2], y_output[2], test_size=testFraction, random_state=8,  shuffle=toShuffle)\n",
        "\n",
        "        # Phase_2b: split nut_phase 2b dataset into a training and a testing set (n=42)\n",
        "        X_2b_train, X_2b_test, y_2b_train, y_2b_test = train_test_split(X_input[3], y_output[3], test_size=testFraction, random_state=8,  shuffle=toShuffle)\n",
        "\n",
        "        # Phase_3: split nut_phase 3 dataset into a training and a testing set (n=19)\n",
        "        X_3_train, X_3_test, y_3_train, y_3_test = train_test_split(X_input[4], y_output[4], test_size=testFraction, random_state=8,  shuffle=toShuffle)\n",
        "\n",
        "        # Phase_4: split nut_phase 4 dataset into a training and a testing set (n=7)\n",
        "        X_4_train, X_4_test, y_4_train, y_4_test = train_test_split(X_input[5], y_output[5], test_size=testFraction, random_state=8,  shuffle=toShuffle)\n",
        "\n",
        "        # Append all sets to a list\n",
        "        X_train=listMultiAppend(X_1a_train, X_1b_train, X_2a_train, X_2b_train, X_3_train, X_4_train)\n",
        "        y_train=listMultiAppend(y_1a_train, y_1b_train, y_2a_train, y_2b_train, y_3_train, y_4_train)\n",
        "\n",
        "        X_test=listMultiAppend(X_1a_test, X_1b_test, X_2a_test, X_2b_test, X_3_test, X_4_test)\n",
        "        y_test=listMultiAppend(y_1a_test, y_1b_test, y_2a_test, y_2b_test, y_3_test, y_4_test)\n",
        "\n",
        "        trainTestDatasets=listMultiAppend(X_train, y_train, X_test, y_test)\n",
        "\n",
        "    return(trainTestDatasets)\n"
      ],
      "metadata": {
        "id": "dzgBmFbBdMfC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def k_FoldCrossValidationTrainTestSplit(dataframeToSplit):\n",
        "    ''' '''\n",
        "    # Variables\n",
        "\n",
        "    # Reference Columns\n",
        "    nonEssential_columns = [\"rec_num\", \"sample_id\", \"nut_phase\"]    # arbitrary columns to exclude from the dataframe to be used for Hold-out Cross-validation Stratified Sampling (HCSS)\n",
        "    output_columns = [\"Phase_1a\",\"Phase_1b\",\"Phase_2a\", \"Phase_2b\",\"Phase_3\", \"Phase_4\"]    # these are also referred to as \"target\" columns\n",
        "\n",
        "    # Define the input(x-values) and output (y-values) datasets\n",
        "    X_dataset = dataframeToSplit.drop(columns=nonEssential_columns)\n",
        "    y_dataset = dataframeToSplit[output_columns]\n"
      ],
      "metadata": {
        "id": "WZi16OJn__7w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "####################################################################\n",
        "#-------------------------------------------------------------------\n",
        "#    TEST FOR \"k_FoldCrossValidationTrainTestSplit()\" FUNCTION\n",
        "#-------------------------------------------------------------------\n",
        "####################################################################\n",
        "\n",
        "# Read CSV datafile and convert to a Pandas Dataframe object\n",
        "filename=\"nut_phase_questionnaire_data_fullset.csv\"\n",
        "df=pd.read_csv(filename)\n",
        "\n",
        "k_FoldCrossValidationTrainTestSplit(df)\n"
      ],
      "metadata": {
        "id": "KgXJBxJyALv_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "**Shuffle Dataframes Functions**\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "O0AoVqGyN7Bh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def shuffleDataFrame(datasetToShuffle):\n",
        "    '''Shuffles records in a dataframe and returns the shuffled dataframe'''\n",
        "\n",
        "    # Variables\n",
        "    shuffledIndeces=[]\n",
        "\n",
        "    numRecords = (len(datasetToShuffle))  #numberOfRecords\n",
        "    shuffledIndeces = list(range(0, numRecords))  # creates a number list of length equivalent to dataset size\n",
        "    random.shuffle(shuffledIndeces)   # shuffles the \"shuffledIndeces\" list to use in shuffling datasets by index\n",
        "\n",
        "    tempDataset = []                  # temporary list object for storing the shuffled dataset\n",
        "    shuffledDataset = []              # variable for storing and returning the shuffled datasets\n",
        "\n",
        "\n",
        "    # # Loop for assembling a list of shuffled data by shuffled index\n",
        "    for index in shuffledIndeces:\n",
        "        tempDataset.append(datasetToShuffle.iloc[index])  # appends shuffled element to temp list object\n",
        "\n",
        "    # # Convert lists to pandas dataframes\n",
        "    shuffledDataFrame = pd.DataFrame(tempDataset)\n",
        "\n",
        "    shuffledIndeces=[]\n",
        "\n",
        "    return (shuffledDataFrame)    # returns shuffled dataframe\n"
      ],
      "metadata": {
        "id": "22ESW488716h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "####################################################################\n",
        "#-------------------------------------------------------------------\n",
        "#            TEST FOR \"shuffleDataFrame()\" FUNCTION\n",
        "#-------------------------------------------------------------------\n",
        "####################################################################\n",
        "\n",
        "# Variables\n",
        "filename=\"nut_phase_questionnaire_data_fullset.csv\"\n",
        "\n",
        "#--- METHOD CALLS ------------------------------------------\n",
        "# Read CSV datafile and convert to a Pandas Dataframe object\n",
        "df=readCSVFile(filename)\n",
        "df2=pd.DataFrame(df)\n",
        "\n",
        "splitData=splitDataFrameByNutPhase(df2)\n",
        "shuffledDataFrame=shuffleDataFrame(splitData[0])\n",
        "#shuffledDataFrame\n",
        "print(splitData[0])\n",
        "print(shuffledDataFrame)\n"
      ],
      "metadata": {
        "id": "_6XJ6uW6AbEe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def shuffleDataFrames(X_dataframeToShuffle, y_dataframeToShuffle, numRecords):\n",
        "    '''Function to shuffle train or test datasets'''\n",
        "\n",
        "    dataset_size = numRecords   # might not need the (-1)\n",
        "    shuffledIndeces = list(range(0, dataset_size))  # creates a number list of length equivalent to dataset size\n",
        "    random.shuffle(shuffledIndeces)   # shuffles the \"shuffledIndeces\" list to use in shuffling datasets by index\n",
        "\n",
        "    X_dataframe = X_dataframeToShuffle    # assigns X_datasetToShuffle to a function-level variable for processing\n",
        "    y_dataframe = y_dataframeToShuffle    # assigns y_datasetToShuffle to a function-level variable for processing\n",
        "    X_temp = []                       # temporary list object for storing shuffled dataset of X-values being assembled\n",
        "    y_temp = []                       # temporary list object for storing shuffled dataset of y-values being assembled\n",
        "    shuffledDataFrames = []           # variable for returning shuffled datasets\n",
        "\n",
        "    # Loop for assembling lists of shuffled data by shuffled index\n",
        "    for index in shuffledIndeces:\n",
        "        X_temp.append(X_dataframe[index])  # appends X_dataset element to X_temp list object\n",
        "        y_temp.append(y_dataframe[index])  # appends y_dataset element to y_temp list object\n",
        "\n",
        "\n",
        "    # Convert lists to pandas dataframes\n",
        "    X_dataframe = pd.DataFrame(X_temp)\n",
        "    y_dataframe = pd.DataFrame(y_temp)\n",
        "\n",
        "    # Multi-append \"X_dataset\" and \"y_dataset\" dataframe to \"shuffledDataFrames\" list\n",
        "    shuffledDataFrames=listMultiAppend(X_dataframe, y_dataframe)\n",
        "\n",
        "    return (shuffledDataFrames)    # returns shuffled dataframes\n"
      ],
      "metadata": {
        "id": "skxdD8hpHGAX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def shufflePWSNutPhasePartitionedDataFrames(nutPhasePartitionedDFToShuffle):\n",
        "    '''(OPTIONAL): Shuffles a dataset. Note: Use only with datasets separated by nutritional phase'''\n",
        "\n",
        "    # Variables\n",
        "    shuffledDataFrame=[]\n",
        "    shuffledDataFrames=[]\n",
        "    i=0\n",
        "\n",
        "    dataFramesToShuffle=nutPhasePartitionedDFToShuffle\n",
        "\n",
        "    for dataFrame in dataFramesToShuffle:\n",
        "      shuffledDataFrame=shuffleDataFrame(dataFrame)\n",
        "      shuffledDataFrames.append(shuffledDataFrame)\n",
        "\n",
        "    return(shuffledDataFrames)\n"
      ],
      "metadata": {
        "id": "C7_gHMALnZ-Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "####################################################################\n",
        "#-------------------------------------------------------------------\n",
        "#            TEST FOR \"shuffleDataFrames()\" FUNCTION\n",
        "#-------------------------------------------------------------------\n",
        "####################################################################\n",
        "\n",
        "# Variables\n",
        "filename=\"nut_phase_questionnaire_data_fullset.csv\"\n",
        "\n",
        "#--- METHOD CALLS ------------------------------------------\n",
        "# Read CSV datafile and convert to a Pandas Dataframe object\n",
        "df=readCSVFile(filename)\n",
        "df2=pd.DataFrame(df)\n",
        "\n",
        "shuffledDataFrame=shuffleDataFrame(df2)\n",
        "shuffledDataFrame\n",
        "\n",
        "filename=\"nut_phase_questionnaire_data_fullset.csv\"\n",
        "modelTrainingDatasets=[]\n",
        "\n",
        "\n",
        "#--- METHOD CALLS ------------------------------------------\n",
        "# Read CSV datafile and convert to a Pandas Dataframe object\n",
        "dataFrame=readCSVFile(filename)\n",
        "\n",
        "# Partition the dataset by nut_phase\n",
        "splitDataFrame = splitDataFrameByNutPhase(dataFrame)\n",
        "\n",
        "# Shuffle the nut+phase partitioned dataframes\n",
        "shuffledDataFrames=shufflePWSNutPhasePartitionedDataFrames(splitDataFrame)\n",
        "\n",
        "# Merge the partitioned datasets\n",
        "mergedDataFrames=mergeDataFramesRow_Wise(shuffledDataFrames)\n",
        "\n",
        "# Split merged dataset into input and output dataframes by nutritional phase for Train/Test split\n",
        "datasetForTrainTestSplit=splitDataFrameIntoInputOutputByNutPhase(mergedDataFrames)\n",
        "\n"
      ],
      "metadata": {
        "id": "jVKtULn9KBU7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def shuffleDataFrames2(X_dataframeToShuffle, y_dataframeToShuffle, numRecords):\n",
        "    '''Shuffles records in two dataframes, 'X' and 'y', and returns the shuffled datasets'''\n",
        "\n",
        "    numRecords = numRecords   # might not need the (-1)\n",
        "    shuffledIndeces = list(range(0, numRecords))  # creates a number list of length equivalent to dataset size\n",
        "    random.shuffle(shuffledIndeces)   # shuffles the \"shuffledIndeces\" list to use in shuffling datasets by index\n",
        "\n",
        "    X_dataframe = X_dataframeToShuffle    # assigns X_datasetToShuffle to a function-level variable for processing\n",
        "    y_dataframe = y_dataframeToShuffle    # assigns y_datasetToShuffle to a function-level variable for processing\n",
        "    X_temp = []                           # temporary list object for storing shuffled dataset of X-values being assembled\n",
        "    y_temp = []                           # temporary list object for storing shuffled dataset of y-values being assembled\n",
        "    shuffledDataFrames = []               # variable for returning shuffled datasets\n",
        "\n",
        "    # Loop for assembling lists of shuffled data by shuffled index\n",
        "    for index in shuffledIndeces:\n",
        "        X_temp.append(X_dataframe.iloc[index])  # appends X_dataset element to X_temp list object\n",
        "        y_temp.append(y_dataframe.iloc[index])  # appends y_dataset element to y_temp list object\n",
        "\n",
        "\n",
        "    # Convert lists to pandas dataframes\n",
        "    X_dataframe = pd.DataFrame(X_temp)\n",
        "    y_dataframe = pd.DataFrame(y_temp)\n",
        "\n",
        "    # Multi-append \"X_dataset\" and \"y_dataset\" dataframe to \"shuffledDataFrames\" list\n",
        "    shuffledDataFrames=listMultiAppend(X_dataframe, y_dataframe)\n",
        "\n",
        "    return (shuffledDataFrames)    # returns shuffled dataframes\n"
      ],
      "metadata": {
        "id": "wC0IuY63OPv1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "####################################################################\n",
        "#-------------------------------------------------------------------\n",
        "#  TEST FOR \"shufflePWSNutPhasePartitionedDataFrames()\" FUNCTION\n",
        "#-------------------------------------------------------------------\n",
        "####################################################################\n",
        "\n",
        "# Variables\n",
        "filename=\"nut_phase_questionnaire_data_fullset.csv\"\n",
        "\n",
        "#--- METHOD CALLS ------------------------------------------\n",
        "# Read CSV datafile and convert to a Pandas Dataframe object\n",
        "df=readCSVFile(filename)\n",
        "df2=pd.DataFrame(df)\n",
        "\n",
        "splitData=splitDataFrameByNutPhase(df2)\n",
        "\n",
        "splitData[1][5]\n"
      ],
      "metadata": {
        "id": "8dzyMstzOrny"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "**Merge Dataframes Functions**\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "y_YF2tdZN0uK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def mergePWSNutPhaseSubsets(datasetsToMerge):\n",
        "    '''Takes in a 2D array. Merges PWS nut_phase split data subsets into a full train and a full test sets.\n",
        "    Returns a 3D array of train, test dataframes:\n",
        "    [mergedTrainSets[X_trainfullset [...], y_train_fullset], ...],  mergedTestSets[X_testfullset [...], y_test_fullset], ...]'''\n",
        "\n",
        "    #--- MERGE NUT_PHASE TRAIN AND TEST SUBSETS TO CREATE FULL TRAIN AND TEST DATASETS --------------------\n",
        "    # dataset order --> X_train, X_test, y_train, y_test\n",
        "\n",
        "    # Variables\n",
        "    mergedTrainSets=[]\n",
        "    mergedTestSets=[]\n",
        "    mergedDataSets=[]\n",
        "\n",
        "    X_trainSets=datasetsToMerge[0]\n",
        "    y_trainSets=datasetsToMerge[1]\n",
        "\n",
        "    X_testSets=datasetsToMerge[2]\n",
        "    y_testSets=datasetsToMerge[3]\n",
        "\n",
        "    # X_train dataset [.....1a.......,.....1b.......,.....2a........,.....2b..........,....3........,.......4.......]\n",
        "    X_train_subsets = [X_trainSets[0],X_trainSets[1], X_trainSets[2], X_trainSets[3], X_trainSets[4], X_trainSets[5]]\n",
        "    X_train_fullset = pd.concat(X_train_subsets, axis=0) #.values.astype(\"float32\")    # concatenate frames row-wise (axis=0)\n",
        "\n",
        "    # X_test dataset\n",
        "    X_test_subsets = [ X_testSets[0],  X_testSets[1],  X_testSets[2],  X_testSets[3],  X_testSets[4],  X_testSets[5]]\n",
        "    X_test_fullset = pd.concat(X_test_subsets, axis=0) #.values.astype(\"float32\")      # concatenate frames row-wise (axis=0)\n",
        "\n",
        "    # y_train dataset\n",
        "    y_train_subsets = [y_trainSets[0], y_trainSets[1], y_trainSets[2], y_trainSets[3], y_trainSets[4], y_trainSets[5]]\n",
        "    y_train_fullset = pd.concat(y_train_subsets, axis=0) #.values.astype(\"float32\")    # concatenate frames row-wise (axis=0)\n",
        "\n",
        "    # y_test dataset\n",
        "    y_test_subsets = [y_testSets[0], y_testSets[1], y_testSets[2], y_testSets[3], y_testSets[4], y_testSets[5]]\n",
        "    y_test_fullset = pd.concat(y_test_subsets, axis=0) #.values.astype(\"float32\")       # concatenate frames row-wise (axis=0)\n",
        "\n",
        "    mergedTrainSets=listMultiAppend(X_train_fullset, y_train_fullset)\n",
        "    mergedTestSets=listMultiAppend(X_test_fullset, y_test_fullset)\n",
        "    mergedDataSets=listMultiAppend(mergedTrainSets, mergedTestSets)\n",
        "\n",
        "    #--- DATASET SHAPE (ARRAY DIMENSIONS) --------------------\n",
        "    # print(\"X_train array dimensions: \" + str(X_train_fullset.shape) + \"; (rows, cols)\" + \"\\n\" +\n",
        "    #       \"y_train array dimensions: \" + str(y_train_fullset.shape) + \"; (rows, cols)\")\n",
        "\n",
        "    return(mergedDataSets)\n"
      ],
      "metadata": {
        "id": "PNYnyzjGGgh3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def mergeDataFramesRow_Wise(dataFramesToMerge):\n",
        "    ''' '''\n",
        "\n",
        "    dataFrames=[]\n",
        "\n",
        "    for dataFrame in dataFramesToMerge:\n",
        "        dataFrames.append(dataFrame)\n",
        "\n",
        "    mergedDataFrames=pd.concat(dataFrames, axis=0)\n",
        "\n",
        "    return(mergedDataFrames)\n"
      ],
      "metadata": {
        "id": "bk8IWtubaDWy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "**Construct a Sequential Neural Network (SNN) Functions**\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "gpLyla3HH33o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def constructSequentialNeuralNetwork(numFeatures, nodesPerLayer):\n",
        "    '''Constructs a Sequential Neural Network (SNN) of dimensions 43*([86]*6)*6 => (input nodes*([height in nodes]*depth in layers)*output nodes)'''\n",
        "\n",
        "    # Exhaustive analysis of height vs depth concluded that 6-8 hidden\n",
        "    # layers, 86-172 nodes in height, are sufficient to produce maximal\n",
        "    # performance for the current dataset having 43 features (fields).\n",
        "    # TensorFlow TensorSpecs dimensions and data types of the model input => [None,43], float32. \"None\" indicates unknown batch size (number of records).\n",
        "\n",
        "    # Variables\n",
        "    nodes=nodesPerLayer\n",
        "\n",
        "    tf.random.set_seed(42)\n",
        "    model = Sequential()   # create an instance of a Sequential object\n",
        "\n",
        "    # -- Add input layer -----------------------------------------\n",
        "    model.add(tf.keras.layers.Input(shape=(numFeatures, )))              # input layer (follows matrix mult (A X B), B is m rows and n cols, thus A is k rows and m cols (where k=#records in the dataset and m=#input cols))\n",
        "\n",
        "    # -- Add hidden layers ---------------------------------------\n",
        "    model.add(tf.keras.layers.Dense(nodes, activation=\"relu\"))    # first hidden layer   (1)\n",
        "    model.add(tf.keras.layers.Dense(nodes, activation=\"relu\"))    # second hidden layer  (2)\n",
        "    model.add(tf.keras.layers.Dense(nodes, activation=\"relu\"))    # third hidden layer   (3)\n",
        "    model.add(tf.keras.layers.Dense(nodes, activation=\"relu\"))    # fourth hidden layer  (4)\n",
        "    model.add(tf.keras.layers.Dense(nodes, activation=\"relu\"))    # fifth hidden layer   (5)\n",
        "    model.add(tf.keras.layers.Dense(nodes, activation=\"relu\"))    # sixth hidden layer   (6)\n",
        "    # model.add(tf.keras.layers.Dense(nodes, activation=\"relu\"))    # seventh hidden layer (7)\n",
        "    # model.add(tf.keras.layers.Dense(nodes, activation=\"relu\"))    # eight hidden layer   (8)\n",
        "    # model.add(tf.keras.layers.Dense(nodes, activation=\"relu\"))    # seventh hidden layer (9)\n",
        "    # model.add(tf.keras.layers.Dense(nodes, activation=\"relu\"))    # eight hidden layer   (10)\n",
        "\n",
        "\n",
        "    # -- Add output layer ----------------------------------------\n",
        "    model.add(tf.keras.layers.Dense(6, activation=\"softmax\"))   # output layer.  Softmax converts a vector (array) of values into a probability distribution with a range (0,1).\n",
        "    tempModel=model\n",
        "    return(model)\n"
      ],
      "metadata": {
        "id": "PrKb-BHrf6H_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def constructSequentialNeuralNetworkWithDropout(numFeatures):\n",
        "    '''Constructs a Sequential Neural Network (SNN) of dimensions 43/[86]*4/6'''\n",
        "\n",
        "    tf.random.set_seed(42)\n",
        "    model = Sequential()   # create an instance of a Sequential object\n",
        "\n",
        "    # -- Add input layer -----------------------------------------\n",
        "    model.add(tf.keras.layers.Input(shape=(numFeatures, )))             # input layer (follows matrix mult (A X B), B is m rows and n cols, thus A is k rows and m cols (where k=#records in the dataset and m=#input cols))\n",
        "\n",
        "    # -- Add hidden layers ---------------------------------------\n",
        "    model.add(tf.keras.layers.Dropout(0.4))                    # Dropout helps protect the model from memorizing or \"overfitting\" the training data\n",
        "    model.add(tf.keras.layers.Dense(86, activation=\"relu\"))    # first hidden layer (1)\n",
        "    # model.add(tf.keras.layers.Dropout(0.2))                  # Dropout helps protect the model from memorizing or \"overfitting\" the training data\n",
        "    model.add(tf.keras.layers.Dense(86, activation=\"relu\"))    # second hidden layer (2)\n",
        "    model.add(tf.keras.layers.Dropout(0.2))                    # Dropout helps protect the model from memorizing or \"overfitting\" the training data\n",
        "    model.add(tf.keras.layers.Dense(86, activation=\"relu\"))    # third hidden layer (3)\n",
        "    # model.add(tf.keras.layers.Dropout(0.2))                  # Dropout helps protect the model from memorizing or \"overfitting\" the training data\n",
        "    model.add(tf.keras.layers.Dense(86, activation=\"relu\"))    # fourth hidden layer (4)\n",
        "    model.add(tf.keras.layers.Dropout(0.2))                    # Dropout helps protect the model from memorizing or \"overfitting\" the training data\n",
        "    # model.add(tf.keras.layers.Dropout(0.2))                  # Dropout helps protect the model from memorizing or \"overfitting\" the training data\n",
        "\n",
        "    # -- Add output layer ----------------------------------------\n",
        "    model.add(tf.keras.layers.Dense(6, activation=\"softmax\"))  # output layer.  Softmax converts a vector (array) of values into a probability distribution with a range (0,1).\n",
        "\n",
        "    return(model)\n"
      ],
      "metadata": {
        "id": "J6b5c4fREsQC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def constructCustomSequentialNeuralNetwork():\n",
        "    '''Constructs a custom Sequential Neural Network (SNN) by prompting user for network hypervariables to use in implementation'''\n",
        "\n",
        "    # Variables\n",
        "    rndSeed=42\n",
        "    numRecords=1\n",
        "    dimHiddenLayers=numRecords*2\n",
        "    layerNum=1\n",
        "    numHiddenLayers=1\n",
        "\n",
        "    # Initialize a Sequential Neural Network Object (SNN)\n",
        "    tf.random.set_seed(rndSeed)\n",
        "    model=Sequential()\n",
        "\n",
        "    # Prompt user for Sequential Neural Network hyperparameters\n",
        "    rndSeed=int(input(\"Please enter a seed number (default seed is: 42): \"))\n",
        "    numRecords=int(input(\"Please enter the number of records in the dataset: \"))\n",
        "    numHiddenLayers=int(input(\"Please enter the number of hidden layers desired: \"))\n",
        "    dimHiddenLayers=input(\"Please enter the dimension of the hidden layers, otherwise the default value of [(input variables) X 2] will be used: \")\n",
        "    print(\"Please note that for now the 'relu' and 'softmax' activation functions will be used\")\n",
        "\n",
        "    # Case if user does not enter a number for dimension of hidden layer(s)\n",
        "    if (dimHiddenLayers==''):\n",
        "        dimHiddenLayers=numRecords*2\n",
        "\n",
        "    # --- Build the model ---\n",
        "    # Add input layer\n",
        "    model.add(tf.keras.layers.Input(shape=(numRecords, )))    # input layer (follows matrix mult (A X B), B is m rows and n cols, thus A is k rows and m cols (where k=#records in the dataset and m=#input cols))\n",
        "\n",
        "    # Add hidden layers to model\n",
        "    while (layerNum < numHiddenLayers+1):\n",
        "        model.add(tf.keras.layers.Dense(dimHiddenLayers, activation=\"relu\"))    # N-hidden layer\n",
        "        layerNum+=1    # increment loop index by one(1)\n",
        "    layerNum=0    # reset loop index to zero(0)\n",
        "\n",
        "    # Add output layer\n",
        "    model.add(tf.keras.layers.Dense(6, activation=\"softmax\"))   # output layer.  Softmax converts a vector of values to a probability distribution with a range (0,1).\n",
        "\n",
        "    # Display summary of model structure\n",
        "    displayNeuralNetworkSummary(model)\n",
        "\n",
        "    # Display graphical structure of model\n",
        "    displayNeuralNetworkStructure(model)\n",
        "\n",
        "    return(model)    # return the model\n"
      ],
      "metadata": {
        "id": "_c9KjTPd-6jW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "####################################################################\n",
        "#-------------------------------------------------------------------\n",
        "#  TEST FOR \"constructCustomSequentialNeuralNetwork()\" FUNCTION\n",
        "#-------------------------------------------------------------------\n",
        "####################################################################\n",
        "\n",
        "constructCustomSequentialNeuralNetwork()\n"
      ],
      "metadata": {
        "id": "MEB1eRFZG1CH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "**Compile Sequential Neural Network Functions**\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "-DfY9oyKBnlw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def compileNeuralNetwork(model):\n",
        "    '''Compiles a neural network'''\n",
        "\n",
        "    #model.compile(loss=\"categorical_crossentropy\", optimizer=keras.optimizers.SGD(learning_rate=0.01, momentum=0.9, nesterov=True), metrics=[\"accuracy\"])\n",
        "    model.compile(keras.optimizers.Adam(learning_rate=0.001), loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])\n",
        "\n",
        "    return(model)\n"
      ],
      "metadata": {
        "id": "ivMP-lh6gAQu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "**Summarize and Display Structure of Sequential Neural Network Functions**\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "xFIvjiHTBuKI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def displayNeuralNetworkSummary(model):\n",
        "    '''Displays a visual representation of the neural network'''\n",
        "\n",
        "    print(model.summary())\n"
      ],
      "metadata": {
        "id": "xjXGVe-OgES2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def displayNeuralNetworkStructure(model):\n",
        "    '''Displays a visual representation of the neural network'''\n",
        "\n",
        "    networkStructure=plot_model(model, to_file='model_plot.jpg', show_shapes=True, show_layer_names=True)\n",
        "\n",
        "    return(networkStructure)\n"
      ],
      "metadata": {
        "id": "3vSiJSYl5TPK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "**Model Training Functions**\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "7TX-9hxIIfLK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Training Functions:**\n",
        "1. trainNeuralNetwork()\n",
        "2. trainNeuralNetwork_N_Fold()\n",
        "3. trainNeuralNetworkUsing_K_FoldCrossValidation()"
      ],
      "metadata": {
        "id": "A24Q_SkujCVr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def trainNeuralNetwork(model, trainingDatasets, epochs, userDefinedVerbose):\n",
        "    '''Trains a neural network'''\n",
        "\n",
        "    # Extract the Train datasets\n",
        "    X_train=trainingDatasets[0][0]\n",
        "    y_train=trainingDatasets[0][1]\n",
        "\n",
        "    # Extract the Test datasets\n",
        "    X_test=trainingDatasets[1][0]\n",
        "    y_test=trainingDatasets[1][1]\n",
        "\n",
        "    # Train the model\n",
        "    model.fit(X_train, y_train, epochs=epochs, batch_size=6, verbose=userDefinedVerbose, validation_data=(X_test,y_test)) #, callbacks=callbacks_list)\n",
        "\n",
        "    return(model)\n"
      ],
      "metadata": {
        "id": "hRO7uKIggGuW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def trainNeuralNetwork_TEST(model, trainingDatasets, userDefinedVerbose):\n",
        "    '''Trains a neural network'''\n",
        "\n",
        "    # Extract the Train and Test sets from passed list\n",
        "    modelTrainSets=trainingDatasets[0]    # Extracts the train sets\n",
        "    modelTestSets=trainingDatasets[1]     # Extracts the test sets\n",
        "\n",
        "    # Train the model\n",
        "    history=model.fit(modelTrainSets[0], modelTrainSets[1], epochs=75, batch_size=12, verbose=userDefinedVerbose, validation_data=(modelTestSets[0], modelTestSets[1])) #, callbacks=callbacks_list)\n",
        "    model=history\n",
        "    print(\"History:\")\n",
        "    print(history.history.keys())\n",
        "    return(model)\n"
      ],
      "metadata": {
        "id": "87CeJ6PosqR-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def trainNeuralNetwork_N_Fold(model, trainingDatasets, trainingIterations):\n",
        "    '''Trains a neural network N-times and returns a list of models'''\n",
        "\n",
        "    # Variables\n",
        "    iterationCount=0    # while loop index\n",
        "    trainedModels=[]    # list for storing trained models\n",
        "\n",
        "    # Extract the Train and Test sets from passed list\n",
        "    modelTrainSets=trainingDatasets[0]    # Extracts the train sets\n",
        "    modelTestSets=trainingDatasets[1]     # Extracts the test sets\n",
        "\n",
        "    while (iterationCount < trainingIterations):\n",
        "        # Train the model\n",
        "        model.fit(modelTrainSets[0], modelTrainSets[1], epochs=100, batch_size=10, verbose=2, validation_data=(modelTestSets[0], modelTestSets[1]), callbacks=[history])\n",
        "        trainedModels.append(model)\n",
        "        iterationCount +=1\n",
        "\n",
        "    # reset loop index to zero(0)\n",
        "    iterationCount=0\n",
        "\n",
        "    return(trainedModels)\n"
      ],
      "metadata": {
        "id": "zhPaiyKg6AWH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def k_FoldCrossValidation(model, trainingDatasets, splits):\n",
        "    '''Takes in a pre-constructed model and performs k-fold cross-validation on the referenced dataset'''\n",
        "\n",
        "    # Variables\n",
        "    foldNum=1\n",
        "    accuracyPerFold=[]\n",
        "\n",
        "    # Prepare the features and target datasets\n",
        "    nonEssential_columns = [\"rec_num\", \"sample_id\", \"nut_phase\", \"Phase_1a\",\"Phase_1b\",\"Phase_2a\", \"Phase_2b\",\"Phase_3\", \"Phase_4\"]\n",
        "    output_columns = [\"Phase_1a\",\"Phase_1b\",\"Phase_2a\", \"Phase_2b\",\"Phase_3\", \"Phase_4\"]\n",
        "\n",
        "    X_input=trainingDatasets.drop(columns=nonEssential_columns)\n",
        "    y_output=trainingDatasets[output_columns]\n",
        "\n",
        "    X=X_input.to_numpy()  # cannot use datasets in Pandas dataframe format. Must convert to numpy arrays.\n",
        "    y= y_output.to_numpy()\n",
        "\n",
        "    # Initialize an instance of a KFold object\n",
        "    cv = KFold(n_splits=splits, shuffle=True, random_state=7)\n",
        "\n",
        "    for train, test in cv.split(X, y):\n",
        "        X_train=X[train]\n",
        "        y_train=y[train]\n",
        "\n",
        "        X_test=X[test]\n",
        "        y_test=y[test]\n",
        "\n",
        "        # fit data to model\n",
        "        history=model.fit(X_train, y_train, validation_data=(X_test, y_test), batch_size=7, epochs=50, verbose=0)\n",
        "\n",
        "        # Evaluate the model - store accuracy score into list\n",
        "        scores=model.evaluate(X_test, y_test, verbose=0)\n",
        "        accuracyPerFold.append(scores[1]*100)\n",
        "        foldNum +=1\n",
        "\n",
        "    x=1\n",
        "\n",
        "    # Display model accuracy values for each split\n",
        "    avg=sum(accuracyPerFold)/len(accuracyPerFold)\n",
        "    for acc in accuracyPerFold:\n",
        "        print(\"Model's accuracy for fold \" + str(x) + \" \" + str(int(acc)) + \"%\")\n",
        "        x+=1\n",
        "    print(\"Average accuracy score: \" + str(int(avg)) + \"%\")\n",
        "\n",
        "    return(models)\n"
      ],
      "metadata": {
        "id": "6BWcwX6XCG9J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "######################################################################\n",
        "#---------------------------------------------------------------------\n",
        "# TEST FOR \"trainNeuralNetworkUsing_K_FoldCrossValidation()\" FUNCTION\n",
        "#---------------------------------------------------------------------\n",
        "######################################################################\n",
        "\n",
        "# Variables\n",
        "model=tempModel\n",
        "filename=\"nut_phase_questionnaire_data_augmented; 09-05-2023.csv\"\n",
        "splits=5\n",
        "\n",
        "#--- METHOD CALLS ------------------------------------------\n",
        "# Read CSV datafile and convert to a Pandas Dataframe object\n",
        "dataFrame=readCSVFile(filename)\n",
        "models = k_FoldCrossValidation(model, dataFrame, splits)\n",
        "\n",
        "print(\"\\n\"+\"Number of models: \"+str(len(models)))\n"
      ],
      "metadata": {
        "id": "IeV78SAkCtvq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def k_FoldCrossValidationWithNeuralNetworkConstruct(trainingDatasets):\n",
        "    '''Creates a model to use for k-fold cross-validation'''\n",
        "\n",
        "    # Variables\n",
        "    foldNum=1\n",
        "    accuracyPerFold=[]\n",
        "\n",
        "    # Reference columns\n",
        "    nonEssential_columns = [\"rec_num\", \"sample_id\", \"nut_phase\", \"Phase_1a\",\"Phase_1b\",\"Phase_2a\", \"Phase_2b\",\"Phase_3\", \"Phase_4\"]\n",
        "    output_columns = [\"Phase_1a\",\"Phase_1b\",\"Phase_2a\", \"Phase_2b\",\"Phase_3\", \"Phase_4\"]\n",
        "\n",
        "    X_input=trainingDatasets.drop(columns=nonEssential_columns)\n",
        "    y_output=trainingDatasets[output_columns]\n",
        "\n",
        "    X=X_input.to_numpy()  # cannot use X_input in dataframe format. Must convert to numpy array.\n",
        "    y= y_output.to_numpy()\n",
        "\n",
        "    # Initialize an instance of a KFold object\n",
        "    cv = KFold(n_splits=5, shuffle=True, random_state=7)\n",
        "\n",
        "    # construct SNN model - do this inside the for loop to train a new model at each iteration of the loop\n",
        "    model=constructSequentialNeuralNetwork()\n",
        "\n",
        "    # compile model\n",
        "    model=compileNeuralNetwork(model)\n",
        "\n",
        "    for train, test in cv.split(X, y):\n",
        "        X_train=X[train]\n",
        "        y_train=y[train]\n",
        "\n",
        "        X_test=X[test]\n",
        "        y_test=y[test]\n",
        "\n",
        "        # Clear the current model\n",
        "        tf.keras.backend.clear_session()    # clears all previously created models from memory\n",
        "\n",
        "        # fit data to model\n",
        "        history=model.fit(X_train, y_train, validation_data=(X_test, y_test), batch_size=7, epochs=50, verbose=0)\n",
        "\n",
        "        # save the model to list object\n",
        "        # model.save(\"model_fold_\"+str(foldNum)+\"_\"+str(datetime.now())+\".h5\")\n",
        "\n",
        "        # Evaluate the model - store accuracy score into list\n",
        "        scores=model.evaluate(X_test, y_test, verbose=0)\n",
        "        accuracyPerFold.append(scores[1]*100)\n",
        "        foldNum +=1\n",
        "\n",
        "    x=1\n",
        "\n",
        "    # Display model accuracy values for each split\n",
        "    avg=sum(accuracyPerFold)/len(accuracyPerFold)\n",
        "    for acc in accuracyPerFold:\n",
        "        print(\"Model's accuracy for fold \" + str(x) + \" \" + str(int(acc)) + \"%\")\n",
        "        x+=1\n",
        "    print(\"Average accuracy score: \" + str(int(avg)) + \"%\")\n",
        "\n",
        "    return(models)\n"
      ],
      "metadata": {
        "id": "PT8JeixjC2XJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "####################################################################\n",
        "#----------------------------------------------------------------------\n",
        "# TEST FOR \"trainNeuralNetworkUsing_K_FoldCrossValidation_2()\" FUNCTION\n",
        "#-------------------------------------------------------------------\n",
        "####################################################################\n",
        "\n",
        "# Variables\n",
        "filename=\"nut_phase_questionnaire_data_augmented; 09-05-2023.csv\"\n",
        "\n",
        "#--- METHOD CALLS ------------------------------------------\n",
        "# Read CSV datafile and convert to a Pandas Dataframe object\n",
        "dataFrame=readCSVFile(filename)\n",
        "models = k_FoldCrossValidationWithNeuralNetworkConstruct(dataFrame)\n",
        "\n",
        "print(\"\\n\"+\"Number of models: \"+str(len(models)))\n"
      ],
      "metadata": {
        "id": "uTf5WVB5C9UR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "\n",
        "**Model Validation Functions**\n",
        "\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "Q-8PzhL-v-gv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Validation Functions:**\n",
        "1. verifyModelHasLearned()\n",
        "2. evaluateModel()\n",
        "3. getModelsPerformance()"
      ],
      "metadata": {
        "id": "iks230lOibIC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def verifyModelHasLearned(model):\n",
        "    '''Displays edge weights of model. If model did not learn, no weights will be displayed'''\n",
        "\n",
        "    # Display weights\n",
        "    model.weights\n"
      ],
      "metadata": {
        "id": "X3Joc5MGge52"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluateModel(model, trainTestDataFrames):\n",
        "    '''Displays model performance metrics.  Returns evaluation scores as a 2D array object'''\n",
        "\n",
        "    # Variables\n",
        "    scores=[]\n",
        "\n",
        "    # Train datasets\n",
        "    modelTrainSets=trainTestDataFrames[0]    # Extracts only the test sets\n",
        "    X_train=modelTrainSets[0]    # Extracts the X_test set (input values) from \"modelTestSets\"\n",
        "    y_train=modelTrainSets[1]    # Extracts the y_test set (output values) from \"modelTestSets\"\n",
        "\n",
        "    # Test datasets\n",
        "    modelTestSets=trainTestDataFrames[1]    # Extracts only the test sets\n",
        "    X_test=modelTestSets[0]    # Extracts the X_test set (input values) from \"modelTestSets\"\n",
        "    y_test=modelTestSets[1]    # Extracts the y_test set (output values) from \"modelTestSets\"\n",
        "\n",
        "    # Train and Test dataset evaluation scores\n",
        "    # [loss [0], accuracy [1]]\n",
        "    trainScores = model.evaluate(X_train, y_train, verbose=2)  # returns a list with two values, final loss function value and the model's accuracy on the train data\n",
        "    testScores = model.evaluate(X_test, y_test, verbose=2)  # returns a list with two values, final loss function value and the model's accuracy on the test data\n",
        "\n",
        "\n",
        "    # Print Train dataset evaluation scores\n",
        "    print(\"\\n\"+\"Model evaluation results:\")\n",
        "    print(\"Train dataset loss:\", trainScores[0])\n",
        "    print(\"Train dataset accuracy:\", trainScores[1])\n",
        "\n",
        "    # Print Test dataset evaluation scores\n",
        "    print(\"Validation dataset loss:\", testScores[0])\n",
        "    print(\"Validation dataset accuracy:\", testScores[1])\n",
        "\n",
        "    # Store evaluation scores into \"scores\" list object to be returned\n",
        "    scores=listMultiAppend(trainScores, testScores)\n",
        "\n",
        "    return(scores)    # returns a 2D array object\n"
      ],
      "metadata": {
        "id": "HScE7-G_Debp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def getModelsPerformance(model, trainingDatasets):\n",
        "    '''Displays model performance metrics.  Returns evaluation scores as a 2D array object'''\n",
        "\n",
        "    # Variables\n",
        "    scores=[]\n",
        "\n",
        "    # Extract the Train datasets\n",
        "    X_train=trainingDatasets[0][0]\n",
        "    y_train=trainingDatasets[0][1]\n",
        "\n",
        "    # Extracts the Test sets\n",
        "    X_test=trainingDatasets[1][0]\n",
        "    y_test=trainingDatasets[1][1]\n",
        "\n",
        "\n",
        "    # Train and Test dataset evaluation scores\n",
        "    # [loss [0], accuracy [1]]\n",
        "    trainScores = model.evaluate(X_train, y_train, verbose=2)  # returns a list with two values, final loss function value and the model's accuracy on the train data\n",
        "    testScores = model.evaluate(X_test, y_test, verbose=2)  # returns a list with two values, final loss function value and the model's accuracy on the test data\n",
        "\n",
        "    # Store evaluation scores into \"scores\" list object to be returned\n",
        "    scores=listMultiAppend(trainScores, testScores)\n",
        "\n",
        "    return(scores)    # returns a 2D array object\n"
      ],
      "metadata": {
        "id": "K-0nBin1oxeK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def multiClassConfusionMatrix(model, dataframe):\n",
        "    '''Function constructs a confusion matrix for a multi-class model'''\n",
        "\n",
        "    # Variables\n",
        "    df=dataframe\n",
        "\n",
        "    # Make prediction (model, dataset, shuffle_dataset (True or False))\n",
        "    results=predictPWSNutPhase(model, df, False)\n",
        "\n",
        "    # Create the multiclass confusion matrix\n",
        "    confusionMatrix=pd.crosstab(results.Predicted, results.Actual)\n",
        "    fig=plt.figure(figsize=(17,5))\n",
        "    ax=plt.subplot(121)\n",
        "    ax.set_title(\"PWS Nut_Phase Questionnaire Deep Neural Network\")\n",
        "    sn.heatmap(confusionMatrix, annot=True, cmap=\"Reds\")\n",
        "\n",
        "    # Calculate model's overall accuracy\n",
        "    numRecords=confusionMatrix.sum().sum()\n",
        "    accuracy=round((np.diag(confusionMatrix).sum()/numRecords*100),2)\n",
        "\n",
        "    report=pd.DataFrame(metrics.classification_report(results.Actual, results.Predicted, output_dict=True))\n",
        "    report=report.transpose()\n",
        "    report.columns=[\"precision\", \"recall\", \"f1-score\", \"no_records\"]\n",
        "\n",
        "    return(report)\n"
      ],
      "metadata": {
        "id": "jYnfN2BizR2l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "**Model Utility Functions**\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "FvQnv4bnIvRk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Utility Functions:**\n",
        "1. calcAvgEdgeWeights()\n",
        "2. saveModelEdgeWeights()\n",
        "3. saveTrainedModel()"
      ],
      "metadata": {
        "id": "TTtmaFWQh9SF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def calcAvgEdgeWeights(modelsToAvg):\n",
        "    '''Calculates the stochastic average edge weights for a model from weights obtained by performing multiple training iterations'''\n",
        "    # Function variables\n",
        "    i=0                       # inner while loop iteration index\n",
        "    j=0                       # outer while loop iteration index\n",
        "    numModels=0               # stores the number of models passed by the function call\n",
        "    arraysToAverage=[]        # stores the arrays to be averaged\n",
        "    calcAvgWeights=[]         # stores the calculated average of the sum of weights for a given neural layer\n",
        "    modelWeights=[]           # stores the weights of each model\n",
        "    avgModelWeights=[]        # stores all of the calculated average edge weights to be returned by the function\n",
        "\n",
        "\n",
        "    # Collect the weights of each model and store in the \"modelWeights\" list\n",
        "    for model in modelsToAvg:\n",
        "      modelWeights.append(model.weights)\n",
        "\n",
        "    # Obtain the number of models to be averaged\n",
        "    numModels=len(modelsToAvg)\n",
        "\n",
        "    # Obtain the number of weight arrays in the models (all should have the same number of arrays.  Will selecte the first model \"[0]\" to obtain this number)\n",
        "    numWeightArrays=len(modelWeights[0])\n",
        "\n",
        "    if numModels > 1:  # if the number of models passed > 1, then calculate  model's average edge weights\n",
        "        # Calculate the model's average edge weights\n",
        "        while j < numWeightArrays:\n",
        "            while i < numModels:\n",
        "                arraysToAverage.append(modelWeights[i][j])            # populate the \"arraysToAverage\" list with arrays to be averaged.  Note loop indeces, [i = model][j = array]\n",
        "                calcAvgWeights=np.mean(arraysToAverage, axis=0)       # calculate the average of the array values for the current model array\n",
        "                i=i+1    # increment inner loop counter 'i' by one(1)\n",
        "\n",
        "            avgModelWeights.append(calcAvgWeights)    # append calculated average weights to final averages list to be returned\n",
        "            arraysToAverage=[]     # clear the arraysToCount list\n",
        "            calcAvgWeights=[]      # clear the avgVals list\n",
        "            i=0      # reset inner loop counter 'i' to zero(0)\n",
        "            j=j+1    # increment outer loop counter 'j' by one(1)\n",
        "\n",
        "        # Models summary\n",
        "        print(\"Number of models: \" + str(numModels))\n",
        "        print(\"Weight arrays per model: \" + str(numWeightArrays) + \"\\n\")\n",
        "        print(avgModelWeights)\n",
        "\n",
        "        # Reset all variables\n",
        "        numModels=0\n",
        "        arraysToAverage=[]\n",
        "        calcAvgWeights=[]\n",
        "        modelWeights=[]\n",
        "        avgModelWeights=[]\n",
        "        i=0\n",
        "        j=0\n",
        "\n",
        "        # Return model's average edge weights\n",
        "        return(avgModelWeights)\n"
      ],
      "metadata": {
        "id": "zwBNVlNzGznt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def saveModelEdgeWeights(model, filename=\"pws_qnr_dnn_model.weights.h5\"):\n",
        "    ''' Saves all layer weights to a file. Target file name must end in \".weight.h5\" '''\n",
        "\n",
        "    # Save the model's weights\n",
        "    model_weigths = model.save_weights(model, filename)\n"
      ],
      "metadata": {
        "id": "Kth9zZOIvZj_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from datetime import date\n",
        "\n",
        "def saveTrainedModelAsKeras(model, filename=\"sequential_neural_network_models\"):\n",
        "    '''Saves the trained model to a file, catalogued by date and time'''\n",
        "\n",
        "\n",
        "    # Ask user if he/she would like to save the trained model\n",
        "    response=input(\"Would you like to save this model? (y/n)\")\n",
        "\n",
        "    # Get today's date\n",
        "    todaysDate = str(date.today()).replace(\"-\", \"\")\n",
        "\n",
        "    # Create file name to save model\n",
        "    filename = \"pws_qnr_dnn_model_\" + todaysDate + \".keras\"\n",
        "\n",
        "    # Action to perform based on user's response\n",
        "    if (response=='y'):    # Save the model\n",
        "        model = model.save(filename)\n",
        "\n",
        "    elif (response=='n'):  # Do not save the model\n",
        "      print(\"Ok, the model will not be saved\")\n"
      ],
      "metadata": {
        "id": "bgBc6g265N6q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# *** DEPRECATED ***\n",
        "def saveTrainedModelAsOnnx(model):\n",
        "    '''Saves the trained model as a ONNX object'''\n",
        "\n",
        "    # Ask user if he/she would like to save the trained model\n",
        "    response=input(\"Would you like to save this model? (y/n)\")\n",
        "\n",
        "    # Action to perform based on user's response\n",
        "    if (response=='y'):    # Save the model\n",
        "\n",
        "        # Define the dimensions and datatype of the TensorFlow model's input: [None,43], float32. \"None\" indicates unknown batch size (number of records).\n",
        "        input_signature = [tf.TensorSpec([None, 43], tf.float32, name='x')]   # the tf.TensorSpec => num of input values and num of output values, nothing to do with the number of records!\n",
        "\n",
        "        onnx_model, _ = tf2onnx.convert.from_keras(model, input_signature, opset=13)\n",
        "        onnx.save(onnx_model, \"pws_qnr_dnn_model.onnx\")\n",
        "\n",
        "    elif (response=='n'):  # Do not save the model\n",
        "        print(\"Ok, the model will not be saved\")\n"
      ],
      "metadata": {
        "id": "-Wd5ECmQY_r-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "\n",
        "**Model Optimization Functions**\n",
        "\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "0ovbYPblERzy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Hyperparameter Tuning:**\n",
        "* Batch size and training epochs\n",
        "* Optimization algorithms\n",
        "* Learning rate and momentum\n",
        "* Network weight initialization\n",
        "* Activation functions\n",
        "* Dropout regularization\n",
        "* Number of neurons in the hidden layer\n",
        "\n",
        "**Optimization Functions:**\n",
        "1. optimizeTestSizeSplit()\n",
        "2. optimizeDimensionsOfSequentialNeuralNetwork()\n",
        "3. optimizeDepthOfSequentialNeuralNetwork()\n",
        "4. gridSearchCVOfSNN()"
      ],
      "metadata": {
        "id": "nHVhzrLi7AXa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def optimizeTestSizeSplit(trainTestDataFrames, shuffleTrainTestSplit):\n",
        "    '''Function to find optimal Train-Test Split.  Must pass a merged dataframe when using with data split by nut_phase!'''\n",
        "\n",
        "    # Function variables\n",
        "    lossVals=[]                         # List for storing loop loss score values\n",
        "    accuracyVals=[]                     # List for storing loop accuracy values\n",
        "    testBatchSize=list(range(5,55,5))   # list of range percent of data to use as test in training of model. Range 5-80%, incremented by 5%\n",
        "    models=[]                           # stores models generated\n",
        "    splitFraction=[]                    # stores percent values assayed\n",
        "    AvgEdgeWeights=[]                   # stores calculated average edge weights received from \"calcAvgEdgeWeights()\" function call\n",
        "    toShuffle=shuffleTrainTestSplit     # stores boolean value declaring whether or not to shuffle dataset during Train Test split\n",
        "    i=0                                 # Loop index\n",
        "\n",
        "    # Columns to remove or select from original dataframe to create working dataframes for model training\n",
        "    nonEssential_columns = [\"rec_num\",\"sample_id\",\"nut_phase\",\"Phase_1a\",\"Phase_1b\",\"Phase_2a\",\"Phase_2b\",\"Phase_3\",\"Phase_4\"]\n",
        "    output_columns = [\"Phase_1a\",\"Phase_1b\",\"Phase_2a\",\"Phase_2b\",\"Phase_3\",\"Phase_4\"]\n",
        "\n",
        "    # Shuffle the dataset\n",
        "    df_sh = shuffle(trainTestDataFrames)\n",
        "\n",
        "    # Create dataframes for training SNN model\n",
        "    X_input=df_sh.drop(columns=nonEssential_columns)\n",
        "    y_output=df_sh[output_columns]\n",
        "\n",
        "\n",
        "    # While loop to iterate through various test_size fractions to find optimal percentage\n",
        "    # producing the lowest loss score and highest accuracy value\n",
        "    while i < len(testBatchSize):\n",
        "        testSize=testBatchSize[i]/100\n",
        "\n",
        "        # Split the dataset so that test dataset size =\"testSize\", where \"testSize\" is given by the percentage of the current loop iteration\n",
        "        X_train, X_test, y_train, y_test = train_test_split(X_input, y_output, test_size=testSize, shuffle=True)\n",
        "\n",
        "        # Train the model\n",
        "        model.fit(X_train, y_train, epochs=100, batch_size=10, verbose=0, validation_data=(X_test, y_test)) #, callbacks=callbacks_list)\n",
        "\n",
        "        # Evaluate the model\n",
        "        score = model.evaluate(X_test, y_test, verbose=0)\n",
        "\n",
        "        # Store current loop iteration loss and accuracy values in their respective lists\n",
        "        lossVals.append(score[0])\n",
        "        accuracyVals.append(score[1])\n",
        "\n",
        "        # Save the models\n",
        "        models.append(model)\n",
        "\n",
        "        # Display current iteration percent value\n",
        "        print(str(int(testSize*100)) + \"%\")\n",
        "\n",
        "        # increment loop index by one (1)\n",
        "        i=i+1\n",
        "\n",
        "        # AvgEdgeWeights=calcAvgEdgeWeights(models)\n",
        "\n",
        "    # Display accuracy and loss scores\n",
        "    # plt.plot(splitFraction, lossVals)\n",
        "    # plt.xlabel(\"Test Fraction (%)\")\n",
        "    # plt.ylabel(\"Loss Function Score\")\n",
        "    # plt.show()\n",
        "\n",
        "    # plt.plot(splitFraction, accuracyVals)\n",
        "    # plt.xlabel(\"Test Fraction (%)\")\n",
        "    # plt.ylabel(\"Accuracy Score\")\n",
        "    # plt.show()\n",
        "\n"
      ],
      "metadata": {
        "id": "ECLiUsyIG_H9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def arrangeArraysInColumnMajorOrder(inputArray):\n",
        "    '''Function inputs an array of 2D lists (3D object) and returns the averages of the lists within each 2D list block'''\n",
        "\n",
        "    listsToAvg=inputArray\n",
        "    block=0\n",
        "    arrStruct=0\n",
        "    elem=0\n",
        "    tempList=[]\n",
        "    elements=[]\n",
        "    groupedElements=[]\n",
        "\n",
        "    while block<len(listsToAvg): #n=2\n",
        "        while elem<len(listsToAvg[0][0]): #n=8\n",
        "            while arrStruct<len(listsToAvg[0]): #n=3\n",
        "                tempList.append(listsToAvg[block][arrStruct][elem])\n",
        "                arrStruct+=1\n",
        "            elements.append(tempList)\n",
        "            arrStruct=0\n",
        "            elem+=1\n",
        "            tempList=[]\n",
        "        groupedElements.append(elements)\n",
        "        elements=[]\n",
        "        elem=0\n",
        "        block+=1\n",
        "    block=0\n",
        "\n",
        "    i=0\n",
        "\n",
        "    return(groupedElements)\n"
      ],
      "metadata": {
        "id": "WEm8QmdUWky6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "testList1=[[1,2,3,4,5,6,7,8],\n",
        "          [6,7,8,9,10,11,12,13],\n",
        "          [3,5,8,2,10,15,23,17],\n",
        "          [3,5,8,2,10,15,23,17]]\n",
        "\n",
        "testList2=[[5,6,7,8,9,10,12,13],\n",
        "          [11,12,13,14,15,16,17,19],\n",
        "          [11,2,13,14,15,16,17,19],\n",
        "          [3,5,8,2,10,15,23,17]]\n",
        "\n",
        "testList3=[[15,6,27,8,9,10,12,13],\n",
        "          [11,2,13,14,15,16,17,19],\n",
        "          [3,5,8,2,10,15,23,17],\n",
        "          [23,5,28,2,10,15,23,17]]\n",
        "\n",
        "testList4=[[15,6,27,8,9,10,12,13],\n",
        "          [11,2,13,14,15,16,17,19],\n",
        "          [3,5,8,2,10,15,23,17],\n",
        "          [23,5,28,2,10,15,23,17]]\n",
        "\n",
        "testList5=[[15,6,27,8,9,10,12,13],\n",
        "          [11,2,13,14,15,16,17,19],\n",
        "          [3,5,8,2,10,15,23,17],\n",
        "          [23,5,28,2,10,15,23,17]]\n",
        "\n",
        "allLists=listMultiAppend(testList1, testList2, testList3, testList4, testList5)\n",
        "\n",
        "print(arrangeArraysInColumnMajorOrder(allLists))\n",
        "\n"
      ],
      "metadata": {
        "id": "nR867pK_3CJy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b2294277-2586-46eb-d3e2-2dcf9a82dc9b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[[1, 6, 3, 3], [2, 7, 5, 5], [3, 8, 8, 8], [4, 9, 2, 2], [5, 10, 10, 10], [6, 11, 15, 15], [7, 12, 23, 23], [8, 13, 17, 17]], [[5, 11, 11, 3], [6, 12, 2, 5], [7, 13, 13, 8], [8, 14, 14, 2], [9, 15, 15, 10], [10, 16, 16, 15], [12, 17, 17, 23], [13, 19, 19, 17]], [[15, 11, 3, 23], [6, 2, 5, 5], [27, 13, 8, 28], [8, 14, 2, 2], [9, 15, 10, 10], [10, 16, 15, 15], [12, 17, 23, 23], [13, 19, 17, 17]], [[15, 11, 3, 23], [6, 2, 5, 5], [27, 13, 8, 28], [8, 14, 2, 2], [9, 15, 10, 10], [10, 16, 15, 15], [12, 17, 23, 23], [13, 19, 17, 17]], [[15, 11, 3, 23], [6, 2, 5, 5], [27, 13, 8, 28], [8, 14, 2, 2], [9, 15, 10, 10], [10, 16, 15, 15], [12, 17, 23, 23], [13, 19, 17, 17]]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def optimizeDimensionsOfSequentialNeuralNetwork(dataset, maxLayerWidth, networkDepth, numInputFeatures, layersToAddPerIteration, epochs, shuffleTrainTestSplit, userDefinedVerbose):\n",
        "    '''Function finds the optimal depth (layers) and width (nodes/layer) of a sequential neural network for a given dataset'''\n",
        "\n",
        "    # Variables\n",
        "    rndSeed=42                            # Sets the random seed. \"If neither the global seed nor the operation-level seed is set: A randomly picked seed is used for this op. - TensorFlow\"\n",
        "    modelNum=1\n",
        "    layerNum=0\n",
        "    sum=0.0\n",
        "    modelVersion=0.1\n",
        "    constructNum=1\n",
        "    nodes=numInputFeatures\n",
        "    maxNumNodes=maxLayerWidth\n",
        "    numNodes=numInputFeatures\n",
        "    verbose=userDefinedVerbose\n",
        "    toShuffle=shuffleTrainTestSplit\n",
        "    trainingDataset=dataset\n",
        "    maxNumLayers=networkDepth\n",
        "    layerCount=layersToAddPerIteration\n",
        "\n",
        "    # Lists\n",
        "    totalNumNodes=[]\n",
        "    performanceScores=[]\n",
        "    trainAccuracyToLossRatios=[]\n",
        "    testAccuracyToLossRatios=[]\n",
        "    modelDimensions=[]\n",
        "    trainToTestLossRatio=[]\n",
        "    tempTrainLossScores=[]\n",
        "    tempTestLossScores=[]\n",
        "    tempTrainAccScores=[]\n",
        "    tempTestAccScores=[]\n",
        "    resultsTrainLoss=[]\n",
        "    resultsTestLoss=[]\n",
        "    resultsTrainAcc=[]\n",
        "    resultsTestAcc=[]\n",
        "    numModels=[]\n",
        "    models=[]\n",
        "    results=[]\n",
        "    trainLossSum=[]\n",
        "    testLossSum=[]\n",
        "    trainStdDevs=[]\n",
        "    testStdDevs=[]\n",
        "    trainTestLossRatio=[]\n",
        "    sse=[]\n",
        "\n",
        "\n",
        "    # Begin recording total runtime\n",
        "    startTime = time()\n",
        "\n",
        "    while(layerCount < networkDepth):\n",
        "        while(numNodes < maxLayerWidth):\n",
        "            # Clears all previously created models from memory\n",
        "            tf.keras.backend.clear_session()\n",
        "\n",
        "            # Set random seed. \"If neither the global seed nor the operation-level seed is set: A randomly picked seed is used for this op. - TensorFlow\"\n",
        "            tf.random.set_seed(rndSeed)\n",
        "\n",
        "            # Create an instance of a Sequential Neural Network object\n",
        "            model=Sequential()\n",
        "\n",
        "            # Build the Neural Network\n",
        "            # -- Add Input layer --------------------------------------------\n",
        "            model.add(tf.keras.layers.Input(shape=(numInputFeatures, )))         # input layer (follows matrix mult (A X B), B is m rows and n cols, thus A is k rows and m cols (where k=#records in the dataset and m=#input cols))\n",
        "\n",
        "            # -- Add Hidden layers ------------------------------------------\n",
        "            while(layerNum < layerCount):\n",
        "                model.add(tf.keras.layers.Dense(numNodes, activation=\"relu\"))\n",
        "                layerNum+=1\n",
        "            layerNum=0\n",
        "            numNodes+=numInputFeatures\n",
        "\n",
        "            # -- Add Output layer -------------------------------------------\n",
        "            model.add(tf.keras.layers.Dense(6, activation=\"softmax\"))        # output layer.  Softmax converts a vector of values to a probability distribution with a range (0,1).\n",
        "\n",
        "\n",
        "            # -- Compile the model ------------------------------------------\n",
        "            model=compileNeuralNetwork(model)\n",
        "\n",
        "            # -- Train the model --------------------------------------------\n",
        "            model=trainNeuralNetwork(model, trainingDataset, epochs, verbose)\n",
        "\n",
        "            # -- Evaluate model ---------------------------------------------\n",
        "            print(\"\\n\" + \"Model number: \" + str(modelNum+modelVersion) + \"\\n\" +\n",
        "                  \"Model construct number: \" + str(constructNum) + \"\\n\" +\n",
        "                  \"Model's dimensions (depth, height): \" + \"(\" + str(layerCount)\n",
        "                   + \", \" + str(numNodes-numInputFeatures)  + \")\")\n",
        "\n",
        "            performanceScores=getModelsPerformance(model, trainingDataset)\n",
        "\n",
        "\n",
        "            # -- Record loss and accuracy values ----------------------------\n",
        "            # Append loss scores to the \"lossScore\" list\n",
        "            # Train dataset loss --> [0][0]\n",
        "            # Test dataset loss --> [1][0]\n",
        "            tempTrainLossScores.append(performanceScores[0][0])\n",
        "            tempTestLossScores.append(performanceScores[1][0])\n",
        "\n",
        "            # Append validation accuracy value to the \"accuracyScore\" list\n",
        "            # Train dataset accuracy --> [0][1]\n",
        "            # Test dataset accuracy --> [1][1]\n",
        "            tempTrainAccScores.append(performanceScores[0][1])\n",
        "            tempTestAccScores.append(performanceScores[1][1])\n",
        "\n",
        "            # Store Results\n",
        "            if (modelVersion==0.5):\n",
        "                resultsTrainLoss.append(tempTrainLossScores)  # elements are listed in ascending order of network dimension\n",
        "                resultsTestLoss.append(tempTestLossScores)    # elements are listed in ascending order of network dimension\n",
        "                resultsTrainAcc.append(tempTrainAccScores)    # elements are listed in ascending order of network dimension\n",
        "                resultsTestAcc.append(tempTestAccScores)      # elements are listed in ascending order of network dimension\n",
        "                tempTrainLossScores=[]\n",
        "                tempTestLossScores=[]\n",
        "                tempTrainAccScores=[]\n",
        "                tempTestAccScores=[]\n",
        "\n",
        "            # Calculate the validation accuracy value to loss score ratio and append to  to the \"accuracyToLossRatio\" list\n",
        "            trainAccuracyToLossRatios.append(np.array(performanceScores[0][1])/np.array(performanceScores[0][0]))    # must convert list to numpy array to be able to perform calculations\n",
        "            testAccuracyToLossRatios.append(np.array(performanceScores[1][1])/np.array(performanceScores[1][0]))\n",
        "\n",
        "            # Train to test loss ratio calculation\n",
        "            trainToTestLossRatio.append(np.array(performanceScores[0][0])/np.array(performanceScores[1][0]))\n",
        "\n",
        "            # model.summary()\n",
        "            modelsDimensions=\"(\" + str(layerCount) + \", \" + str(numNodes-numInputFeatures)  + \")\"\n",
        "            modelDimensions.append(modelsDimensions)\n",
        "            modelVersion+=0.1\n",
        "            constructNum+=1\n",
        "        numModels.append(modelNum)\n",
        "        models.append(\"model_\"+str(modelNum)+\" (n=\"+str(layerCount)+\")\")\n",
        "        modelNum+=1\n",
        "        modelVersion=0.1\n",
        "        numNodes=numInputFeatures\n",
        "        layerCount+=2\n",
        "\n",
        "    results=listMultiAppend(resultsTrainLoss, resultsTestLoss, resultsTrainAcc, resultsTestAcc)\n",
        "    results=arrangeArraysInColumnMajorOrder(results)\n",
        "    numOfResults=[]\n",
        "    numOfResults=list(range(1, len(results[0])+1, 1))\n",
        "\n",
        "    i=0\n",
        "    j=0\n",
        "    sum=0.0\n",
        "\n",
        "    refList=results[0]\n",
        "\n",
        "    # # Add up train losses for each dimension\n",
        "    while (i < len(refList)):\n",
        "        while (j < len(refList[0])):\n",
        "            sum=sum+refList[i][j]\n",
        "            j+=1\n",
        "        trainLossSum.append(sum)\n",
        "        trainStdDevs.append(st.stdev(refList[i]))\n",
        "        sum=0\n",
        "        j=0\n",
        "        i+=1\n",
        "    j=0\n",
        "    i=0\n",
        "\n",
        "    refList=results[1]\n",
        "\n",
        "    # Add up test losses for each dimension\n",
        "    while (i < len(refList)):         # n=5\n",
        "        while (j < len(refList[1])):  # n=3\n",
        "            sum=sum+refList[i][j]\n",
        "            j+=1\n",
        "        testLossSum.append(sum)\n",
        "        testStdDevs.append(st.stdev(refList[i]))\n",
        "        sum=0\n",
        "        j=0\n",
        "        i+=1\n",
        "    j=0\n",
        "    i=0\n",
        "\n",
        "    # Calculate the sum of square error for test loss\n",
        "    for num in testLossSum:\n",
        "        sse.append(num**2/networkDepth)\n",
        "\n",
        "    # Calculate the difference between test and train loss (test > train)\n",
        "    # thus optimal performance occurs when test-train --> 0\n",
        "    while i < len(testLossSum):\n",
        "        trainTestLossRatio.append(trainLossSum[i]-testLossSum[i])\n",
        "        i+=1\n",
        "    i=0\n",
        "\n",
        "\n",
        "    # -- Plot SNN Dimensions Optimization Results ---------------------------\n",
        "    dims=['(n, '+str(nodes)+')', '(n, '+str(nodes*2)+')', '(n, '+str(nodes*3)+')', '(n, '+str(nodes*4)+')', '(n, '+str(nodes*5)+')']\n",
        "\n",
        "    # -- Plot train loss score ----------------------------------------------\n",
        "    plt.plot(dims, results[0])    # extract and plot Train loss scores\n",
        "    plt.xlabel(\"Network Dimension\")\n",
        "    plt.ylabel(\"Train Loss Scores\")\n",
        "    plt.legend(models, loc='best')\n",
        "    plt.figure(figsize=(10, 6))    # (width_size, height_size)\n",
        "    plt.show()\n",
        "\n",
        "    # -- Plot test loss score -----------------------------------------------\n",
        "    plt.plot(dims, results[1])    # extract and plot Train loss scores\n",
        "    plt.xlabel(\"Network Dimension\")\n",
        "    plt.ylabel(\"Test Loss Scores\")\n",
        "    plt.legend(models, loc='best')\n",
        "    plt.figure(figsize=(10, 6))    # (width_size, height_size)\n",
        "    plt.show()\n",
        "\n",
        "    # -- Plot train accuracy scores -----------------------------------------\n",
        "    # plt.plot(dims, results[2])    # extract and plot Train accuracy scores\n",
        "    # plt.xlabel(\"Network Dimension\")\n",
        "    # plt.ylabel(\"Train Accuracy Scores\")\n",
        "    # plt.legend(models, loc='best')\n",
        "    # plt.figure(figsize=(10, 6))    # (width_size, height_size)\n",
        "    # plt.show()\n",
        "\n",
        "    # -- Plot test accuracy scores ------------------------------------------\n",
        "    # plt.plot(dims, results[3])    # extract and plot Test accuracy scores\n",
        "    # plt.xlabel(\"Network Dimension\")\n",
        "    # plt.ylabel(\"Test Accuracy Scores\")\n",
        "    # plt.legend(models, loc='best')\n",
        "    # plt.figure(figsize=(10, 6))    # (width_size, height_size)\n",
        "    # plt.show()\n",
        "\n",
        "    # -- Plot sum of loss differences ------------------------------------------\n",
        "    plt.plot(dims, trainLossSum)   # extract and plot Test accuracy scores\n",
        "    plt.plot(dims, testLossSum)    # extract and plot Test accuracy scores\n",
        "    plt.xlabel(\"Network Dimension\")\n",
        "    plt.ylabel(\"Sum of Test Loss Differences\")\n",
        "    plt.legend([\"Train Loss Diff\", \"Test Loss Diff\"], loc='upper right')\n",
        "    plt.figure(figsize=(10, 6))    # (width_size, height_size)\n",
        "    plt.show()\n",
        "\n",
        "    # # -- Plot standard deviations of loss ------------------------------------------\n",
        "    # plt.plot(dims, trainStdDevs)   # extract and plot Test accuracy scores\n",
        "    # plt.plot(dims, testStdDevs)    # extract and plot Test accuracy scores\n",
        "    # plt.xlabel(\"Network Dimension\")\n",
        "    # plt.ylabel(\"Standard Deviation\")\n",
        "    # plt.legend([\"Train stdDev\", \"Test stdDev\"], loc='upper right')\n",
        "    # plt.figure(figsize=(10, 6))    # (width_size, height_size)\n",
        "    # plt.show()\n",
        "\n",
        "    # -- Plot SSE of test loss ------------------------------------------\n",
        "    plt.plot(dims, sse)   # extract and plot Test accuracy scores\n",
        "    plt.xlabel(\"Network Dimension\")\n",
        "    plt.ylabel(\"Sum of Squared Error (SSE) for Test Loss\")\n",
        "    plt.figure(figsize=(10, 6))    # (width_size, height_size)\n",
        "    plt.show()\n",
        "\n",
        "    # # -- Plot Train minusTest diff.  Optimal occurs when diff = 0 ------------\n",
        "    # plt.plot(dims, trainTestLossRatio)    # extract and plot Test accuracy scores\n",
        "    # plt.xlabel(\"Network Dimension\")\n",
        "    # plt.ylabel(\"Test minus Train Loss\")\n",
        "    # plt.figure(figsize=(10, 6))    # (width_size, height_size)\n",
        "    # plt.show()\n",
        "\n",
        "    # -- Print total run time -----------------------------------------------\n",
        "    print(\"\\n\" + \"Total run time: \" + str(int((time()-startTime)/60)) + \"mins\")\n",
        "\n",
        "    # -- Print Summary of Results -------------------------------------------\n",
        "    print(\"\\n\" + \"------------------- Results -----------------------\")\n",
        "    print(\"Training loss sums:\")\n",
        "    print(trainLossSum)\n",
        "    print(\"Test loss sums:\")\n",
        "    print(testLossSum)\n",
        "    print(\"Train-Test loss Difference:\")\n",
        "    print(np.subtract(testLossSum, trainLossSum))\n"
      ],
      "metadata": {
        "id": "7av4X0KeCvHK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#######################################################################\n",
        "#----------------------------------------------------------------------\n",
        "# TEST FOR \"optimizeSequentialNeuralNetworkDimensions()\" FUNCTION\n",
        "#----------------------------------------------------------------------\n",
        "#######################################################################\n",
        "\n",
        "# Variables\n",
        "filename1=\"snn_nut_phase_questionnaire_training_dataset_NO_age; 09-26-2023.csv\"\n",
        "filename2=\"snn_nut_phase_questionnaire_training_dataset_WITH_age; 09-26-2023.csv\"\n",
        "filename3=\"snn_nut_phase_questionnaire_norm_training_dataset_NO_age; 09-22-2023.csv\"\n",
        "filename4=\"snn_nut_phase_questionnaire_norm_training_dataset_WITH_age; 09-22-2023.csv\"\n",
        "toShuffle=True\n",
        "\n",
        "# Read CSV datafile and convert to a Pandas Dataframe object\n",
        "df=pd.DataFrame(readCSVFile(filename4))\n",
        "\n",
        "# PARTITION THE DATASET BY NUTRITIONAL PHASE\n",
        "splitDataFrame = splitDataFrameIntoInputOutputByNutPhase(df)\n",
        "\n",
        "# SPLIT INPUT AND OUTPUT DATAFRAMES INTO TRAIN, TEST SETS (NOTE: \"TRUE\" HYPERPARAMETER SELECTED FOR SHUFFLING OF DATASET DURING SPLIT PROCEDURE)\n",
        "modelTrainingDatasets = splitNutPhaseSeparatedDataFrameIntoTrainTestSets(splitDataFrame, True)\n",
        "\n",
        "# MERGE THE TRAIN AND TEST SETS, RESPECTIVELY\n",
        "mergedDatasets = mergePWSNutPhaseSubsets(modelTrainingDatasets)\n",
        "\n",
        "# Parameters: (dataset, maxLayerWidth(height), networkDepth(number of hidden layers(even #)+1), numInputFeatures, layersToAddPerIteration, epochs, shuffleTrainTestSplit, userDefinedVerbose)\n",
        "optimizeDimensionsOfSequentialNeuralNetwork(mergedDatasets, 374, 17, 44, 2, 30, False, 0)\n"
      ],
      "metadata": {
        "id": "EJ4J8ONyc5WZ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 390
        },
        "outputId": "33498bdd-55f6-4c46-c1b5-7d97ba71ffc2",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: 'snn_nut_phase_questionnaire_norm_training_dataset_WITH_age; 09-22-2023.csv'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-70-3df53134bb5d>\u001b[0m in \u001b[0;36m<cell line: 15>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;31m# Read CSV datafile and convert to a Pandas Dataframe object\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m \u001b[0mdf\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreadCSVFile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;31m# PARTITION THE DATASET BY NUTRITIONAL PHASE\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-25-c6e9a3ee94e7>\u001b[0m in \u001b[0;36mreadCSVFile\u001b[0;34m(datafile)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mreadCSVFile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdatafile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0;34m'''Function reads a csv file passed by the function caller and copies its contents into a pandas dataframe object'''\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdatafile\u001b[0m\u001b[0;34m)\u001b[0m    \u001b[0;31m# main dataframe\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0mdf2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m    \u001b[0;31m# working copy of the main data frame\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/util/_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    209\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    210\u001b[0m                     \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnew_arg_name\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnew_arg_value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 211\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    212\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    213\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/util/_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    329\u001b[0m                     \u001b[0mstacklevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfind_stack_level\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m                 )\n\u001b[0;32m--> 331\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    332\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    333\u001b[0m         \u001b[0;31m# error: \"Callable[[VarArg(Any), KwArg(Any)], Any]\" has no\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[1;32m    948\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    949\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 950\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    951\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    952\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    603\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    604\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 605\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    606\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    607\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1440\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1441\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1442\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1443\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1444\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1733\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1734\u001b[0m                     \u001b[0mmode\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m\"b\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1735\u001b[0;31m             self.handles = get_handle(\n\u001b[0m\u001b[1;32m   1736\u001b[0m                 \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1737\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    854\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    855\u001b[0m             \u001b[0;31m# Encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 856\u001b[0;31m             handle = open(\n\u001b[0m\u001b[1;32m    857\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    858\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'snn_nut_phase_questionnaire_norm_training_dataset_WITH_age; 09-22-2023.csv'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def optimizeDepthOfSequentialNeuralNetwork(dataset, maxLayerWidth, networkDepth, numInputFeatures, shuffleTrainTestSplit, userDefinedVerbose):\n",
        "    '''Function finds the optimal depth of a neural network needed for a given dataset'''\n",
        "\n",
        "    # Variables\n",
        "    rndSeed=42                            # sets the random seed. \"If neither the global seed nor the operation-level seed is set, a randomly picked seed is used for this op. - TensorFlow\"\n",
        "    hiddenLayerNum=1                      # hidden layer reference number\n",
        "    hiddenLayerCount=0                    # count of total number of hidden layers\n",
        "    numHiddenLayers=networkDepth\n",
        "    nodes=maxLayerWidth\n",
        "    trainingDataset=dataset\n",
        "    toShuffle=shuffleTrainTestSplit\n",
        "    verbose=userDefinedVerbose\n",
        "    performanceScores=[]\n",
        "    lossScores=[]\n",
        "    accuracyScores=[]\n",
        "    accuracyToLossRatios=[]\n",
        "    x_vals=list(range(1,networkDepth,1))\n",
        "\n",
        "    # Keywords:\n",
        "    # \"Input Layer: this represents the input variables, sometimes called the visible layer.\n",
        "    # Hidden Layers: these are the layers of nodes between the input and output layers. The network may contain one or more of these layers.\n",
        "    # Output Layer: the final layer of nodes. Produces the output variables.\n",
        "    # Size: number of nodes in the model.\n",
        "    # Width: number of nodes in a specific layer.\n",
        "    # Depth: number of layers in the neural network.\n",
        "    # Capacity: type or structure of functions that can be learned by the current network configuration.\n",
        "    # Architecture: the specific arrangement of the layers and nodes in the neural network.\"\"\n",
        "    # *** Note: choose depth over width ***\n",
        "\n",
        "    # Read CSV datafile and convert to a Pandas Dataframe object\n",
        "    df=pd.DataFrame(readCSVFile(trainingDataset))\n",
        "\n",
        "    # Shuffle the dataframe prior to splitting into Train and Test sets\n",
        "    shuffledDataFrame=shuffleDataFrame(df)\n",
        "\n",
        "    # Split the dataset into a Train set and a Test set\n",
        "    trainingDataset=splitDataFrameIntoTrainTestSets(shuffledDataFrame, toShuffle)\n",
        "\n",
        "\n",
        "    while (hiddenLayerNum < (len(x_vals)+1)):\n",
        "        tf.keras.backend.clear_session()    # clears all previously created models from memory\n",
        "\n",
        "        # Initialize a Sequential Neural Network Object (SNN)\n",
        "        tf.random.set_seed(rndSeed)    # \"If neither the global seed nor the operation-level seed is set: A randomly picked seed is used for this op. - TensorFlow\"\n",
        "        model=Sequential()    # create an instance of a Sequential Neural Network object\n",
        "\n",
        "        # --- Build the model ---\n",
        "        # Add input layer\n",
        "        model.add(tf.keras.layers.Input(shape=(numInputFeatures, )))    # input layer (follows matrix mult (A X B), B is m rows and n cols, thus A is k rows and m cols (where k=#records in the dataset and m=#input cols))\n",
        "\n",
        "        # Add hidden layers to model\n",
        "        while (hiddenLayerCount < hiddenLayerNum):\n",
        "            model.add(tf.keras.layers.Dense(nodes, activation=\"relu\"))    # N-hidden layer\n",
        "            hiddenLayerCount+=1    # increment nested while loop index by one(1)\n",
        "        hiddenLayerCount=0    # reset nested while loop index to zero(0)\n",
        "\n",
        "        # Add output layer\n",
        "        model.add(tf.keras.layers.Dense(6, activation=\"softmax\"))   # output layer.  Softmax converts a vector of values to a probability distribution with a range (0,1).\n",
        "\n",
        "        # compile the model\n",
        "        model=compileNeuralNetwork(model)\n",
        "\n",
        "        # train the model\n",
        "        model=trainNeuralNetwork(model, trainingDataset, verbose)\n",
        "\n",
        "        # print model's performance with the current number of hidden layers\n",
        "        print(\"\\n\" + \"MODEL#: \" + str(hiddenLayerNum) + \"\\n\" + \"Number of hidden layers: \" + str(hiddenLayerNum))\n",
        "        performanceScores=getModelsPerformance(model, trainingDataset)\n",
        "\n",
        "        # Append loss score to the \"lossScore\" list\n",
        "        lossScores.append(performanceScores[0])\n",
        "\n",
        "        # Append validation accuracy value to the \"accuracyScore\" list\n",
        "        accuracyScores.append(performanceScores[1])\n",
        "\n",
        "        # Calculate the validation accuracy value to loss score ratio and append to  to the \"accuracyToLossRatio\" list\n",
        "        accuracyToLossRatios.append(np.array(performanceScores[1])/np.array(performanceScores[0]))    # must convert list to numpy array to be able to perform calculations\n",
        "        print(\"\\n\")\n",
        "\n",
        "        hiddenLayerNum+=1    # increment outer while loop index by one(1)\n",
        "\n",
        "    hiddenLayerCount=0       # reset nested while loop index to zero(0)\n",
        "    hiddenLayerNum=0         # reset outer while loop index to zero(0)\n",
        "    tf.keras.backend.clear_session()    # clears all previously created models from memory\n",
        "\n",
        "    # Plot loss score change with increasing number of hidden layers\n",
        "    plt.plot(x_vals, lossScores)\n",
        "    plt.xlabel(\"Number of Hidden Layers\")\n",
        "    plt.ylabel(\"Loss Function Score\")\n",
        "    plt.show()\n",
        "\n",
        "    # Plot accuracy score change with increasing number of hidden layers\n",
        "    plt.plot(x_vals, accuracyScores)\n",
        "    plt.xlabel(\"Number of Hidden Layers\")\n",
        "    plt.ylabel(\"Accuracy Score on Validation Dataset\")\n",
        "    plt.show()\n",
        "\n",
        "    # Plot accuracy-to-loss score ratio with increasing number of hidden layers\n",
        "    plt.plot(x_vals, accuracyToLossRatios)\n",
        "    plt.xlabel(\"Number of Hidden Layers\")\n",
        "    plt.ylabel(\"Accuracy-to-Loss Ratio\")\n",
        "    plt.show()\n",
        "\n",
        "    print(\"x_vals: \"+str(len(x_vals)))\n",
        "    print(\"loss_score: \"+str(len(lossScores)))\n",
        "    print(\"acc_score: \"+str(len(accuracyScores)))\n",
        "    print(\"acc_to_loss_ratio: \"+str(len(accuracyToLossRatios)))\n",
        "\n",
        "    #return(model)\n",
        "\n"
      ],
      "metadata": {
        "id": "f3xr2qJsMMtU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "####################################################################\n",
        "#-------------------------------------------------------------------\n",
        "#  TEST FOR \"optimizeDepthOfSequentialNeuralNetwork()\" FUNCTION\n",
        "#-------------------------------------------------------------------\n",
        "####################################################################\n",
        "\n",
        "# Variables\n",
        "filename=\"nut_phase_questionnaire_data_fullset.csv\"\n",
        "\n",
        "# Parameters: (dataset, maxLayerWidth, networkDepth, numInputFeatures, shuffleTrainTestSplit, userDefinedVerbose)\n",
        "optimizeDepthOfSequentialNeuralNetwork(filename, 86, 15, 43, True, 0)\n"
      ],
      "metadata": {
        "id": "QHmr4QcpE2Fa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def createSNNModelForHPTuning(neurons, batch_size, learning_rate, epochs, numFeatures):\n",
        "    '''Function creates a Sequential Neural Network for tuning hyperparameters of the Nut_Phase Questionnaire deep learning model'''\n",
        "\n",
        "    tf.random.set_seed(42)\n",
        "    model = Sequential()   # create an instance of a Sequential object\n",
        "\n",
        "    # Add layers to the network\n",
        "    model.add(tf.keras.layers.Input(shape=(numFeatures, )))\n",
        "    model.add(tf.keras.layers.Dense(neurons, activation=\"relu\"))     # first hidden layer (1)\n",
        "    model.add(tf.keras.layers.Dense(neurons, activation=\"relu\"))     # second hidden layer (2)\n",
        "    model.add(tf.keras.layers.Dense(neurons, activation=\"relu\"))     # third hidden layer (3)\n",
        "    model.add(tf.keras.layers.Dense(neurons, activation=\"relu\"))     # fourth hidden layer (4)\n",
        "    model.add(tf.keras.layers.Dense(neurons, activation=\"relu\"))     # fifth hidden layer (5)\n",
        "    model.add(tf.keras.layers.Dense(6, activation=\"softmax\"))        # output layer.  Softmax converts a vector (array) of values into a probability distribution with a range (0,1).\n",
        "\n",
        "    model.compile(keras.optimizers.Adam(learning_rate=learning_rate), loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "    return(model)\n"
      ],
      "metadata": {
        "id": "Vrvm_VCvFgTX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def gridSearchCVOfSNN():\n",
        "    '''Grid search cross-validation for tuning hyperparameters of deep Sequential Neural Network'''\n",
        "\n",
        "    # Variables\n",
        "    filename=\"nut_phase_questionnaire_data_fullset.csv\"\n",
        "    modelTrainingDatasets=[]\n",
        "\n",
        "    # Read CSV datafile and convert to a Pandas dataframe object\n",
        "    dataFrame=readCSVFile(filename)\n",
        "\n",
        "    # Separate the dataframe by nut_phase\n",
        "    splitDataFrame = splitDataFrameIntoInputOutputByNutPhase(dataFrame)\n",
        "\n",
        "    # Split dataframes into Train, Test sets.  \"True\" hyperparameter selected for shuffling of dataset\n",
        "    modelTrainingDatasets = splitNutPhaseSeparatedDataFrameIntoTrainTestSets(splitDataFrame, True)\n",
        "\n",
        "    # Merge the Train and the Test sets, respectively\n",
        "    mergedDatasets = mergePWSNutPhaseSubsets(modelTrainingDatasets)\n",
        "\n",
        "    # Extract input (x-values - features) and output(y-values - labels) train sets\n",
        "    X_train= pd.DataFrame(mergedDatasets[0][0]).to_numpy()\n",
        "    y_train= pd.DataFrame(mergedDatasets[0][1]).to_numpy()\n",
        "\n",
        "    # param_grid=dict(neurons=neurons, learn_rate=learn_rate, batch_size=batch_size, epochs=epochs). Possible # of combinations: 54\n",
        "    param_grid={\n",
        "        \"neurons\": [6, 36, 86],\n",
        "        \"batch_size\": [6, 12],\n",
        "        \"learning_rate\": [0.001,0.01,0.2],\n",
        "        \"epochs\": [10,20,30]\n",
        "    }\n",
        "\n",
        "    # Initialize a KerasClassifier object and pass to it a Sequential Neural Network object\n",
        "    model=KerasClassifier(build_fn=createSNNModelForHPTuning)\n",
        "\n",
        "    # Initialize a Grid Search Cross-Validation object\n",
        "    grid=GridSearchCV(estimator=model, param_grid=param_grid, verbose=0, n_jobs=1, cv=5)\n",
        "\n",
        "    # Run GridSearchCV with all parameters\n",
        "    grid_result=grid.fit(X_train, y_train)\n",
        "\n",
        "    # Print best parameters:\n",
        "    print(\"Best: %f using %s\" % (grid_results.best_score_, grid_results.best_params_))\n",
        "    best_model=grid_result.best_estimator_\n"
      ],
      "metadata": {
        "id": "tLwI99YboZhy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "####################################################################\n",
        "#-------------------------------------------------------------------\n",
        "#           TEST FOR \"gridSearchCVOfSNN()\" FUNCTION\n",
        "#-------------------------------------------------------------------\n",
        "####################################################################\n",
        "\n",
        "gridSearchCVOfSNN()\n"
      ],
      "metadata": {
        "id": "iW0J9i_rHWbD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "######################################\n",
        "#  REFERENCE GRIDSEARCHCV FUNCTION\n",
        "######################################\n",
        "\n",
        "def create_model(neurons, batch_size, learning_rate, epochs):\n",
        "    '''Function for tuning hyperparameters of the Nut_Phase Questionnaire Sequential Neural Network model'''\n",
        "\n",
        "    # Notes: categorical_crossentropy is used as the loss function since this is a classification model.\n",
        "    # Initialize a Sequential Neural Network object\n",
        "    model=Sequential()\n",
        "\n",
        "    # Add layers to the network\n",
        "    model.add(tf.keras.layers.Input(shape=(43, )))\n",
        "    model.add(tf.keras.layers.Dense(neurons, activation=\"relu\"))     # first hidden layer (1)\n",
        "    model.add(tf.keras.layers.Dense(neurons, activation=\"relu\"))     # second hidden layer (2)\n",
        "    model.add(tf.keras.layers.Dense(neurons, activation=\"relu\"))     # third hidden layer (3)\n",
        "    model.add(tf.keras.layers.Dense(neurons, activation=\"relu\"))     # fourth hidden layer (4)\n",
        "    model.add(tf.keras.layers.Dense(neurons, activation=\"relu\"))     # fifth hidden layer (5)\n",
        "    model.add(tf.keras.layers.Dense(6, activation=\"softmax\"))        # output layer.  Softmax converts a vector (array) of values into a probability distribution with a range (0,1).\n",
        "\n",
        "    model.compile(keras.optimizers.Adam(learning_rate=learning_rate), loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])\n",
        "\n",
        "    return(model)\n"
      ],
      "metadata": {
        "id": "Tl3Yn_pYfiIF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "######################################\n",
        "#  REFERENCE GRIDSEARCHCV FUNCTION\n",
        "######################################\n",
        "\n",
        "# Variables\n",
        "filename=\"nut_phase_questionnaire_data_fullset.csv\"\n",
        "modelTrainingDatasets=[]\n",
        "\n",
        "# Read CSV datafile and convert to a Pandas dataframe object\n",
        "dataFrame=readCSVFile(filename)\n",
        "\n",
        "# Separate the dataframe by nut_phase\n",
        "splitDataFrame = splitDataFrameIntoInputOutputByNutPhase(dataFrame)\n",
        "\n",
        "# Split dataframes into Train, Test sets.  \"True\" hyperparameter selected for shuffling of dataset\n",
        "modelTrainingDatasets = splitNutPhaseSeparatedDataFrameIntoTrainTestSets(splitDataFrame, True)\n",
        "\n",
        "# Merge the Train and the Test sets, respectively\n",
        "mergedDatasets = mergePWSNutPhaseSubsets(modelTrainingDatasets)\n",
        "\n",
        "# Extract input (x-values - features) and output(y-values - labels) train sets\n",
        "X_train= pd.DataFrame(mergedDatasets[0][0]).to_numpy()\n",
        "Y_train= pd.DataFrame(mergedDatasets[0][1]).to_numpy()\n",
        "\n",
        "# param_grid=dict(neurons=neurons, learn_rate=learn_rate, batch_size=batch_size, epochs=epochs). Possible # of combinations: 54\n",
        "param_grid={\n",
        "    \"neurons\": [36, 86, 172],\n",
        "    \"batch_size\": [6, 12],\n",
        "    \"learning_rate\": [0.001,0.01,0.2],\n",
        "    \"epochs\": [10,20,30]\n",
        "}\n",
        "\n",
        "# Initialize a Grid Search Cross-Validation object\n",
        "grid = GridSearchCV(estimator=KerasClassifier(build_fn=create_model), param_grid=param_grid, n_jobs=1, verbose=0, cv=5)\n",
        "\n",
        "# Run GridSearchCV with all parameters\n",
        "grid_results=grid.fit(X_train,Y_train)\n",
        "\n",
        "# Print best parameters:\n",
        "print(\"Best: %f using %s\" % (grid_results.best_score_, grid_results.best_params_))\n",
        "\n",
        "best_model=grid_results.best_estimator_"
      ],
      "metadata": {
        "id": "nxeqihJFff3-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "**Main Function (Driver)**\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "kI2KRiuRJOsX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#---------------------------------------------------------------------------------\n",
        "# Title: PWS Nutritional Phase Predictor\n",
        "# Author: Carlos Sulsona\n",
        "# Date: 08/15/2023\n",
        "# Overview: Dataset was split by Nut_Phase and shuffling was conducted during the\n",
        "# Train/Test split.\n",
        "# Description: dataset is split into six(6) subsets by nutritional phase. Each\n",
        "# subset is then split into Train/Test sets. Train and test sets are then merged\n",
        "# to produce contiguous Train/Test datasets. These \"full\" datasets are then\n",
        "# used to train and test the Sequential Neural Network model.\n",
        "#---------------------------------------------------------------------------------\n",
        "\n",
        "# FUNCTION GLOBAL VARIABLES\n",
        "filename1=\"snn_nut_phase_questionnaire_training_dataset_NO_age; 09-26-2023.csv\"           # features: 43; nodes - 129\n",
        "filename2=\"snn_nut_phase_questionnaire_training_dataset_WITH_age; 09-26-2023.csv\"         # features: 44; nodes - 176\n",
        "filename3=\"snn_nut_phase_questionnaire_norm_training_dataset_NO_age; 09-22-2023.csv\"      # features: 43; nodes - 129\n",
        "filename4=\"snn_nut_phase_questionnaire_norm_training_dataset_WITH_age; 09-22-2023.csv\"    # features: 44; nodes - 176\n",
        "filename5=\"snn_nut_phase_questionnaire_num_training_dataset_NO_age; 10-26-2023.csv\"       # features: 43; nodes - 129\n",
        "modelTrainingDatasets=[]\n",
        "\n",
        "\n",
        "#--- METHOD CALLS ------------------------------------------\n",
        "# READ CSV DATAFILE AND CONVERT TO A PANDAS DATAFRAME OBJECT\n",
        "dataFrame=readCSVFile(filename1)\n",
        "\n",
        "# PARTITION THE DATASET BY NUTRITIONAL PHASE\n",
        "partitionedDataFrames = splitDataFrameIntoInputOutputByNutPhase(dataFrame)\n",
        "\n",
        "# SPLIT INPUT AND OUTPUT DATAFRAMES INTO TRAIN, TEST SETS (NOTE: \"TRUE\"\n",
        "# HYPERPARAMETER SELECTED FOR SHUFFLING OF DATASET DURING SPLIT PROCEDURE)\n",
        "trainTestSplits = splitNutPhaseSeparatedDataFrameIntoTrainTestSets(partitionedDataFrames, True)\n",
        "\n",
        "# MERGE THE TRAIN AND TEST SETS, RESPECTIVELY\n",
        "mergedTrainTestDatasets = mergePWSNutPhaseSubsets(trainTestSplits)\n",
        "\n",
        "# BUILD A SEQUENTIAL NEURAL NETWORK (SNN) - parameter (number of features, nodes per layer)\n",
        "model = constructSequentialNeuralNetwork(43, 129)\n",
        "\n",
        "# COMPILE THE SEQUENTIAL NEURAL NETWORK\n",
        "model = compileNeuralNetwork(model)\n",
        "\n",
        "# DISPLAY SUMMARY OF SNN STRUCTURE\n",
        "displayNeuralNetworkSummary(model)\n",
        "\n",
        "# DISPLAY GRAPHICAL STRUCTURE OF SNN\n",
        "#displayNeuralNetworkStructure(model)\n",
        "\n",
        "# TRAIN THE NEURAL NETWORK (NOTE: VERBOSE SET AT '2' TO DISPLAY INFORMATION ONTO\n",
        "# SCREEN DURING TRAINING OF MODEL)\n",
        "# (model, trainingDatasets, epochs, userDefinedVerbose):\n",
        "trainNeuralNetwork(model, mergedTrainTestDatasets, 50, 2)\n",
        "\n",
        "# VERIFY THE MODEL HAS LEARNED\n",
        "verifyModelHasLearned(model)\n",
        "\n",
        "# ASSESS NEURAL NETWORK'S PERFORMANCE\n",
        "modelsPerformance=getModelsPerformance(model, mergedTrainTestDatasets)\n",
        "\n",
        "tempModel=model\n",
        "tempModel_1=model\n",
        "\n",
        "# SAVE THE TRAINED MODEL (USER WILL BE PROMPTED TO SAVE MODEL)\n",
        "#saveTrainedModelAsH5(model)\n",
        "saveTrainedModelAsKeras(model)\n",
        "# saveTrainedModelAsOnnx(model)\n",
        "\n",
        "#findOptimalTestSize(mergedDatasets)\n",
        "\n",
        "#find optimal neural network depth\n",
        "#findOptimalDimensionsOfSequentialNeuralNetwork(mergedDatasets, 43)\n"
      ],
      "metadata": {
        "id": "WE6n7H6tyMrF",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "a1985c21-50dc-4b29-827f-6d47b8a391ce"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"sequential_3\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_3\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                        \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape               \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
              "│ dense_21 (\u001b[38;5;33mDense\u001b[0m)                     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m129\u001b[0m)                 │           \u001b[38;5;34m5,676\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_22 (\u001b[38;5;33mDense\u001b[0m)                     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m129\u001b[0m)                 │          \u001b[38;5;34m16,770\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_23 (\u001b[38;5;33mDense\u001b[0m)                     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m129\u001b[0m)                 │          \u001b[38;5;34m16,770\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_24 (\u001b[38;5;33mDense\u001b[0m)                     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m129\u001b[0m)                 │          \u001b[38;5;34m16,770\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_25 (\u001b[38;5;33mDense\u001b[0m)                     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m129\u001b[0m)                 │          \u001b[38;5;34m16,770\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_26 (\u001b[38;5;33mDense\u001b[0m)                     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m129\u001b[0m)                 │          \u001b[38;5;34m16,770\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_27 (\u001b[38;5;33mDense\u001b[0m)                     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m6\u001b[0m)                   │             \u001b[38;5;34m780\u001b[0m │\n",
              "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                         </span>┃<span style=\"font-weight: bold\"> Output Shape                </span>┃<span style=\"font-weight: bold\">         Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
              "│ dense_21 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">129</span>)                 │           <span style=\"color: #00af00; text-decoration-color: #00af00\">5,676</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_22 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">129</span>)                 │          <span style=\"color: #00af00; text-decoration-color: #00af00\">16,770</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_23 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">129</span>)                 │          <span style=\"color: #00af00; text-decoration-color: #00af00\">16,770</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_24 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">129</span>)                 │          <span style=\"color: #00af00; text-decoration-color: #00af00\">16,770</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_25 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">129</span>)                 │          <span style=\"color: #00af00; text-decoration-color: #00af00\">16,770</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_26 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">129</span>)                 │          <span style=\"color: #00af00; text-decoration-color: #00af00\">16,770</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_27 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">6</span>)                   │             <span style=\"color: #00af00; text-decoration-color: #00af00\">780</span> │\n",
              "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m90,306\u001b[0m (352.76 KB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">90,306</span> (352.76 KB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m90,306\u001b[0m (352.76 KB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">90,306</span> (352.76 KB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "None\n",
            "Epoch 1/50\n",
            "41/41 - 3s - 76ms/step - accuracy: 0.3444 - loss: 1.5711 - val_accuracy: 0.5566 - val_loss: 1.1218\n",
            "Epoch 2/50\n",
            "41/41 - 0s - 10ms/step - accuracy: 0.7925 - loss: 0.6544 - val_accuracy: 1.0000 - val_loss: 0.1104\n",
            "Epoch 3/50\n",
            "41/41 - 0s - 8ms/step - accuracy: 0.9793 - loss: 0.1613 - val_accuracy: 0.9528 - val_loss: 0.3014\n",
            "Epoch 4/50\n",
            "41/41 - 0s - 6ms/step - accuracy: 0.9751 - loss: 0.1516 - val_accuracy: 0.9906 - val_loss: 0.0856\n",
            "Epoch 5/50\n",
            "41/41 - 0s - 5ms/step - accuracy: 0.9876 - loss: 0.0412 - val_accuracy: 0.9717 - val_loss: 0.2799\n",
            "Epoch 6/50\n",
            "41/41 - 0s - 7ms/step - accuracy: 0.9668 - loss: 0.1944 - val_accuracy: 1.0000 - val_loss: 0.0250\n",
            "Epoch 7/50\n",
            "41/41 - 0s - 6ms/step - accuracy: 1.0000 - loss: 0.0110 - val_accuracy: 0.9811 - val_loss: 0.0390\n",
            "Epoch 8/50\n",
            "41/41 - 0s - 10ms/step - accuracy: 1.0000 - loss: 0.0023 - val_accuracy: 0.9811 - val_loss: 0.0320\n",
            "Epoch 9/50\n",
            "41/41 - 0s - 12ms/step - accuracy: 1.0000 - loss: 0.0012 - val_accuracy: 0.9811 - val_loss: 0.0362\n",
            "Epoch 10/50\n",
            "41/41 - 0s - 11ms/step - accuracy: 1.0000 - loss: 7.9881e-04 - val_accuracy: 0.9811 - val_loss: 0.0384\n",
            "Epoch 11/50\n",
            "41/41 - 1s - 17ms/step - accuracy: 1.0000 - loss: 5.5012e-04 - val_accuracy: 0.9811 - val_loss: 0.0396\n",
            "Epoch 12/50\n",
            "41/41 - 1s - 13ms/step - accuracy: 1.0000 - loss: 3.9375e-04 - val_accuracy: 0.9811 - val_loss: 0.0413\n",
            "Epoch 13/50\n",
            "41/41 - 1s - 20ms/step - accuracy: 1.0000 - loss: 2.8922e-04 - val_accuracy: 0.9811 - val_loss: 0.0425\n",
            "Epoch 14/50\n",
            "41/41 - 1s - 15ms/step - accuracy: 1.0000 - loss: 2.1941e-04 - val_accuracy: 0.9811 - val_loss: 0.0433\n",
            "Epoch 15/50\n",
            "41/41 - 0s - 10ms/step - accuracy: 1.0000 - loss: 1.7116e-04 - val_accuracy: 0.9811 - val_loss: 0.0443\n",
            "Epoch 16/50\n",
            "41/41 - 1s - 16ms/step - accuracy: 1.0000 - loss: 1.3088e-04 - val_accuracy: 0.9811 - val_loss: 0.0472\n",
            "Epoch 17/50\n",
            "41/41 - 1s - 17ms/step - accuracy: 1.0000 - loss: 9.0473e-05 - val_accuracy: 0.9811 - val_loss: 0.0641\n",
            "Epoch 18/50\n",
            "41/41 - 0s - 8ms/step - accuracy: 1.0000 - loss: 6.1142e-05 - val_accuracy: 0.9811 - val_loss: 0.0710\n",
            "Epoch 19/50\n",
            "41/41 - 0s - 9ms/step - accuracy: 1.0000 - loss: 4.0516e-05 - val_accuracy: 0.9811 - val_loss: 0.0688\n",
            "Epoch 20/50\n",
            "41/41 - 0s - 6ms/step - accuracy: 1.0000 - loss: 2.8479e-05 - val_accuracy: 0.9811 - val_loss: 0.0628\n",
            "Epoch 21/50\n",
            "41/41 - 0s - 4ms/step - accuracy: 1.0000 - loss: 2.0549e-05 - val_accuracy: 0.9811 - val_loss: 0.0635\n",
            "Epoch 22/50\n",
            "41/41 - 0s - 7ms/step - accuracy: 1.0000 - loss: 1.5169e-05 - val_accuracy: 0.9811 - val_loss: 0.0615\n",
            "Epoch 23/50\n",
            "41/41 - 0s - 7ms/step - accuracy: 1.0000 - loss: 1.1839e-05 - val_accuracy: 0.9811 - val_loss: 0.0601\n",
            "Epoch 24/50\n",
            "41/41 - 0s - 7ms/step - accuracy: 1.0000 - loss: 9.7156e-06 - val_accuracy: 0.9811 - val_loss: 0.0617\n",
            "Epoch 25/50\n",
            "41/41 - 0s - 6ms/step - accuracy: 1.0000 - loss: 8.2446e-06 - val_accuracy: 0.9811 - val_loss: 0.0625\n",
            "Epoch 26/50\n",
            "41/41 - 0s - 8ms/step - accuracy: 1.0000 - loss: 7.1989e-06 - val_accuracy: 0.9811 - val_loss: 0.0633\n",
            "Epoch 27/50\n",
            "41/41 - 0s - 8ms/step - accuracy: 1.0000 - loss: 6.4224e-06 - val_accuracy: 0.9811 - val_loss: 0.0636\n",
            "Epoch 28/50\n",
            "41/41 - 0s - 7ms/step - accuracy: 1.0000 - loss: 5.8234e-06 - val_accuracy: 0.9811 - val_loss: 0.0640\n",
            "Epoch 29/50\n",
            "41/41 - 0s - 7ms/step - accuracy: 1.0000 - loss: 5.3262e-06 - val_accuracy: 0.9811 - val_loss: 0.0662\n",
            "Epoch 30/50\n",
            "41/41 - 0s - 8ms/step - accuracy: 1.0000 - loss: 4.9014e-06 - val_accuracy: 0.9811 - val_loss: 0.0671\n",
            "Epoch 31/50\n",
            "41/41 - 0s - 7ms/step - accuracy: 1.0000 - loss: 4.5512e-06 - val_accuracy: 0.9811 - val_loss: 0.0678\n",
            "Epoch 32/50\n",
            "41/41 - 0s - 4ms/step - accuracy: 1.0000 - loss: 4.2282e-06 - val_accuracy: 0.9811 - val_loss: 0.0689\n",
            "Epoch 33/50\n",
            "41/41 - 0s - 7ms/step - accuracy: 1.0000 - loss: 3.9442e-06 - val_accuracy: 0.9811 - val_loss: 0.0694\n",
            "Epoch 34/50\n",
            "41/41 - 0s - 7ms/step - accuracy: 1.0000 - loss: 3.7014e-06 - val_accuracy: 0.9811 - val_loss: 0.0707\n",
            "Epoch 35/50\n",
            "41/41 - 0s - 4ms/step - accuracy: 1.0000 - loss: 3.4738e-06 - val_accuracy: 0.9811 - val_loss: 0.0715\n",
            "Epoch 36/50\n",
            "41/41 - 0s - 7ms/step - accuracy: 1.0000 - loss: 3.2790e-06 - val_accuracy: 0.9811 - val_loss: 0.0727\n",
            "Epoch 37/50\n",
            "41/41 - 0s - 4ms/step - accuracy: 1.0000 - loss: 3.0979e-06 - val_accuracy: 0.9811 - val_loss: 0.0736\n",
            "Epoch 38/50\n",
            "41/41 - 0s - 4ms/step - accuracy: 1.0000 - loss: 2.9416e-06 - val_accuracy: 0.9811 - val_loss: 0.0744\n",
            "Epoch 39/50\n",
            "41/41 - 0s - 7ms/step - accuracy: 1.0000 - loss: 2.7937e-06 - val_accuracy: 0.9811 - val_loss: 0.0748\n",
            "Epoch 40/50\n",
            "41/41 - 0s - 7ms/step - accuracy: 1.0000 - loss: 2.6691e-06 - val_accuracy: 0.9811 - val_loss: 0.0754\n",
            "Epoch 41/50\n",
            "41/41 - 0s - 5ms/step - accuracy: 1.0000 - loss: 2.5504e-06 - val_accuracy: 0.9811 - val_loss: 0.0757\n",
            "Epoch 42/50\n",
            "41/41 - 0s - 4ms/step - accuracy: 1.0000 - loss: 2.4321e-06 - val_accuracy: 0.9811 - val_loss: 0.0760\n",
            "Epoch 43/50\n",
            "41/41 - 0s - 5ms/step - accuracy: 1.0000 - loss: 2.3258e-06 - val_accuracy: 0.9811 - val_loss: 0.0780\n",
            "Epoch 44/50\n",
            "41/41 - 0s - 4ms/step - accuracy: 1.0000 - loss: 2.2333e-06 - val_accuracy: 0.9811 - val_loss: 0.0782\n",
            "Epoch 45/50\n",
            "41/41 - 0s - 7ms/step - accuracy: 1.0000 - loss: 2.1383e-06 - val_accuracy: 0.9811 - val_loss: 0.0789\n",
            "Epoch 46/50\n",
            "41/41 - 0s - 4ms/step - accuracy: 1.0000 - loss: 2.0444e-06 - val_accuracy: 0.9811 - val_loss: 0.0795\n",
            "Epoch 47/50\n",
            "41/41 - 0s - 8ms/step - accuracy: 1.0000 - loss: 1.9617e-06 - val_accuracy: 0.9811 - val_loss: 0.0798\n",
            "Epoch 48/50\n",
            "41/41 - 0s - 6ms/step - accuracy: 1.0000 - loss: 1.8880e-06 - val_accuracy: 0.9811 - val_loss: 0.0802\n",
            "Epoch 49/50\n",
            "41/41 - 0s - 4ms/step - accuracy: 1.0000 - loss: 1.8045e-06 - val_accuracy: 0.9811 - val_loss: 0.0819\n",
            "Epoch 50/50\n",
            "41/41 - 0s - 4ms/step - accuracy: 1.0000 - loss: 1.7253e-06 - val_accuracy: 0.9811 - val_loss: 0.0824\n",
            "8/8 - 0s - 4ms/step - accuracy: 1.0000 - loss: 1.6214e-06\n",
            "4/4 - 0s - 7ms/step - accuracy: 0.9811 - loss: 0.0824\n",
            "Would you like to save this model? (y/n)y\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "**Test Model's Ability To Make A Prediction**\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "u2bsflwZgjfP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "###################################################################################################\n",
        "#--------------------------------------------------------------------------------------------------\n",
        "#                                        PREDICT CASE\n",
        "#--------------------------------------------------------------------------------------------------\n",
        "###################################################################################################\n",
        "import numpy as np\n",
        "\n",
        "#--- VARIABLES --------------------\n",
        "yes_counter=0\n",
        "ns_counter=0\n",
        "\n",
        "#--- TEST SEQUENTIAL NEURAL NETWORK MODEL'S ABILITY TO MAKE AN ACCURATE PREDICTION - PREDICT PWS NUTRITIONAL PHASE ---\n",
        "# nut_phases [.....1a(n=7)......|.1b(n=3)..|...2a(n=5)....|.......2b(n=7)........|...........3(n=13).............|.....4(n=6).....]\n",
        "testSubmission = [0,0,0,0,0,0,0,                 # 1a\n",
        "                  0,0,0,                         # 1b\n",
        "                  0,0,0,0,0,                     # 2a\n",
        "                  1,0,1,1,1,1,1,0,1,             # 2b\n",
        "                  0,0,0,1,1,0,0,0,1,0,0,0,1,     # 3\n",
        "                  0,1,0,1,1,1]                   # 4\n",
        "\n",
        "\n",
        "# -- Use this line of code when including age(yrs) --\n",
        "# testSubmission = [0.33,                          # age(yrs)\n",
        "#                   0,0,0,0,0,0,0,                 # 1a\n",
        "#                   0,0,0,                         # 1b\n",
        "#                   0,0,0,0,0,                     # 2a\n",
        "#                   1,0,1,1,1,1,1,0,1,             # 2b\n",
        "#                   0,0,0,1,1,0,0,0,1,0,0,0,1,     # 3\n",
        "#                   0,1,0,1,1,1]                   # 4\n",
        "\n",
        "#--- COUNT THE NUMBER OF \"YES\" AND \"NS\" SELECTIONS MADE --------------------\n",
        "for selection in testSubmission:\n",
        "    if (selection==1):\n",
        "        yes_counter+=1\n",
        "    elif (selection==0.5):\n",
        "        ns_counter+=1\n",
        "\n",
        "#--- DETERMINE IF SELECTIONS VALID FOR USE IN PREDICTIVE ANALYTICS WERE MADE --------------------\n",
        "if (ns_counter!=0 and yes_counter==0):\n",
        "    print(\"Please make selections other than just 'NS' on Nut_Phase Questionnaire form\")\n",
        "\n",
        "elif (yes_counter==0):\n",
        "    print(\"Please make valid selections on Nut_Phase Questionnaire form\")\n",
        "\n",
        "elif (yes_counter!=0):\n",
        "    #--- MAKE PREDICTION --------------------\n",
        "    keras_model = tf.keras.models.load_model('pws_qnr_dnn_model_05242024.keras')\n",
        "    input_data = np.array([testSubmission])\n",
        "    prediction = keras_model.predict([input_data])\n",
        "    print(prediction)\n",
        "\n",
        "\n",
        "    #--- DISPLAY PREDICTED NUTRITIONAL PHASE --------------------\n",
        "    output_columns = [\"Phase_1a\",\"Phase_1b\",\"Phase_2a\", \"Phase_2b\",\"Phase_3\", \"Phase_4\"]\n",
        "    print(\"\\n\"+ \"Predicted PWS Nutritional Phase:\" + \"\\n\"+\n",
        "          output_columns[prediction.argmax()] + \"\\n\")\n",
        "\n",
        "\n",
        "    #--- PROBABILITY DISTRIBUTION VALUES FOR ALL NUTRITIONAL PHASES REPRESENTED AS PERCENTAGES --------------------\n",
        "    i=0   # loop counting index\n",
        "    output_columns = [\"Phase_1a\",\"Phase_1b\",\"Phase_2a\", \"Phase_2b\",\"Phase_3\", \"Phase_4\"]\n",
        "\n",
        "    print(\"Probability Distribution:\")\n",
        "\n",
        "    for prob_dist_val in prediction[0,]:\n",
        "      print(output_columns[i] + \": \" + str(round(prediction[0,i]*100,1)) + \"%\")\n",
        "      i=i+1\n"
      ],
      "metadata": {
        "id": "IqA2AFDjgnnP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c5f4f8c7-504e-4db2-ce3f-c490dd112b8e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 181ms/step\n",
            "[[2.7802112e-20 5.4950245e-13 6.4088529e-13 1.5490211e-07 9.9999988e-01\n",
            "  6.0535192e-23]]\n",
            "\n",
            "Predicted PWS Nutritional Phase:\n",
            "Phase_3\n",
            "\n",
            "Probability Distribution:\n",
            "Phase_1a: 0.0%\n",
            "Phase_1b: 0.0%\n",
            "Phase_2a: 0.0%\n",
            "Phase_2b: 0.0%\n",
            "Phase_3: 100.0%\n",
            "Phase_4: 0.0%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "**Main Function_2 (Driver)**\n",
        "\n",
        "---\n"
      ],
      "metadata": {
        "id": "YL3IGu0KfGcq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#---------------------------------------------------------------------------------\n",
        "# Title: PWS Nutritional Phase Predictor\n",
        "# Author: Carlos Sulsona\n",
        "# Date: 08/22/2023\n",
        "# Overview: Dataset was split by Nut_Phase and \"PRE-SHUFFLING\" of subsets was\n",
        "# conducted prior to Train/Test split.\n",
        "# Description: dataset is split into six(6) subsets by nutritional phase then\n",
        "# each subset is shuffled. Shuffled subsets are then merged row_wise to produce a\n",
        "# contiguous full dataset. This \"pre-shuffled\" dataset is then used to generate\n",
        "# Train/Test sets for training and testing the Sequential Neural Network model.\n",
        "#---------------------------------------------------------------------------------\n",
        "\n",
        "# FUNCTION GLOBAL VARIABLES\n",
        "filename1=\"snn_nut_phase_questionnaire_training_dataset_NO_age; 09-26-2023.csv\"           # features: 43\n",
        "filename2=\"snn_nut_phase_questionnaire_training_dataset_WITH_age; 09-26-2023.csv\"         # features: 44\n",
        "filename3=\"snn_nut_phase_questionnaire_norm_training_dataset_NO_age; 09-22-2023.csv\"      # features: 43\n",
        "filename4=\"snn_nut_phase_questionnaire_norm_training_dataset_WITH_age; 09-22-2023.csv\"    # features: 44\n",
        "filename5=\"snn_nut_phase_questionnaire_num_training_dataset_NO_age; 10-26-2023.csv\"       # features: 43\n",
        "\n",
        "modelTrainingDatasets=[]\n",
        "\n",
        "\n",
        "#--- METHOD CALLS ------------------------------------------\n",
        "# READ CSV DATAFILE AND CONVERT TO A PANDAS DATAFRAME OBJECT\n",
        "dataFrame=readCSVFile(filename1)\n",
        "\n",
        "# PARTITION THE DATASET BY NUTRITIONAL PHASE\n",
        "splitDataFrame = splitDataFrameByNutPhase(dataFrame)\n",
        "\n",
        "# SHUFFLE THE PARTITIONED DATAFRAMES\n",
        "shuffledDataFrames=shufflePWSNutPhasePartitionedDataFrames(splitDataFrame)\n",
        "\n",
        "# MERGE THE PARTITIONED DATAFRAMES ROW-WISE\n",
        "mergedDataFrames=mergeDataFramesRow_Wise(shuffledDataFrames)\n",
        "\n",
        "# SPLIT THE MERGED DATAFRAME INTO INPUT AND OUTPUT DATAFRAMES BY NUTRITIONAL PHASE\n",
        "dataFramesForTrainTestSplit=splitDataFrameIntoInputOutputByNutPhase(mergedDataFrames)\n",
        "\n",
        "# SPLIT INPUT AND OUTPUT DATAFRAMES INTO TRAIN, TEST SETS (NOTE: \"FALSE\"\n",
        "# HYPERPARAMETER SELECTED FOR SHUFFLING OF DATASET DURING SPLIT PROCEDURE)\n",
        "trainTestSplits = splitNutPhaseSeparatedDataFrameIntoTrainTestSets(dataFramesForTrainTestSplit, False)\n",
        "\n",
        "# MERGE THE TRAIN AND TEST SETS, RESPECTIVELY\n",
        "mergedDatasets = mergePWSNutPhaseSubsets(trainTestSplits)\n",
        "\n",
        "# BUILD A SEQUENTIAL NEURAL NETWORK (SNN) - parameter (number of features, nodes per layer)\n",
        "model2 = constructSequentialNeuralNetwork(43, 129)\n",
        "\n",
        "# COMPILE THE SEQUENTIAL NEURAL NETWORK\n",
        "model2 = compileNeuralNetwork(model2)\n",
        "\n",
        "# DISPLAY SUMMARY OF SNN STRUCTURE\n",
        "displayNeuralNetworkSummary(model2)\n",
        "\n",
        "# DISPLAY GRAPHICAL STRUCTURE OF SNN\n",
        "# displayNeuralNetworkStructure(model2)\n",
        "\n",
        "# TRAIN THE NEURAL NETWORK (NOTE: VERBOSE SET AT '2' TO DISPLAY INFORMATION ONTO\n",
        "# SCREEN DURING TRAINING OF MODEL)\n",
        "# (model, trainingDatasets, epochs, userDefinedVerbose):\n",
        "trainNeuralNetwork(model2, mergedDatasets, 50, 2)\n",
        "\n",
        "# VERIFY THE MODEL HAS LEARNED\n",
        "verifyModelHasLearned(model2)\n",
        "\n",
        "# ASSESS NEURAL NETWORK'S PERFORMANCE\n",
        "# modelsPerformance=getModelsPerformance(model2, mergedDatasets)\n",
        "\n",
        "# SAVE THE TRAINED MODEL (USER WILL BE PROMPTED TO SAVE MODEL)\n",
        "# saveTrainedModel(model2)\n",
        "\n",
        "\n",
        "#findOptimalTestSize(mergedDatasets)\n",
        "\n",
        "#find optimal neural network depth\n",
        "#findOptimalDimensionsOfSequentialNeuralNetwork(mergedDatasets, 43)\n",
        "\n",
        "tempModel=model2\n",
        "tempModel_2=model2\n"
      ],
      "metadata": {
        "id": "iskBExNYefQI",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "c12cc25d-64f5-44af-a009-51cf9834b792"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"sequential_4\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_4\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                        \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape               \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
              "│ dense_28 (\u001b[38;5;33mDense\u001b[0m)                     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m129\u001b[0m)                 │           \u001b[38;5;34m5,676\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_29 (\u001b[38;5;33mDense\u001b[0m)                     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m129\u001b[0m)                 │          \u001b[38;5;34m16,770\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_30 (\u001b[38;5;33mDense\u001b[0m)                     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m129\u001b[0m)                 │          \u001b[38;5;34m16,770\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_31 (\u001b[38;5;33mDense\u001b[0m)                     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m129\u001b[0m)                 │          \u001b[38;5;34m16,770\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_32 (\u001b[38;5;33mDense\u001b[0m)                     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m129\u001b[0m)                 │          \u001b[38;5;34m16,770\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_33 (\u001b[38;5;33mDense\u001b[0m)                     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m129\u001b[0m)                 │          \u001b[38;5;34m16,770\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_34 (\u001b[38;5;33mDense\u001b[0m)                     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m6\u001b[0m)                   │             \u001b[38;5;34m780\u001b[0m │\n",
              "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                         </span>┃<span style=\"font-weight: bold\"> Output Shape                </span>┃<span style=\"font-weight: bold\">         Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
              "│ dense_28 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">129</span>)                 │           <span style=\"color: #00af00; text-decoration-color: #00af00\">5,676</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_29 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">129</span>)                 │          <span style=\"color: #00af00; text-decoration-color: #00af00\">16,770</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_30 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">129</span>)                 │          <span style=\"color: #00af00; text-decoration-color: #00af00\">16,770</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_31 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">129</span>)                 │          <span style=\"color: #00af00; text-decoration-color: #00af00\">16,770</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_32 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">129</span>)                 │          <span style=\"color: #00af00; text-decoration-color: #00af00\">16,770</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_33 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">129</span>)                 │          <span style=\"color: #00af00; text-decoration-color: #00af00\">16,770</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_34 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">6</span>)                   │             <span style=\"color: #00af00; text-decoration-color: #00af00\">780</span> │\n",
              "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m90,306\u001b[0m (352.76 KB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">90,306</span> (352.76 KB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m90,306\u001b[0m (352.76 KB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">90,306</span> (352.76 KB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "None\n",
            "Epoch 1/50\n",
            "41/41 - 2s - 61ms/step - accuracy: 0.4315 - loss: 1.4562 - val_accuracy: 0.7925 - val_loss: 1.0650\n",
            "Epoch 2/50\n",
            "41/41 - 0s - 11ms/step - accuracy: 0.8506 - loss: 0.5699 - val_accuracy: 0.8491 - val_loss: 0.4770\n",
            "Epoch 3/50\n",
            "41/41 - 0s - 7ms/step - accuracy: 0.9751 - loss: 0.0965 - val_accuracy: 0.9811 - val_loss: 0.2424\n",
            "Epoch 4/50\n",
            "41/41 - 0s - 6ms/step - accuracy: 0.9710 - loss: 0.1412 - val_accuracy: 0.9811 - val_loss: 0.1673\n",
            "Epoch 5/50\n",
            "41/41 - 0s - 7ms/step - accuracy: 0.9876 - loss: 0.0992 - val_accuracy: 0.9528 - val_loss: 0.2727\n",
            "Epoch 6/50\n",
            "41/41 - 0s - 7ms/step - accuracy: 1.0000 - loss: 0.0151 - val_accuracy: 0.9811 - val_loss: 0.2319\n",
            "Epoch 7/50\n",
            "41/41 - 0s - 4ms/step - accuracy: 1.0000 - loss: 0.0023 - val_accuracy: 0.9811 - val_loss: 0.2950\n",
            "Epoch 8/50\n",
            "41/41 - 0s - 4ms/step - accuracy: 1.0000 - loss: 7.5106e-04 - val_accuracy: 0.9811 - val_loss: 0.3555\n",
            "Epoch 9/50\n",
            "41/41 - 0s - 5ms/step - accuracy: 1.0000 - loss: 3.5982e-04 - val_accuracy: 0.9811 - val_loss: 0.3919\n",
            "Epoch 10/50\n",
            "41/41 - 0s - 4ms/step - accuracy: 1.0000 - loss: 2.0249e-04 - val_accuracy: 0.9811 - val_loss: 0.4103\n",
            "Epoch 11/50\n",
            "41/41 - 0s - 7ms/step - accuracy: 1.0000 - loss: 1.2293e-04 - val_accuracy: 0.9811 - val_loss: 0.4271\n",
            "Epoch 12/50\n",
            "41/41 - 0s - 5ms/step - accuracy: 1.0000 - loss: 8.2259e-05 - val_accuracy: 0.9811 - val_loss: 0.4461\n",
            "Epoch 13/50\n",
            "41/41 - 0s - 5ms/step - accuracy: 1.0000 - loss: 5.9285e-05 - val_accuracy: 0.9811 - val_loss: 0.4620\n",
            "Epoch 14/50\n",
            "41/41 - 0s - 4ms/step - accuracy: 1.0000 - loss: 4.5108e-05 - val_accuracy: 0.9811 - val_loss: 0.4743\n",
            "Epoch 15/50\n",
            "41/41 - 0s - 5ms/step - accuracy: 1.0000 - loss: 3.5357e-05 - val_accuracy: 0.9811 - val_loss: 0.4861\n",
            "Epoch 16/50\n",
            "41/41 - 0s - 5ms/step - accuracy: 1.0000 - loss: 2.7941e-05 - val_accuracy: 0.9717 - val_loss: 0.4971\n",
            "Epoch 17/50\n",
            "41/41 - 0s - 6ms/step - accuracy: 1.0000 - loss: 2.2127e-05 - val_accuracy: 0.9717 - val_loss: 0.5054\n",
            "Epoch 18/50\n",
            "41/41 - 0s - 7ms/step - accuracy: 1.0000 - loss: 1.7754e-05 - val_accuracy: 0.9717 - val_loss: 0.5129\n",
            "Epoch 19/50\n",
            "41/41 - 0s - 4ms/step - accuracy: 1.0000 - loss: 1.4536e-05 - val_accuracy: 0.9717 - val_loss: 0.5201\n",
            "Epoch 20/50\n",
            "41/41 - 0s - 7ms/step - accuracy: 1.0000 - loss: 1.2150e-05 - val_accuracy: 0.9717 - val_loss: 0.5275\n",
            "Epoch 21/50\n",
            "41/41 - 0s - 4ms/step - accuracy: 1.0000 - loss: 1.0277e-05 - val_accuracy: 0.9717 - val_loss: 0.5345\n",
            "Epoch 22/50\n",
            "41/41 - 0s - 7ms/step - accuracy: 1.0000 - loss: 8.8090e-06 - val_accuracy: 0.9717 - val_loss: 0.5412\n",
            "Epoch 23/50\n",
            "41/41 - 0s - 8ms/step - accuracy: 1.0000 - loss: 7.6431e-06 - val_accuracy: 0.9717 - val_loss: 0.5466\n",
            "Epoch 24/50\n",
            "41/41 - 0s - 5ms/step - accuracy: 1.0000 - loss: 6.7172e-06 - val_accuracy: 0.9717 - val_loss: 0.5521\n",
            "Epoch 25/50\n",
            "41/41 - 0s - 5ms/step - accuracy: 1.0000 - loss: 5.9416e-06 - val_accuracy: 0.9717 - val_loss: 0.5565\n",
            "Epoch 26/50\n",
            "41/41 - 0s - 7ms/step - accuracy: 1.0000 - loss: 5.3055e-06 - val_accuracy: 0.9717 - val_loss: 0.5616\n",
            "Epoch 27/50\n",
            "41/41 - 0s - 5ms/step - accuracy: 1.0000 - loss: 4.7589e-06 - val_accuracy: 0.9717 - val_loss: 0.5653\n",
            "Epoch 28/50\n",
            "41/41 - 0s - 5ms/step - accuracy: 1.0000 - loss: 4.2955e-06 - val_accuracy: 0.9717 - val_loss: 0.5704\n",
            "Epoch 29/50\n",
            "41/41 - 0s - 4ms/step - accuracy: 1.0000 - loss: 3.8968e-06 - val_accuracy: 0.9717 - val_loss: 0.5740\n",
            "Epoch 30/50\n",
            "41/41 - 0s - 7ms/step - accuracy: 1.0000 - loss: 3.5490e-06 - val_accuracy: 0.9717 - val_loss: 0.5794\n",
            "Epoch 31/50\n",
            "41/41 - 0s - 5ms/step - accuracy: 1.0000 - loss: 3.2394e-06 - val_accuracy: 0.9717 - val_loss: 0.5832\n",
            "Epoch 32/50\n",
            "41/41 - 0s - 7ms/step - accuracy: 1.0000 - loss: 2.9807e-06 - val_accuracy: 0.9717 - val_loss: 0.5880\n",
            "Epoch 33/50\n",
            "41/41 - 0s - 7ms/step - accuracy: 1.0000 - loss: 2.7398e-06 - val_accuracy: 0.9717 - val_loss: 0.5911\n",
            "Epoch 34/50\n",
            "41/41 - 0s - 7ms/step - accuracy: 1.0000 - loss: 2.5256e-06 - val_accuracy: 0.9717 - val_loss: 0.5948\n",
            "Epoch 35/50\n",
            "41/41 - 0s - 7ms/step - accuracy: 1.0000 - loss: 2.3441e-06 - val_accuracy: 0.9717 - val_loss: 0.5992\n",
            "Epoch 36/50\n",
            "41/41 - 0s - 7ms/step - accuracy: 1.0000 - loss: 2.1853e-06 - val_accuracy: 0.9717 - val_loss: 0.6023\n",
            "Epoch 37/50\n",
            "41/41 - 0s - 8ms/step - accuracy: 1.0000 - loss: 2.0394e-06 - val_accuracy: 0.9717 - val_loss: 0.6065\n",
            "Epoch 38/50\n",
            "41/41 - 0s - 6ms/step - accuracy: 1.0000 - loss: 1.8979e-06 - val_accuracy: 0.9717 - val_loss: 0.6090\n",
            "Epoch 39/50\n",
            "41/41 - 0s - 7ms/step - accuracy: 1.0000 - loss: 1.7733e-06 - val_accuracy: 0.9717 - val_loss: 0.6121\n",
            "Epoch 40/50\n",
            "41/41 - 0s - 7ms/step - accuracy: 1.0000 - loss: 1.6590e-06 - val_accuracy: 0.9717 - val_loss: 0.6155\n",
            "Epoch 41/50\n",
            "41/41 - 0s - 7ms/step - accuracy: 1.0000 - loss: 1.5586e-06 - val_accuracy: 0.9717 - val_loss: 0.6196\n",
            "Epoch 42/50\n",
            "41/41 - 0s - 8ms/step - accuracy: 1.0000 - loss: 1.4706e-06 - val_accuracy: 0.9717 - val_loss: 0.6223\n",
            "Epoch 43/50\n",
            "41/41 - 1s - 12ms/step - accuracy: 1.0000 - loss: 1.3865e-06 - val_accuracy: 0.9717 - val_loss: 0.6252\n",
            "Epoch 44/50\n",
            "41/41 - 0s - 7ms/step - accuracy: 1.0000 - loss: 1.3068e-06 - val_accuracy: 0.9717 - val_loss: 0.6279\n",
            "Epoch 45/50\n",
            "41/41 - 0s - 8ms/step - accuracy: 1.0000 - loss: 1.2331e-06 - val_accuracy: 0.9717 - val_loss: 0.6309\n",
            "Epoch 46/50\n",
            "41/41 - 0s - 7ms/step - accuracy: 1.0000 - loss: 1.1654e-06 - val_accuracy: 0.9717 - val_loss: 0.6343\n",
            "Epoch 47/50\n",
            "41/41 - 0s - 7ms/step - accuracy: 1.0000 - loss: 1.1035e-06 - val_accuracy: 0.9717 - val_loss: 0.6369\n",
            "Epoch 48/50\n",
            "41/41 - 0s - 7ms/step - accuracy: 1.0000 - loss: 1.0506e-06 - val_accuracy: 0.9717 - val_loss: 0.6393\n",
            "Epoch 49/50\n",
            "41/41 - 0s - 7ms/step - accuracy: 1.0000 - loss: 9.9868e-07 - val_accuracy: 0.9717 - val_loss: 0.6419\n",
            "Epoch 50/50\n",
            "41/41 - 0s - 5ms/step - accuracy: 1.0000 - loss: 9.4774e-07 - val_accuracy: 0.9717 - val_loss: 0.6443\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "###################################################################################################\n",
        "#--------------------------------------------------------------------------------------------------\n",
        "#                                        PREDICT CASE\n",
        "#--------------------------------------------------------------------------------------------------\n",
        "###################################################################################################\n",
        "\n",
        "#--- VARIABLES --------------------\n",
        "yes_counter=0\n",
        "ns_counter=0\n",
        "\n",
        "#--- TEST SEQUENTIAL NEURAL NETWORK MODEL'S ABILITY TO MAKE AN ACCURATE PREDICTION - PREDICT PWS NUTRITIONAL PHASE ---\n",
        "# nut_phases [.....1a(n=7)......|.1b(n=3)..|...2a(n=5)....|.......2b(n=7)........|...........3(n=13).............|.....4(n=6).....]\n",
        "testSubmission = [0,0,0,0,0,0,0,                 # 1a\n",
        "                  0,0,0,                         # 1b\n",
        "                  0,0,0,0,0,                     # 2a\n",
        "                  1,0,1,1,1,1,1,0,1,             # 2b\n",
        "                  0,0,0,1,1,0,0,0,1,0,0,0,1,     # 3\n",
        "                  0,1,0,1,1,1]                   # 4\n",
        "\n",
        "\n",
        "# -- Use this line of code when including age(yrs) --\n",
        "# testSubmission = [0.33,                          # age(yrs)\n",
        "#                   0,0,0,0,0,0,0,                 # 1a\n",
        "#                   0,0,0,                         # 1b\n",
        "#                   0,0,0,0,0,                     # 2a\n",
        "#                   1,0,1,1,1,1,1,0,1,             # 2b\n",
        "#                   0,0,0,1,1,0,0,0,1,0,0,0,1,     # 3\n",
        "#                   0,1,0,1,1,1]                   # 4\n",
        "\n",
        "\n",
        "#--- COUNT THE NUMBER OF \"YES\" AND \"NS\" SELECTIONS MADE --------------------\n",
        "for selection in testSubmission:\n",
        "    if (selection==1):\n",
        "        yes_counter+=1\n",
        "    elif (selection==0.5):\n",
        "        ns_counter+=1\n",
        "\n",
        "#--- DETERMINE IF SELECTIONS VALID FOR USE IN PREDICTIVE ANALYTICS WERE MADE --------------------\n",
        "if (ns_counter!=0 and yes_counter==0):\n",
        "    print(\"Please make selections other than just 'NS' on Nut_Phase Questionnaire form\")\n",
        "\n",
        "elif (yes_counter==0):\n",
        "    print(\"Please make valid selections on Nut_Phase Questionnaire form\")\n",
        "\n",
        "elif (yes_counter!=0):\n",
        "    #--- MAKE PREDICTION --------------------\n",
        "    arr = np.array([testSubmission])\n",
        "    prediction = tempModel.predict([arr])\n",
        "    print(prediction)\n",
        "\n",
        "\n",
        "    #--- DISPLAY PREDICTED NUTRITIONAL PHASE --------------------\n",
        "    output_columns = [\"Phase_1a\",\"Phase_1b\",\"Phase_2a\", \"Phase_2b\",\"Phase_3\", \"Phase_4\"]\n",
        "    print(\"\\n\"+ \"Predicted PWS Nutritional Phase:\" + \"\\n\"+\n",
        "          output_columns[prediction.argmax()] + \"\\n\")\n",
        "\n",
        "\n",
        "    #--- PROBABILITY DISTRIBUTION VALUES FOR ALL NUTRITIONAL PHASES REPRESENTED AS PERCENTAGES --------------------\n",
        "    i=0   # loop counting index\n",
        "    output_columns = [\"Phase_1a\",\"Phase_1b\",\"Phase_2a\", \"Phase_2b\",\"Phase_3\", \"Phase_4\"]\n",
        "\n",
        "    print(\"Probability Distribution:\")\n",
        "\n",
        "    for prob_dist_val in prediction[0,]:\n",
        "      print(output_columns[i] + \": \" + str(round(prediction[0,i]*100,1)) + \"%\")\n",
        "      i=i+1\n"
      ],
      "metadata": {
        "id": "Y2zsnL9RhMSw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7f24bda3-9e32-4e83-ab2e-e79ed4ffadd3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 153ms/step\n",
            "[[9.1957072e-26 1.5331028e-19 1.2938971e-25 1.0000000e+00 1.9820524e-20\n",
            "  1.8079337e-14]]\n",
            "\n",
            "Predicted PWS Nutritional Phase:\n",
            "Phase_2b\n",
            "\n",
            "Probability Distribution:\n",
            "Phase_1a: 0.0%\n",
            "Phase_1b: 0.0%\n",
            "Phase_2a: 0.0%\n",
            "Phase_2b: 100.0%\n",
            "Phase_3: 0.0%\n",
            "Phase_4: 0.0%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "**Model Prediction Functions**\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "D9KCnQR7I_72"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def predictPWSNutPhase(model, dataframe, toShuffle):\n",
        "    '''Function takes in a SNN model and a dataframe. Dataframe contains records\n",
        "       of nut_phase questionnaire responses to be analyzed by the model for\n",
        "       predicting the PWS nutritional phase of each patient'''\n",
        "\n",
        "    # Function variables\n",
        "    i=0                       # inner while loop iteration index\n",
        "    j=0                       # outer while loop iteration index\n",
        "    numModels=0               # stores the number of models passed by the function call\n",
        "    numCorrect=0\n",
        "    numIncorrect=0\n",
        "    records=[]\n",
        "    predictions=[]\n",
        "    refColumns=[]\n",
        "    comparisons=[]\n",
        "    mismatchIndeces=[]\n",
        "\n",
        "\n",
        "    #-- DATASET PREPARATION------------------------------------------------\n",
        "    # Dataset to analyze\n",
        "    df=dataframe\n",
        "\n",
        "    # Target columns\n",
        "    output_columns = [\"Phase_1a\", \"Phase_1b\", \"Phase_2a\", \"Phase_2b\",\"Phase_3\", \"Phase_4\"]\n",
        "\n",
        "    # Actual targets\n",
        "    diagnosis=df[\"nut_phase\"]\n",
        "    diagnosis.columns=[\"Diagnosis\"]\n",
        "\n",
        "    # Shuffle dataset\n",
        "    if (toShuffle==True):\n",
        "        df=shuffleDataFrame(df)\n",
        "\n",
        "    # Remove irrelevant data\n",
        "    df=df.drop(columns=[\"rec_num\", \"sample_id\", \"nut_phase\", \"Phase_1a\",\"Phase_1b\",\"Phase_2a\", \"Phase_2b\",\"Phase_3\", \"Phase_4\"])\n",
        "\n",
        "    # Convert dataframe elements (records) to a list object then append items to \"records\" list\n",
        "    while (i < len(df)):\n",
        "        rec=list(df.loc[i])\n",
        "        records.append(rec)\n",
        "        i+=1\n",
        "    i=0    # reset loop index\n",
        "\n",
        "    #-- MAKE PREDICTIONS ----------------------------------------------------\n",
        "    # Make predictions\n",
        "    while (j < len(records)):\n",
        "        prediction = model.predict([records[j]], verbose=0)\n",
        "        predictions.append(output_columns[prediction.argmax()])\n",
        "        j+=1\n",
        "    j=0    # reset loop index\n",
        "\n",
        "    #-- EVALUATE AND COMPARE RESULTS ----------------------------------------\n",
        "    # Convert diagnosis and predictions list objects to Pandas Data Frame objects\n",
        "    diagnosis=pd.DataFrame(diagnosis)\n",
        "    diagnosis.columns=[\"Actual\"]\n",
        "    predictions=pd.DataFrame(predictions)\n",
        "    predictions.columns=[\"Predicted\"]\n",
        "\n",
        "    # Append diagnosis and predicted columns to \"refColumns\" list object\n",
        "    refColumns.append(diagnosis)\n",
        "    refColumns.append(predictions)\n",
        "\n",
        "    # Append dataframes\n",
        "    results=pd.concat(refColumns, axis=1)\n",
        "\n",
        "    # Compare predicted nut_phase with subject's recorded diagnosis\n",
        "    while (i < len(diagnosis)):\n",
        "        if (diagnosis.loc[i][0]==predictions.loc[i][0]):\n",
        "          comparisons.append(\"True\")\n",
        "          numCorrect+=1\n",
        "        elif (diagnosis.loc[i][0]!=predictions.loc[i][0]):\n",
        "          comparisons.append(\"**False\")\n",
        "          numIncorrect+=1\n",
        "          mismatchIndeces.append(i+2)\n",
        "        i+=1\n",
        "    i=0\n",
        "\n",
        "    #-- DISPLAY RESULTS ----------------------------------------------------\n",
        "    # Expand \"results\" dataframe to include \"comparisons\"\n",
        "    comparisons=pd.DataFrame(comparisons)\n",
        "    comparisons.columns=[\"Result\"]\n",
        "    refColumns=[diagnosis, predictions, comparisons]\n",
        "    results=pd.concat(refColumns, axis=1)\n",
        "\n",
        "    # Print summary of results\n",
        "    print(\"Total number of records: \" + str(len(diagnosis)))\n",
        "    print(\"Number of correct predictions: \" + str(numCorrect))\n",
        "    print(\"Number of incorrect predictions: \" + str(numIncorrect) + \"\\n\")\n",
        "    print(\"Percent correct: \" + str(round((numCorrect)/len(diagnosis)*100,1)) + \"%\")\n",
        "    print(\"Percent incorrect: \" + str(round((numIncorrect)/len(diagnosis)*100,1)) + \"%\" + \"\\n\")\n",
        "\n",
        "    # If mismatches are found, then display Excel WorkSheet row numbers for the respective records\n",
        "    if (len(mismatchIndeces)!=0):\n",
        "        print(\"Records predicted incorrectly (Excel WorkSheet row number): \")\n",
        "        print(mismatchIndeces)\n",
        "    elif (len(mismatchIndeces)==0):\n",
        "        print(\"-- No incorrect predictions found --\")\n",
        "\n",
        "    return(results)\n"
      ],
      "metadata": {
        "id": "UYGGwi1vBjXL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def multiClassConfusionMatrixMetrics_MultiModel(listOfModels, dataframe):\n",
        "    '''Function evaluates multiple multiclass models using a multiclass confusion matrix\n",
        "    and returns various performance metrics: accuracy, precision, recall, and f-1_score.\n",
        "    Results can then be used to compare models and select the one with best performance'''\n",
        "\n",
        "    # Variables\n",
        "    i=0\n",
        "    j=0\n",
        "    df=dataframe\n",
        "    modelName=\"\"\n",
        "    confusionMatrices=[]\n",
        "    results=[]\n",
        "    accuracy=[]\n",
        "    outcomesAcc=[]\n",
        "    accuracyMetrics={}\n",
        "    accMetrics={}\n",
        "    allAccMetrics=[]\n",
        "    performanceMetrics={}\n",
        "    allPerfMetrics=[]\n",
        "    nutPhasePerfMetrics=[]\n",
        "    f1_scores=pd.DataFrame()\n",
        "    f1_scores_cols=[]\n",
        "\n",
        "\n",
        "    # Reference\n",
        "    nut_phases=[\"Phase_1a\", \"Phase_1b\", \"Phase_2a\", \"Phase_2b\", \"Phase_3\", \"Phase_4\"]\n",
        "    accMetrics={\"0\":\"TP\", \"1\":\"FP\", \"2\":\"FN\", \"3\":\"TN\"}    # TP-True Positive, FP-False Positive, FN-False Negative, TN-True Negative\n",
        "\n",
        "    # Make prediction (model, dataset, shuffle_dataset (True or False))\n",
        "    for model in models:\n",
        "        results.append(predictPWSNutPhase(model, df, False))\n",
        "\n",
        "\n",
        "    # Create the multiclass confusion matrix\n",
        "    for result in results:\n",
        "        confusionMatrices.append(pd.crosstab(result.Predicted, result.Actual))\n",
        "\n",
        "    # Create a heatmap of the confusion matrix\n",
        "    for confusionMatrix in confusionMatrices:\n",
        "        modelName=(\"Model_\" + str(i+1))\n",
        "        fig=plt.figure(figsize=(17,5))\n",
        "        ax1=plt.subplot(121)\n",
        "        ax1.set_title(\"Model_\" + str(i+1))\n",
        "        sn.heatmap(confusionMatrix, annot=True, cmap=\"Blues\")\n",
        "        i+=1\n",
        "    i=0\n",
        "\n",
        "    # Number of records\n",
        "    numRecords=confusionMatrices[0].sum().sum()\n",
        "\n",
        "    # Overall accuracy\n",
        "    for confusionMatrix in confusionMatrices:\n",
        "        acc=round((np.diag(confusionMatrix).sum()/numRecords*100),2)\n",
        "        accuracy.append(acc)\n",
        "\n",
        "\n",
        "    #-- PREDICTION ASSESSMENT FOR EACH NUT_PHASE -----------------------\n",
        "    # Classes: Phase_1a, Phase_1b, Phase_2a, Phase_2b, Phase_3, Phase_4\n",
        "    # Accuracy categories: {\"0\":\"TP\", \"1\":\"FP\", \"2\":\"FN\", \"3\":\"TN\"}\n",
        "    # Accuracy for each nutritional phase\n",
        "    while (j < len(confusionMatrices)):\n",
        "        while (i < len(confusionMatrices[0])):\n",
        "            TP=confusionMatrices[j].iloc[i,i]            # (TP) True positive predictions\n",
        "            FP=confusionMatrices[j].iloc[i,:].sum()-TP   # (FP) False positive predictions\n",
        "            FN=confusionMatrices[j].iloc[:,i].sum()-TP   # (FN) False negative predictions\n",
        "            TN=numRecords-(TP + FP + FN)                 # (TN) True negative predictions\n",
        "            outcomesAcc=listMultiAppend(TP, FP, FN, TN)\n",
        "            accuracyMetrics[nut_phases[i]+\".m\"+str(j+1)]=outcomesAcc\n",
        "            outcomesAcc=[]\n",
        "            i+=1\n",
        "        allAccMetrics.append(accuracyMetrics)\n",
        "        accuracyMetrics={}\n",
        "        i=0\n",
        "        j+=1\n",
        "    j=0\n",
        "\n",
        "    #-- MODEL PERFORMANCE CALCULATIONS ---------------------------------\n",
        "    while (j < (len(allAccMetrics))):\n",
        "        metricsDict=allAccMetrics[j]\n",
        "\n",
        "        while (i < len(allAccMetrics[0])):\n",
        "            listMetrics=metricsDict[nut_phases[i]+\".m\"+str(j+1)]\n",
        "\n",
        "            # Model's performance calculations\n",
        "            accuracy=(round(((listMetrics[0] + listMetrics[3])/numRecords)*100, 2))        # [0] accuracy\n",
        "            precision=(round((listMetrics[0]/(listMetrics[0] + listMetrics[1]))*100, 2))   # [1] precision\n",
        "            recall=(round((listMetrics[0]/(listMetrics[0] + listMetrics[2]))*100, 2))      # [2] recall\n",
        "            f1_score=(round(((2*precision * recall)/(precision + recall)), 2))             # [3] f-1_score\n",
        "\n",
        "            # Store results\n",
        "            nutPhasePerfMetrics=listMultiAppend(accuracy, precision, recall, f1_score)\n",
        "            performanceMetrics[nut_phases[i]+\".m\"+str(j+1)]=nutPhasePerfMetrics\n",
        "            nutPhasePerfMetrics=[]\n",
        "            i+=1\n",
        "        allPerfMetrics.append(performanceMetrics)\n",
        "        performanceMetrics={}\n",
        "        i=0\n",
        "        j+=1\n",
        "    j=0\n",
        "\n",
        "    #-- DISPLAY PERFORMANCE RESULTS ------------------------------------\n",
        "    while (j < len(allPerfMetrics)):\n",
        "        print(\"\\n\" + \"MODEL \"+str(j+1)+\" SUMMARY: --\")\n",
        "\n",
        "        while (i < len(allPerfMetrics[0])):\n",
        "            perfDict=allPerfMetrics[j]\n",
        "            listMetrics=perfDict[nut_phases[i]+\".m\"+str(j+1)]\n",
        "\n",
        "            print(\"Performance for class \"+ nut_phases[i] + \"\\n\" +\n",
        "                \"Accuracy: \" + str(listMetrics[0]) + \"%\"+ \"\\n\" +\n",
        "                \"Precision: \" + str(listMetrics[1]) + \"%\"+ \"\\n\" +\n",
        "                \"Recall: \" + str(listMetrics[2]) + \"%\" + \"\\n\" +\n",
        "                \"F1_Score: \" + str(listMetrics[3]) + \"%\" + \"\\n\")\n",
        "            i+=1\n",
        "        # print model's overall accuracy\n",
        "        print(\"Overall accuracy: \" + str(listMetrics[0]) + \"%\")\n",
        "        print(\"Total number of records analyzed: \" + str(confusionMatrices[j].sum().sum()))\n",
        "        print(\"\\n\"+\"\\n\")\n",
        "        i=0\n",
        "        j+=1\n",
        "    j=0\n",
        "\n",
        "    #-- DISPLAY CLASSIFICATION REPORTS ----------------------------------------------\n",
        "    while (i < len(allPerfMetrics)):\n",
        "        print(\"\\n\" + \"\\n\" + \"Classification Report for Model_\" + str(i+1))\n",
        "        report=pd.DataFrame(metrics.classification_report(results[i].Actual, results[i].Predicted, output_dict=True)).T\n",
        "        report.transpose()\n",
        "        report.columns=[\"precision\", \"recall\", \"f1-score\", \"#_of_records\"]\n",
        "        print(report)\n",
        "        f1_score=pd.DataFrame(report[\"f1-score\"])\n",
        "        if (i==0):\n",
        "            f1_scores=f1_score\n",
        "        elif (i==1):\n",
        "            f1_scores=pd.concat([f1_scores, f1_score], axis=1)\n",
        "        f1_scores_cols.append(\"Model_\" + str(i+1) + \"_f1\")\n",
        "        i+=1\n",
        "    i=0\n",
        "\n",
        "    # Print table of f-1 scores to compare models\n",
        "    f1_scores.columns=[f1_scores_cols]\n",
        "    print(\"\\n\")\n",
        "    print(f1_scores)\n",
        "    print(\"\\n\")\n",
        "\n",
        "    #-- METRICS NOTE: accuracy, precision, recall, f-_score -----------------\n",
        "    print(\"The F1-score combines the precision and recall of a classifier into a single\" + \"\\n\" +\n",
        "      \" metric by taking their harmonic mean. It is primarily used to compare the\" + \"\\n\" +\n",
        "      \" performance of two classifiers. - Educative.io\"  + \"\\n\" + \"\\n\" +\n",
        "\n",
        "      \"Precision or Recall? Precision measures the extent of error caused by False Pos (FPs)\" + \"\\n\"\n",
        "      \" while Recall measures the extent of error caused by False Neg (FNs).\"  + \"\\n\"\n",
        "      \"Depending on the case, go with the metric and model that produces the least desirable\" + \"\\n\"\n",
        "      \" outcome. Thus if FP, then choose model with highest Precision. If FN, then choose\" + \"\\n\"\n",
        "      \" model with highest Recall.\")\n"
      ],
      "metadata": {
        "id": "mhH3kSWtIUvI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "filename1=\"snn_nut_phase_questionnaire_training_dataset_NO_age; 09-26-2023.csv\"                 # features: 43\n",
        "filename2=\"snn_nut_phase_questionnaire_training_dataset_WITH_age; 09-26-2023.csv\"               # features: 44\n",
        "filename3=\"snn_nut_phase_questionnaire_normalized_training_dataset_NO_age; 09-22-2023.csv\"      # features: 43\n",
        "filename4=\"snn_nut_phase_questionnaire_normalized_training_dataset_WITH_age; 09-22-2023.csv\"    # features: 44\n",
        "\n",
        "# Read CSV datafile and convert to a Pandas Dataframe object\n",
        "df=pd.DataFrame(readCSVFile(filename1))\n",
        "\n",
        "models=[tempModel_1, tempModel_2]\n",
        "results=multiClassConfusionMatrixMetrics_MultiModel(models, df)\n",
        "\n",
        "results\n"
      ],
      "metadata": {
        "id": "-vB8VSSW2443",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 552
        },
        "outputId": "6ae6953f-ca6e-41ae-a8e1-a0bdbf4f48a3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total number of records: 347\n",
            "Number of correct predictions: 347\n",
            "Number of incorrect predictions: 0\n",
            "\n",
            "Percent correct: 100.0%\n",
            "Percent incorrect: 0.0%\n",
            "\n",
            "-- No incorrect predictions found --\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "IndexError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-55-497f6541c1cb>\u001b[0m in \u001b[0;36m<cell line: 10>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mmodels\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtempModel_1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtempModel_2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0mresults\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmultiClassConfusionMatrixMetrics_MultiModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0mresults\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-52-0b87c2e034fd>\u001b[0m in \u001b[0;36mmultiClassConfusionMatrixMetrics_MultiModel\u001b[0;34m(listOfModels, dataframe)\u001b[0m\n\u001b[1;32m     29\u001b[0m     \u001b[0;31m# Make prediction (model, dataset, shuffle_dataset (True or False))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmodels\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m         \u001b[0mresults\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredictPWSNutPhase\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-53-b55869e1404f>\u001b[0m in \u001b[0;36mpredictPWSNutPhase\u001b[0;34m(model, dataframe, toShuffle)\u001b[0m\n\u001b[1;32m     46\u001b[0m     \u001b[0;32mwhile\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mj\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrecords\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m         \u001b[0mprediction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mrecords\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 48\u001b[0;31m         \u001b[0mpredictions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_columns\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mprediction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     49\u001b[0m         \u001b[0mj\u001b[0m\u001b[0;34m+=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m     \u001b[0mj\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m    \u001b[0;31m# reset loop index\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mIndexError\u001b[0m: list index out of range"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "**Model Calibration**\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "g8wd3oJnLzWT"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "R-TA_H1ULyIb"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}